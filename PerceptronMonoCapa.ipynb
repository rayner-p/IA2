{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "PerceptronMonoCapa",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayner-p/IA2/blob/main/PerceptronMonoCapa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOFHqOqxUcRN"
      },
      "source": [
        "#**Universidad PolitÃ©cnica Salesiana**\n",
        "## Inteligencia Artificial II\n",
        "### Rayner Palta\n",
        "## ***Redes Neuronales y One Hot Encoding***\n"
      ],
      "id": "BOFHqOqxUcRN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "medical-parker",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b8713578-9044-4b44-ba3b-c1c7fd3bacfb"
      },
      "source": [
        "!pip install viznet\n"
      ],
      "id": "medical-parker",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting viznet\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/a8/487ca80821847046011621f492f05949cfa5296342ff24f775c3ad444902/viznet-0.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from viznet) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from viznet) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from viznet) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->viznet) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->viznet) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->viznet) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->viznet) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->viznet) (1.15.0)\n",
            "Installing collected packages: viznet\n",
            "Successfully installed viznet-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "vertical-kenya",
        "outputId": "999b53b8-9eb3-476c-b90d-cf2b8317e0e2"
      },
      "source": [
        "import matplotlib.pyplot as pp\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "x=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "d=np.array([[0,0],[1,1],[1,1],[0,0]])\n",
        "#pp.scatter(x[:,0],x[:,1],color=['blue' if i==1 else 'red' for i in d])\n",
        "\n",
        "pp.scatter(x[:,0],x[:,1],color=['blue' if (i==1).any() else 'red' for i in d])\n",
        "\n",
        "#pp.grid(True)\n",
        "pp.show()"
      ],
      "id": "vertical-kenya",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "fitted-keeping",
        "outputId": "3591c318-de04-4c55-d947-856ee35557f8"
      },
      "source": [
        "from viznet import connecta2a, node_sequence, NodeBrush, EdgeBrush, DynamicShow\n",
        "entrada= 2\n",
        "neuronas_capa_oculta=4\n",
        "neuronas_capa_salida=2\n",
        "\n",
        "def dibujar_red_neuronal(ax, num_node_list):\n",
        "\n",
        "    num_hidden_layer = len(num_node_list) - 2\n",
        "    token_list = ['\\sigma^z'] + \\\n",
        "        ['y^{(%s)}' % (i + 1) for i in range(num_hidden_layer)] + ['\\psi']\n",
        "    kind_list = ['nn.input'] + ['nn.hidden'] * num_hidden_layer + ['nn.output']\n",
        "    radius_list = [0.3] + [0.2] * num_hidden_layer + [0.3]\n",
        "    y_list = 1.5 * np.arange(len(num_node_list))\n",
        "\n",
        "    seq_list = []\n",
        "    for n, kind, radius, y in zip(num_node_list, kind_list, radius_list, y_list):\n",
        "        b = NodeBrush(kind, ax)\n",
        "        seq_list.append(node_sequence(b, n, center=(0, y)))\n",
        "\n",
        "    eb = EdgeBrush('-->', ax)\n",
        "    for st, et in zip(seq_list[:-1], seq_list[1:]):\n",
        "        connecta2a(st, et, eb)\n",
        "\n",
        "\n",
        "def real_bp():\n",
        "    with DynamicShow((6, 6), '_feed_forward.png') as d:\n",
        "        dibujar_red_neuronal(d.ax, num_node_list=[entrada, neuronas_capa_oculta, neuronas_capa_salida])\n",
        "\n",
        "\n",
        "real_bp()\n",
        "    "
      ],
      "id": "fitted-keeping",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Press `c` to save figure to \"_feed_forward.png\", `Ctrl+d` to break >>\n",
            "> /usr/local/lib/python3.7/dist-packages/viznet/context.py(61)__exit__()\n",
            "-> plt.savefig(self.filename, dpi=300, transparent=True)\n",
            "(Pdb) c\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "personal-palace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "94e305ba-115c-43c9-a073-b4561d229e50"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp=MLPClassifier(solver = 'lbfgs', activation='logistic', verbose=True, alpha=1e-4, tol=1e-15, max_iter=10000, \\\n",
        "                  hidden_layer_sizes=(neuronas_capa_oculta, neuronas_capa_salida))\n",
        "print(mlp)\n",
        "\n",
        "mlp.fit (x,d)\n",
        "\n",
        "print('Pesos W^(0): \\n:',mlp.coefs_[0])\n",
        "print('Pesos W^(1): \\n:',mlp.coefs_[1])\n",
        "\n",
        "for entrada in x:\n",
        "    print('\\nPrueba con {','|'.join([str(i) for i in entrada]),'} => ',mlp.predict(entrada.reshape(1,-1)))\n"
      ],
      "id": "personal-palace",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
            "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "              hidden_layer_sizes=(4, 2), learning_rate='constant',\n",
            "              learning_rate_init=0.001, max_fun=15000, max_iter=10000,\n",
            "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "              power_t=0.5, random_state=None, shuffle=True, solver='lbfgs',\n",
            "              tol=1e-15, validation_fraction=0.1, verbose=True,\n",
            "              warm_start=False)\n",
            "Pesos W^(0): \n",
            ": [[ 1.20859271e-02  4.88374634e+00 -4.88577594e+00 -1.52875855e-02]\n",
            " [ 9.31368277e-03 -4.87974538e+00  4.87952069e+00 -3.53721206e-03]]\n",
            "Pesos W^(1): \n",
            ": [[-2.43076516e-02 -5.38451241e-03]\n",
            " [ 6.08500908e+00 -6.08872465e+00]\n",
            " [ 6.08377048e+00 -6.08779246e+00]\n",
            " [-4.85400434e-03 -1.47837797e-03]]\n",
            "\n",
            "Prueba con { 0|0 } =>  [[0 0]]\n",
            "\n",
            "Prueba con { 0|1 } =>  [[1 1]]\n",
            "\n",
            "Prueba con { 1|0 } =>  [[1 1]]\n",
            "\n",
            "Prueba con { 1|1 } =>  [[0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSbm3IAyC7Jd"
      },
      "source": [
        "#One Hot Encoding \n",
        "##PrÃ¡ctica 1. Modificar el cÃ³digo dado con el fin de usar One Hot Encoding dentro del mismo\n",
        " "
      ],
      "id": "WSbm3IAyC7Jd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "functional-transcript",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1222d0a4-b2b1-4c27-880d-4eff3756b646"
      },
      "source": [
        "enc = OneHotEncoder()\n",
        "y = OneHotEncoder().fit_transform(x).toarray()\n",
        "dd = OneHotEncoder().fit_transform(d).toarray()\n",
        "enc.fit(y)\n",
        "enc.fit(dd)\n",
        "labels = enc.transform(y).toarray()\n",
        "labels2 = enc.transform(dd).toarray()\n",
        "print(y)\n",
        "print(d)\n",
        "#labels.shape"
      ],
      "id": "functional-transcript",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 1. 0.]\n",
            " [1. 0. 0. 1.]\n",
            " [0. 1. 1. 0.]\n",
            " [0. 1. 0. 1.]]\n",
            "[[0 0]\n",
            " [1 1]\n",
            " [1 1]\n",
            " [0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "japanese-spain",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "53201520-33aa-4267-e0b1-d22d9845d87a"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp=MLPClassifier(solver = 'lbfgs', activation='logistic', verbose=True, alpha=1e-4, tol=1e-15, max_iter=10000, \\\n",
        "                  hidden_layer_sizes=(neuronas_capa_oculta, neuronas_capa_salida))\n",
        "print(mlp)\n",
        "\n",
        "mlp.fit (y,d)\n",
        "\n",
        "print('Pesos W^(0): \\n:',mlp.coefs_[0])\n",
        "print('Pesos W^(1): \\n:',mlp.coefs_[1])\n",
        "\n",
        "for entrada in y:\n",
        "    print('\\nPrueba con {','|'.join([str(i) for i in entrada]),'} => ',mlp.predict(entrada.reshape(1,-1)))\n"
      ],
      "id": "japanese-spain",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
            "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "              hidden_layer_sizes=(4, 2), learning_rate='constant',\n",
            "              learning_rate_init=0.001, max_fun=15000, max_iter=10000,\n",
            "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "              power_t=0.5, random_state=None, shuffle=True, solver='lbfgs',\n",
            "              tol=1e-15, validation_fraction=0.1, verbose=True,\n",
            "              warm_start=False)\n",
            "Pesos W^(0): \n",
            ": [[-2.97371550e+00 -2.38123595e-03 -2.64341904e+00 -1.20966885e+00]\n",
            " [ 2.97226745e+00 -4.83649548e-03  2.64581952e+00  1.21571252e+00]\n",
            " [ 2.64407394e+00 -2.61432727e-03  2.97522351e+00 -1.20503188e+00]\n",
            " [-2.64634904e+00 -4.39473757e-03 -2.97299666e+00  1.21171204e+00]]\n",
            "Pesos W^(1): \n",
            ": [[ 5.71048537e+00 -5.71534158e+00]\n",
            " [ 4.17919821e-03 -2.79637741e-03]\n",
            " [-5.71294348e+00  5.71230669e+00]\n",
            " [-1.19499342e+00  1.19404733e+00]]\n",
            "\n",
            "Prueba con { 1.0|0.0|1.0|0.0 } =>  [[0 0]]\n",
            "\n",
            "Prueba con { 1.0|0.0|0.0|1.0 } =>  [[1 1]]\n",
            "\n",
            "Prueba con { 0.0|1.0|1.0|0.0 } =>  [[1 1]]\n",
            "\n",
            "Prueba con { 0.0|1.0|0.0|1.0 } =>  [[0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orXg2zZwzta3"
      },
      "source": [
        "# Segundo ejercicio\n",
        "## Genere 1000 puntos aleatorios con coordenadas $(x_{1}, x_{2})$. Con estos puntos, deberÃ¡ realizar las siguientes tareas:\n",
        "\n",
        "### Seleccionar de forma aleatoria 80% de los puntos para entrenar la red y el restante 20% se emplearÃ¡ para probar la red.\n",
        "\n",
        "> \n",
        "\n",
        "1. Seleccionar de forma aleatoria 80% de los puntos para entrenar la red y el restante 20% se emplearÃ¡ para probar la red.\n",
        "2.   Entrenar la red hasta lograr un error mÃ­nimo.\n",
        "3.   Probar la red y presentar la matriz de confusiÃ³n.\n",
        "4. Indicar el nivel de precisiÃ³n (muestras correctamente clasificadas frente al total de muestras):"
      ],
      "id": "orXg2zZwzta3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MjobNGbRzjJO",
        "outputId": "c4746a6a-b8a3-4d48-ebb7-1e00b6f824e5"
      },
      "source": [
        "import random as rd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "coords =[] \n",
        "x1= [(rd.random()) for _ in range(1000)]\n",
        "x2= [(rd.random()) for _ in range(1000)]\n",
        "valores= [x1, x2]\n",
        "#coords = np.random.rand(1000)\n",
        "print(x1)\n",
        "print(x2)\n",
        "print(valores)\n"
      ],
      "id": "MjobNGbRzjJO",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.22103912674625525, 0.8111900042630228, 0.42951426299031836, 0.7469652375524678, 0.7979359623540457, 0.6536373836439845, 0.07201120040511122, 0.8507641059773695, 0.48162443572602454, 0.9673584540752799, 0.5370721778609542, 0.2764988643505668, 0.614662018594901, 0.5623430429941367, 0.6409426282518532, 0.6198776417819745, 0.4870239703384154, 0.7702083954520236, 0.9580320447061101, 0.09508488900109491, 0.0993189241309731, 0.14947398785596466, 0.6497678523724131, 0.4523989180580935, 0.9941709438671489, 0.4192831902086438, 0.2029946712451045, 0.48892695493552474, 0.2644535556546841, 0.3674780738563771, 0.19279225122364307, 0.7186995808470215, 0.40631395955456406, 0.6651365550099916, 0.6289820247829946, 0.8891568314826679, 0.8175679382448483, 0.5085360703394999, 0.6681534105703026, 0.24730396148493516, 0.8394046025844185, 0.27963056952858434, 0.9019917096696914, 0.6834029912109786, 0.610150012036229, 0.36274289198627496, 0.6956694768121002, 0.11076053096730687, 0.07574478456719702, 0.583495386153743, 0.8277043337022649, 0.9734881492836075, 0.13962006598358323, 0.5789186884418022, 0.5135143954252981, 0.5846192258056293, 0.2956574183726247, 0.21263604710587247, 0.5006929119688778, 0.8320486480813896, 0.6436249858334131, 0.341636178361036, 0.3090157136987345, 0.800856286686457, 0.2858944450290434, 0.9850760390000141, 0.7727628371970848, 0.5829471249523998, 0.6893849840226755, 0.8539113470299947, 0.6817899176634633, 0.1869551187741385, 0.834004872024807, 0.04507918431278135, 0.0807645698654208, 0.6237559570143847, 0.6781090105631913, 0.9019495342564238, 0.5630189778591199, 0.20892778029088055, 0.41188266542599106, 0.4967412560463169, 0.8872491981654159, 0.3453791370846869, 0.37878942410077154, 0.26057425003205437, 0.06218089006605487, 0.01905489496171331, 0.005009704353606037, 0.6490689056464117, 0.2362353880372574, 0.775045266137764, 0.01455085875790807, 0.5169296141405103, 0.7308704593180905, 0.10445077661523305, 0.7770435000172137, 0.0211390435983273, 0.0591450562277438, 0.9804737696038958, 0.9710647934103506, 0.817108378881203, 0.9171830111192955, 0.12071918309984675, 0.2343843522030835, 0.05414219758256833, 0.9570273544675788, 0.38493939673991806, 0.2898676153740851, 0.6279346410369557, 0.37010949677099814, 0.47134895528929965, 0.9549225056562952, 0.9411034090368441, 0.6656714861948121, 0.31572258695204203, 0.4091088422739917, 0.4960831701523588, 0.9323220264688661, 0.015505005371738934, 0.4106736667677826, 0.9729279068357328, 0.7469782623404646, 0.31590482166836176, 0.7052336406976385, 0.9442269024421602, 0.08260776797383251, 0.9171048497829355, 0.2950348134291719, 0.5219823412642809, 0.09553136027320897, 0.29202720409741634, 0.5994402299577554, 0.5924591581573991, 0.9875130963835378, 0.685457906805213, 0.5035387787625176, 0.07108706475058524, 0.06131590208239257, 0.23196669187230157, 0.0533009098625864, 0.9876828047737938, 0.8953601624699701, 0.3643445769898639, 0.18665018002595635, 0.011803174933725735, 0.2232431719852871, 0.016560122064046334, 0.8894922119973248, 0.779472417666516, 0.45116311810297915, 0.5169640513302226, 0.46935549768021245, 0.3812485454656478, 0.9901701523424052, 0.6594793486438643, 0.3000330350120316, 0.5368206389310268, 0.20369558796677623, 0.9821657899112646, 0.7751729478745217, 0.6886873039166755, 0.7420468613029666, 0.6659855140556318, 0.4170239817366883, 0.07810473747013591, 0.9240279527845319, 0.9153305868950286, 0.6165639616892881, 0.8754919861072094, 0.346639181441425, 0.7027465713134695, 0.7449489678937677, 0.20080808285059304, 0.2782208022525273, 0.8735417402742093, 0.1560052818509593, 0.7212017974242486, 0.08571680092500011, 0.03633706664835168, 0.7051284857810262, 0.593333395197892, 0.9995679719606146, 0.1381655062792001, 0.021603109693013045, 0.24646902517004832, 0.02890837972359661, 0.15479451039041303, 0.21786961611018474, 0.35658058111784996, 0.4415058510232239, 0.38036001599739766, 0.43400692200540314, 0.6780964448753074, 0.2865734047899279, 0.4421532268735634, 0.4220215585399284, 0.7487449729109392, 0.4604820098159915, 0.8727003718090691, 0.1842651439754106, 0.8392959031084398, 0.22506068096280474, 0.10348533975660767, 0.6807268363575099, 0.21747279158158073, 0.8518740724310474, 0.36559384507665704, 0.2090236430408462, 0.13717321150895045, 0.46694225378084453, 0.7908302876204146, 0.7770817970378033, 0.6896815071828815, 0.6161942450359076, 0.792385251927385, 0.07061286900104591, 0.8687341847450756, 0.4282204919561875, 0.471552100163942, 0.9994986018623357, 0.8938430576829656, 0.12127859880914149, 0.29063503117151845, 0.6040632383385101, 0.32673524221192096, 0.9908454427980734, 0.5881233376837314, 0.5488418348700916, 0.7846754709363577, 0.3468396799285749, 0.8151695358916028, 0.6544542685309493, 0.930015242140173, 0.8691119072388134, 0.35859306055117346, 0.48287578849535184, 0.19660509443758079, 0.44006694751806685, 0.7532966686889837, 0.9120784652915193, 0.6325592665206765, 0.6452115920057965, 0.44174974076918605, 0.36078982358392664, 0.6204647349639503, 0.8009028078178388, 0.9756871724997992, 0.6303547262502435, 0.24771378000268995, 0.8264678964591243, 0.12683119155037836, 0.24171420821043588, 0.2632308328011228, 0.2559153116466616, 0.7414070971630438, 0.3917549386063237, 0.7301910766276495, 0.9992934880527558, 0.86677171817462, 0.48624936987003575, 0.554644609007801, 0.19575099428466636, 0.2490922687057815, 0.5450130163117278, 0.3410091211808073, 0.17257836751973177, 0.0130586252442082, 0.3033779082405218, 0.959777907131499, 0.6158814393900229, 0.37569349705060473, 0.9710972299744295, 0.9557095144353198, 0.3544806836328279, 0.49040251420613346, 0.5141298709898726, 0.6287371724816011, 0.6048901442679656, 0.17107964417507693, 0.2182260476488711, 0.3599134913403147, 0.9211701291823132, 0.7677246611945072, 0.1995693358401579, 0.15572752420555225, 0.03204891819408362, 0.8697084179816341, 0.6513459620409767, 0.6067803537313825, 0.9573588412649686, 0.33656973251217426, 0.6876552685291011, 0.13626343078634684, 0.5825800047805135, 0.9330741531278789, 0.4306294286038338, 0.8226001267038058, 0.7702112108331253, 0.5703709626262959, 0.5438655140492347, 0.742767200237467, 0.710586767870287, 0.4259762742433153, 0.6755377784147991, 0.03759148041042992, 0.7929390934376143, 0.06294252811410361, 0.5484829688494517, 0.9632136372768376, 0.33846393515975937, 0.607290652070273, 0.30848765191964445, 0.1073811642730228, 0.9584768199830103, 0.8031335111996362, 0.35421756701207374, 0.4842800285351938, 0.4123758924011125, 0.3997718524091364, 0.7786678638284608, 0.18075269387836634, 0.14916408458902897, 0.5700134390784323, 0.34764019553362624, 0.8805846500437261, 0.6986137384067126, 0.9017113089224322, 0.05480974161006069, 0.7411104344790785, 0.6080612489444649, 0.24291820071835268, 0.2155241163586068, 0.9529384841642207, 0.026777464776063464, 0.18418683348837472, 0.08678759576402428, 0.7969954255585435, 0.5607063039701555, 0.8766789546366938, 0.010705858992222539, 0.20366471074053993, 0.9346879068943649, 0.12376756979416748, 0.18816644254078496, 0.6836261490297284, 0.34341427395776714, 0.5579773026590239, 0.09716056442960197, 0.23308956084568844, 0.4513090911410641, 0.657813251301382, 0.13834062575498707, 0.16185836051344937, 0.644467998553535, 0.6107212154371839, 0.5781828025111444, 0.8850194782483777, 0.7279084537304513, 0.28779158260032245, 0.26882077846096974, 0.5207235858433491, 0.6151814323057649, 0.589604528074291, 0.9878209642278217, 0.6420837543254991, 0.7011127847587115, 0.6578810697887715, 0.13682901330106245, 0.12188908410747512, 0.43580436889210505, 0.2946556472749299, 0.11677954143161429, 0.8253759333005345, 0.6143964680795835, 0.4558300236674818, 0.515769495511947, 0.20052909339083913, 0.024950832018199143, 0.2268883861691915, 0.38857073174147827, 0.5999170199460846, 0.5855362503198795, 0.3101834802754173, 0.6917583609472449, 0.5907092941575802, 0.6441143220899739, 0.7189236189471642, 0.7007224572599257, 0.08746186351500562, 0.5327886592429653, 0.5494532531782015, 0.15762740277249077, 0.4526539114314545, 0.1690920850668447, 0.49911627255976965, 0.07069850798349109, 0.5609965856884815, 0.4507069477643614, 0.2054976440990196, 0.8709508128772104, 0.7947338330020295, 0.7960225083790698, 0.6420126951190118, 0.37970498352504145, 0.6570545854209047, 0.04736675905858567, 0.6711037372980048, 0.9795313461861574, 0.8966446088023423, 0.9726812379954103, 0.058810444328967315, 0.1325971762992637, 0.26826772386539577, 0.6739340687742597, 0.5312429839000495, 0.358622744174219, 0.9486283550682314, 0.7121530071737475, 0.9786254622404259, 0.7129600594250382, 0.7515492601450531, 0.10200514994395571, 0.5698239588079591, 0.983569103722505, 0.5119337345597044, 0.5009154467203285, 0.16668414511122853, 0.8574322216594626, 0.6651008231184512, 0.18255519588476576, 0.18304941371337635, 0.6359209174932966, 0.5329863930073735, 0.10367744188977535, 0.03587424942706263, 0.5754736090801489, 0.2873388592198154, 0.3954227920392791, 0.23062657719188373, 0.5215567890237832, 0.8194098459240976, 0.9785270700133194, 0.8347161549937182, 0.06870265345159099, 0.9949844734836858, 0.7970226883767024, 0.20310231013618651, 0.36928098467294135, 0.6227581739767474, 0.5000814145182936, 0.3481340616047258, 0.14183888800076416, 0.342124377247377, 0.950347038983807, 0.6675454597076419, 0.16081901305641977, 0.15310931878395118, 0.5473807637460003, 0.7595840356693971, 0.4513347037085841, 0.30083838907152105, 0.33160963195553095, 0.7045723615759838, 0.5896939954567444, 0.1232792822215486, 0.269596288202709, 0.5171761367584721, 0.008640791801551861, 0.08679968913029756, 0.09734295601496556, 0.5578938926497711, 0.3434813326758551, 0.48620592580884636, 0.3270761520465172, 0.2692332005664314, 0.2381802414757922, 0.7335478183601795, 0.626188084370463, 0.009453860225817734, 0.428047788531457, 0.04331064652336902, 0.9225045139148663, 0.6616081623649904, 0.8253346252386404, 0.2508016594471587, 0.11058353747504723, 0.6626260730461645, 0.21813687788438252, 0.23731616223398788, 0.180753040553027, 0.7493318097937411, 0.059108821298735514, 0.5766626761685948, 0.21507436801817503, 0.22367511928438555, 0.17425019733900227, 0.5959972077224781, 0.32715944657755613, 0.2719808515921194, 0.1273395761444026, 0.728145335839452, 0.5356466113101257, 0.2026627530863987, 0.8569716968683811, 0.737217351587602, 0.9368327215081078, 0.99179850993871, 0.8500096294911198, 0.5908812164536726, 0.8782513534882442, 0.16476103929892072, 0.1421575576454439, 0.14236377947700596, 0.3589686603547835, 0.25288387339480056, 0.5598050536534493, 0.0818707990426012, 0.16272579906760787, 0.22731808100684125, 0.40295210014179517, 0.6762832822084294, 0.4424045070278999, 0.8549775110935559, 0.18654747650022763, 0.9505770350509355, 0.6720087242325685, 0.7648104603377346, 0.6977642945269018, 0.16587348759202203, 0.7174038640074459, 0.39924587794369504, 0.8292828226003399, 0.7853898692982674, 0.2883733476155529, 0.7090457286572698, 0.7086833849657413, 0.7331065696014712, 0.9055414660749451, 0.7812110405724049, 0.20025089539930296, 0.2567061094942362, 0.056021855936944975, 0.5170861897042364, 0.6652194881836918, 0.07112056839300673, 0.9157776974344926, 0.2753582473748233, 0.5036291564693604, 0.5165881412698529, 0.2697407776735131, 0.13784431692761923, 0.9812120289113027, 0.4270307642333936, 0.954511241894639, 0.8148547816573318, 0.18581235220755554, 0.46613683475660117, 0.41965970514113204, 0.011785734663164571, 0.9143626110706748, 0.6596818197938866, 0.4741504571013745, 0.8291505197506557, 0.6954695387804356, 0.2798145491817077, 0.346540262655479, 0.5001415883082466, 0.47489008480216344, 0.08693989419805659, 0.15166999476602672, 0.5141591205103709, 0.48328992765405165, 0.6329449128149843, 0.4292406710313277, 0.5103353670972127, 0.4377099021194132, 0.6781133632467223, 0.6777379704645072, 0.6696534292184478, 0.9607401486615063, 0.976723126647542, 0.10714873752465315, 0.8061467892369041, 0.782945060379704, 0.35390144389782363, 0.6649520468863779, 0.7174883166946048, 0.262349844080354, 0.8942379423244092, 0.5726713111864563, 0.3117423182069621, 0.9303987291852291, 0.21915724471031106, 0.7219253855992978, 0.3361332226082434, 0.26053592347767396, 0.07117734437591172, 0.6778364228496393, 0.4535052407282476, 0.18300668220120164, 0.530599566425599, 0.6840360138213053, 0.9734922765171731, 0.4176591802345907, 0.3617768518065033, 0.25525802357735894, 0.40269138011972405, 0.013530449346710327, 0.8294163482519165, 0.8763069303792506, 0.6114436440850204, 0.8892160951805484, 0.8680965061953393, 0.9173796415317592, 0.9282746462416748, 0.9125142398733772, 0.45873309481867297, 0.40107202157297583, 0.6471578398742169, 0.5578612002655083, 0.4657346557048171, 0.5217979292888757, 0.6300881121608799, 0.9483479644871142, 0.5692805328603099, 0.7105152355120767, 0.7254531863867859, 0.725307946688445, 0.9811993597182497, 0.9575050265055409, 0.006018028511397766, 0.07031161337918879, 0.9267913420623413, 0.47804904716610197, 0.047983920091837606, 0.7641751731247141, 0.4837975045062406, 0.5761497708263816, 0.3160717451616616, 0.5370682768377494, 0.5398958040464561, 0.2142621633008407, 0.04414129252837329, 0.5641437240984544, 0.568702295478442, 0.8088863826788268, 0.8511566502509789, 0.29641266960793655, 0.9588645044768901, 0.4562495392759608, 0.041623544799176404, 0.13001203319726884, 0.38386061378005554, 0.045271691540960446, 0.3345454299747196, 0.1344247541668332, 0.6370005858090193, 0.6537072494316049, 0.27898082842206595, 0.582616653956922, 0.27595218070890837, 0.4450464004066246, 0.23539120616618026, 0.9864554479973738, 0.5815885707497914, 0.6940145445282838, 0.6574758366717591, 0.6208987453989206, 0.34046117130014664, 0.5885927201671275, 0.09571836212910101, 0.43237753568099146, 0.29751743683948395, 0.03554056914702752, 0.926205565237621, 0.807332802766471, 0.5847089492292887, 0.48475210031206906, 0.6979859947747417, 0.9033441517211286, 0.7215728511833184, 0.40870623911775095, 0.0923692486336889, 0.9045573978647629, 0.8772250100456854, 0.011124156763532067, 0.4587322725686268, 0.7411865259023391, 0.8350685633215226, 0.8706409233092064, 0.9074873988449815, 0.20210691363161915, 0.6070184336991323, 0.09222595655065535, 0.9549366911225088, 0.012615225716585354, 0.6216505184906639, 0.4989245761674743, 0.8541636221451864, 0.14546480163688646, 0.6384724659291385, 0.9064461043132799, 0.4283772285635471, 0.8418022674617376, 0.05879634634239983, 0.2145765860753479, 0.46573683844193936, 0.19243657473393838, 0.6931590926006685, 0.08422812267766178, 0.843302524671135, 0.019604440950580226, 0.508352460707797, 0.291705342130406, 0.819146536951836, 0.6311148256617425, 0.4084791890966373, 0.0924042065091728, 0.9530699637428814, 0.4438640625123845, 0.8849251574585029, 0.585570920843536, 0.4277917004899534, 0.9416165657588469, 0.6345885755655489, 0.728774268485828, 0.0093229383098411, 0.47573173469606356, 0.9715701884139287, 0.11447464655712813, 0.6826679937889525, 0.7209358649762365, 0.39161598216382143, 0.5288818498743052, 0.9786237542436418, 0.8501722929588108, 0.44436504734299975, 0.4800500460512599, 0.0145744808568683, 0.8833251324884653, 0.40101814842680217, 0.6321006412430001, 0.9281024134641103, 0.37102163544909317, 0.04381304485304893, 0.8947599351387711, 0.37277296643549473, 0.980252266856829, 0.19200630828856802, 0.3073531356890662, 0.9077475361304695, 0.5154858846152509, 0.04601784043177504, 0.06053868881550306, 0.06319622513871725, 0.6635219301572614, 0.39008719916349865, 0.5407945209488421, 0.9415793885440003, 0.9722197411147567, 0.3928150492386464, 0.16995164535562568, 0.7273724666877166, 0.9759506885957581, 0.7342154660022836, 0.39154698804549637, 0.7232483465737544, 0.38811892787927393, 0.42884773644527174, 0.3736304592681554, 0.9600100669479835, 0.06205548596891375, 0.10896864027189346, 0.9170096228485567, 0.8459010992699757, 0.30761449503892613, 0.8478300050877966, 0.9647192815246142, 0.5040867066448698, 0.0068913303921501745, 0.7565473646221516, 0.5325709896275236, 0.10684238666255397, 0.007417179192884871, 0.6430672917148806, 0.7237712763512668, 0.4589398964304189, 0.29410508761055987, 0.064002253159258, 0.8711615334617334, 0.537506683684734, 0.1222249452192733, 0.22544605865175638, 0.8013125029639416, 0.06918388714211765, 0.2818624562395676, 0.3388567087686565, 0.7596190644280962, 0.4712190659273652, 0.7177028480104558, 0.8837427199664432, 0.12255851492611702, 0.8129831563469956, 0.9420051180631097, 0.35642612208360114, 0.3621235958290091, 0.023247078452731995, 0.11957180385869748, 0.6274224465076244, 0.4225483855131913, 0.030309789169150836, 0.4455336069056497, 0.9465406465182978, 0.5022244553673474, 0.1226055551310834, 0.2049912036478969, 0.4021391939526052, 0.23124621798219536, 0.3903776422877223, 0.3332994207567346, 0.03580285942154671, 0.8867295314570874, 0.06857182393828254, 0.07115850835559856, 0.6378989374324618, 0.803880671623502, 0.22809229229220962, 0.753433859268805, 0.38841429438375075, 0.7918651200160481, 0.24360837779052613, 0.9056613738832264, 0.9385736606966754, 0.06622278043418717, 0.967273550043943, 0.8760773395552374, 0.048464755935895276, 0.25523539413061247, 0.5477424561911155, 0.6978624280399159, 0.6066213271536468, 0.016765486949806174, 0.20305201441181453, 0.46435402919592195, 0.03359031987695815, 0.7564039395020755, 0.18214331732560318, 0.3004878570663939, 0.2657149727139655, 0.9130666952288345, 0.10589508942731674, 0.7397472723760098, 0.7515084091075538, 0.764323700984563, 0.18536982532202495, 0.967836929909838, 0.5688021183055577, 0.5251070320784863, 0.8194794875707416, 0.711514479874814, 0.8011950259106132, 0.9161378723435716, 0.9112289666638912, 0.8958606575827054, 0.0049562508779309455, 0.037795321528270254, 0.2785606664539091, 0.7184321601778852, 0.2079440650879505, 0.8499124375902902, 0.2867232389543165, 0.4678808977142527, 0.2682457105288091, 0.13140954010224037, 0.20275686931053472, 0.32159569189149895, 0.12155759786578979, 0.16189159416291143, 0.8192989919803969, 0.35073647862303703, 0.4407366474475377, 0.3810116987201465, 0.9782518588640291, 0.5586230977544117, 0.7249555636359053, 0.06434237964775802, 0.6914896868262008, 0.6275892876706441, 0.735358085403119, 0.6306383056822153, 0.6498757854170806, 0.6322161667688184, 0.19198329779294887, 0.6516687656677962, 0.1081386637660976, 0.030249537292159845, 0.5864263281338024, 0.16889123177987675, 0.9528610414052628, 0.12506080644293738, 0.03990107152024336, 0.7597030025714054, 0.9663534063530033, 0.6958525100171216, 0.8827013476022744, 0.5919202863964861, 0.3216486856637837, 0.1551124077118442, 0.5677654654677756, 0.16876764393362365, 0.9625577025427577, 0.2727327233325769, 0.46829282643751713, 0.20074437955503766, 0.469872344966789, 0.5272350034814809, 0.9121498088157578, 0.42210111241428994, 0.5463807456266974, 0.3929346499696592, 0.8958374590713014, 0.4996363128870627, 0.18980236328096078, 0.8950813876208525, 0.015511744462498922, 0.3509555848533771, 0.43241454565843884, 0.09778093434233415, 0.4243881796740231, 0.7912184913983528, 0.010416734692408558, 0.8420202126201568, 0.5472014312988326, 0.796885383187479, 0.8461032834464772, 0.22369625782029112, 0.93831951618534, 0.151474949289095, 0.7254517676017, 0.9490667115626363, 0.6722300086994522, 0.550210442775023, 0.4835651210996895, 0.9352702379496065, 0.17636778411700205, 0.3615385021658286, 0.6411899253588674, 0.8048528948448236, 0.16070364446189422, 0.5713507443988888, 0.7769015539229669, 0.9679866863535502, 0.47001194584228023, 0.20396903177724524, 0.31565061032176267, 0.6847516518461938, 0.00573523061898229, 0.07196189694525801, 0.9618589893086418, 0.9644619415322963, 0.5181671190358292, 0.5412639482459853, 0.6211936455635816, 0.007105032475052786, 0.2879010055269955, 0.2606533191176159, 0.067745421169767, 0.5730938484939184, 0.31078426086384425, 0.7445326874434041, 0.18800641833380438, 0.06335038297630347, 0.9401895599300039, 0.9289774595609444, 0.2461191478177579, 0.2917131640729026, 0.7033682308571989, 0.6758715800520104, 0.394369928863613, 0.5788317015691845, 0.34360501826008183, 0.6418215005897883, 0.9095510885886179, 0.39246493182534103, 0.24396164757455419, 0.8729946864394247, 0.7735259195306786, 0.25659893596343086, 0.17521588187163017, 0.0008856033840343169, 0.38034141055397164, 0.07724366284648432, 0.6210018632265993, 0.08251691719265741, 0.5430373213637285, 0.6769126876902418, 0.8817843868028564, 0.36545247723282837, 0.8566117155825118]\n",
            "[0.4055057177472672, 0.540045522818006, 0.1959702396477363, 0.27280587997751804, 0.9746538345333309, 0.16721002546677177, 0.8031746101604121, 0.9800612767667516, 0.8685064062104575, 0.3373572627810294, 0.633749868259051, 0.15030479872604197, 0.8264145482495696, 0.8467083632554836, 0.25616804916934, 0.8858682159789822, 0.5306600797809359, 0.7541434343538285, 0.41553043150134905, 0.5439582994560272, 0.09555926878631915, 0.8495627788031032, 0.002668148089704836, 0.0435272853209171, 0.9707842425064103, 0.5092085297526171, 0.7206345467088658, 0.006570943213655034, 0.19631190610244076, 0.2815015784845093, 0.430481604376999, 0.9747430518999523, 0.1574593266298353, 0.6375538314964084, 0.3884288618390256, 0.08867534613014039, 0.46675018027569204, 0.8640245781323402, 0.9212569444117925, 0.5566200853873888, 0.5664937266312985, 0.23176312360986462, 0.31456905384375355, 0.42470847183529636, 0.2103302784187473, 0.8041627033344675, 0.41550689873528845, 0.32959075087048006, 0.2861655378537983, 0.10970540657252026, 0.06485484723119805, 0.06535100492194967, 0.8034450750658653, 0.019230615303481247, 0.9435492189581067, 0.6311330548062037, 0.14893672301781202, 0.7991606906934796, 0.6009625384418708, 0.9442624245473048, 0.17461970983984465, 0.8892760419973478, 0.09153028563367072, 0.901457536528665, 0.28720265433463643, 0.09342105934025446, 0.4690141484976975, 0.7076530877050373, 0.23374937022728426, 0.31100580910126774, 0.7821832134438473, 0.018439358038855325, 0.6219912061468679, 0.5290209002965053, 0.24873732475489618, 0.544403088960513, 0.8229385628270711, 0.11003405422843071, 0.7713029538130239, 0.35978301202151464, 0.04984609880337332, 0.9813722924224052, 0.5330980289577988, 0.6206286191732318, 0.1878825636635304, 0.5201289824794763, 0.3934390927784509, 0.30303838445933284, 0.9090779913771625, 0.1291876904618542, 0.6109534421363754, 0.5698168589036103, 0.3800238562416637, 0.8158383490611885, 0.1155624973776862, 0.3164788357976397, 0.01273638727251769, 0.8500178954669334, 0.7876636331762776, 0.3444417750363983, 0.3318848829617971, 0.1153485839460705, 0.2878866448348206, 0.9940422113116326, 0.3022116068728975, 0.5892874234857602, 0.7681474795888017, 0.5140759398282464, 0.9387449698665191, 0.22399414132364726, 0.23815128926954043, 0.7028405596947731, 0.6550845839661035, 0.5883852500219756, 0.9814861574994144, 0.501694097575286, 0.10796841619096842, 0.003573816444024991, 0.09292711702533418, 0.32536937944120337, 0.7793332036139573, 0.38115453308267744, 0.7895576495931481, 0.01972152946496586, 0.9214957623437761, 0.12794759906319142, 0.40836273856072525, 0.34704958236520844, 0.08398038492672488, 0.5355551733270691, 0.695740065833924, 0.39893851767589517, 0.5938052708604427, 0.11289813679614724, 0.2250248757141261, 0.42225144390615865, 0.8074643390685879, 0.7309170745584107, 0.20868535503475882, 0.7121908231618779, 0.22250543473014295, 0.08104961003172317, 0.6446382329974544, 0.42327663243371416, 0.30887069418263435, 0.14127014147130024, 0.6054226920203846, 0.5640035780824466, 0.7515837298389368, 0.03805255803639096, 0.12230377108970092, 0.26880688600818214, 0.30747145510588836, 0.9550009774556644, 0.09541741321685848, 0.563730560878936, 0.3068577073977342, 0.08338324857118784, 0.5071156884344864, 0.25877549044333603, 0.8589491238481342, 0.9320272847321576, 0.8050798825545973, 0.41335389759643826, 0.18228156752860358, 0.49989873216960334, 0.5074208812231241, 0.6141041869942317, 0.22647544672866615, 0.6922659514797994, 0.7294991640996245, 0.6981114470777301, 0.5337183291974442, 0.008650054220412429, 0.38084443511003696, 0.8180006580654364, 0.6586494868397075, 0.582499636815135, 0.8242196181528032, 0.8116330539407889, 0.28117495662711034, 0.8986296546887724, 0.4803503451180263, 0.5006181786088557, 0.3874385810737363, 0.7756922579544675, 0.6677232850908776, 0.39793834727244226, 0.5241384793961864, 0.04769137433709181, 0.5837534160120866, 0.5669823090588064, 0.4058652746399328, 0.5956528234112007, 0.4079418773304222, 0.24245898917338193, 0.48663602360047953, 0.30045915859324945, 0.5202181061627159, 0.9940473878915692, 0.33707970453495795, 0.11247412715327598, 0.5453874950837865, 0.4385938747694089, 0.7338871136782762, 0.46422585860593146, 0.223755343449761, 0.4197080943746103, 0.4510403313151743, 0.2302071023532678, 0.33913287233904066, 0.38410681874275954, 0.918133711749088, 0.5787535288183369, 0.0467133120798614, 0.6978184034347928, 0.45398449743884006, 0.4177753291138431, 0.479144519788989, 0.16052113865844053, 0.24151586468537234, 0.565134003474222, 0.5000305896633088, 0.7614065785092615, 0.37081421986754015, 0.2726818402554909, 0.29815157153243954, 0.7251547150825699, 0.7320022448867902, 0.059009219138427826, 0.4100180530631933, 0.6408786644414314, 0.7432032767245745, 0.11280610461871432, 0.5731231795578383, 0.1904104474191909, 0.36303232751485714, 0.1689353426534812, 0.4895764633506451, 0.4056663018765271, 0.1974206343580931, 0.6003932987219419, 0.7639165383511827, 0.17302010375660437, 0.06292608176055892, 0.6879342784009939, 0.6901449728711063, 0.7161919081451227, 0.2937747095046368, 0.9894369786277837, 0.604930673154014, 0.0502090593985457, 0.5088519928939145, 0.7532633305714537, 0.9469461946383955, 0.9101518130588184, 0.4723037441951299, 0.16814901776842794, 0.6278903178179956, 0.5751278142357528, 0.7490112289494966, 0.3754193438439961, 0.865929127459765, 0.6031195534394088, 0.17663028000110126, 0.6521227002462701, 0.8748441869465848, 0.1508979610038046, 0.9696492852770874, 0.5064264558202611, 0.7903988729706393, 0.0706490745766909, 0.36437939026481425, 0.9976918517736766, 0.3350818311272272, 0.31614301493921293, 0.41763099865726183, 0.057646004885071545, 0.01255715239952171, 0.4992921321404311, 0.6360727200822568, 0.4292544063287821, 0.7368354698846795, 0.7799234867532088, 0.2928587460240598, 0.3134289998082842, 0.4701904026798829, 0.03820786612938154, 0.5605077655399152, 0.22090452222260548, 0.7378911869495607, 0.7121759267481149, 0.7878656733053162, 0.7525697998051731, 0.546664260443336, 0.7541901238080317, 0.3380714708736495, 0.6863926287187522, 0.5845437057234648, 0.9628909297546804, 0.3641246925875098, 0.33308555921571403, 0.34918067130353714, 0.21183141068758982, 0.8017455946840437, 0.8273830668497958, 0.7883586968331897, 0.8238263536626754, 0.6917475504793746, 0.3808924634605756, 0.12197857550653823, 0.3857805639191024, 0.0002075928032139185, 0.24317507436299413, 0.7752505195182566, 0.5662053613705728, 0.015076830080796855, 0.6419138708490008, 0.3379567220587629, 0.1266794509343917, 0.2267860260794371, 0.40883087126313833, 0.9851868325256066, 0.7404263270881029, 0.012517633429031338, 0.24011627309528738, 0.05169680304915425, 0.2887414685492966, 0.15880138063431315, 0.3836237915691231, 0.2094231490169033, 0.0077374440486817075, 0.7790025270524741, 0.26712489924560257, 0.3285101030063603, 0.09065734238918854, 0.39412990195631514, 0.4334640208099664, 0.6481012645379922, 0.8306486616227736, 0.40554133169091655, 0.2988647680460893, 0.9041225460784164, 0.15401491407137546, 0.9479993292120584, 0.8399824191216682, 0.12835329817964647, 0.11156304019005236, 0.9271417924073274, 0.6274954520495261, 0.9744358387638166, 0.5946406501035733, 0.30974595700634544, 0.6686831302605744, 0.4102957191558564, 0.6515894288916502, 0.7466676705666713, 0.8866339768512363, 0.47999268364602654, 0.3197226010110955, 0.047207219641273945, 0.034225792341118666, 0.35970514897957706, 0.011661077559119937, 0.4545488698486463, 0.20701863235610085, 0.22503452000960433, 0.8384626420315261, 0.17046726115139466, 0.2964419938052242, 0.38478885810665286, 0.4657637524252338, 0.10956618330663936, 0.3400567171584331, 0.10067955889319569, 0.14901133762232188, 0.019728476507586468, 0.9757765739282772, 0.2587124603997467, 0.6350501820120514, 0.35483489031129056, 0.21671557267316732, 0.23092801579708933, 0.13192303498322056, 0.568470406609896, 0.5922879933665711, 0.6905704757064411, 0.8016916243554838, 0.908739730249292, 0.6197419851478809, 0.060592119675525136, 0.8302432120558507, 0.3018137009667978, 0.07243941837112156, 0.17447443700837373, 0.583058867130113, 0.7414026119553448, 0.015332362195816351, 0.5573696042952909, 0.9219193937426192, 0.8565110715984848, 0.5762916011048869, 0.22105990571995005, 0.0453347873876796, 0.5967519183542027, 0.09647934285603454, 0.6739002294186389, 0.6395592720598583, 0.5954369379260598, 0.7114075142719615, 0.9797037175732025, 0.7263184182746878, 0.9490216041668723, 0.9603457011287264, 0.21182019159478027, 0.7062639976395686, 0.040057814770971145, 0.6877475311699709, 0.41007106927657555, 0.7701333354811399, 0.7774421899584301, 0.05782581463529024, 0.12075411392991853, 0.8439377747939596, 0.7220329104090943, 0.7972826520556636, 0.4706723866231566, 0.6065279550004709, 0.18429493458735935, 0.851498400196005, 0.6199178942774727, 0.5615287500613256, 0.9784901349501195, 0.568381572255151, 0.33988437006049543, 0.6291365935008809, 0.3239596625012111, 0.06870654480951621, 0.7990516587585772, 0.9950727387006517, 0.7566767770489196, 0.09424706189378129, 0.05723003307679053, 0.8807316208449966, 0.4080283542903348, 0.5874356684227202, 0.46390267336184765, 0.0811650465890218, 0.4845767110438747, 0.026400120558444984, 0.6798681057472789, 0.13130095671371056, 0.419481542454171, 0.20915332665407171, 0.6261962984293302, 0.022686425460725745, 0.9179994354582636, 0.8738932703737466, 0.7226118723622271, 0.4106385008342479, 0.3686945887620914, 0.26059448169438393, 0.3982710143673187, 0.14462509324451922, 0.05154394884644331, 0.6872891323969436, 0.38207105813442155, 0.14822073239900924, 0.5881340985559261, 0.05045441476274315, 0.25838742130465, 0.46995236576354193, 0.29508558542830754, 0.361012259383761, 0.9912486641391711, 0.6241820688151645, 0.9869860268858404, 0.8800138179297025, 0.40722659665091043, 0.3148137183346289, 0.21332789058190582, 0.0960916968031329, 0.8912640063185698, 0.27938615493745544, 0.6904250011015677, 0.1860240445089888, 0.8074927086795785, 0.23199207908777675, 0.6123267113158924, 0.25138889120217056, 0.43235780610203134, 0.7811742486109365, 0.41203346421127995, 0.8494515231093275, 0.6845013903958128, 0.19862833990946327, 0.1038196607037426, 0.0900989478854014, 0.38775996140400604, 0.3068725047573603, 0.7397879295980329, 0.24430973722743354, 0.030025505065355285, 0.6750201092429109, 0.6904418782582364, 0.5103979403773896, 0.5125052553765873, 0.6977125317723105, 0.24688926246934284, 0.1379993235311826, 0.03516346401491244, 0.803280011099384, 0.5767747944922766, 0.39501369779197504, 0.8418012872879421, 0.7810073117840765, 0.9429958600321756, 0.2501739195069158, 0.3580571585710429, 0.6760485384810102, 0.8027287937657581, 0.11135509685754463, 0.8957472606610739, 0.8300296628512012, 0.35340401762379325, 0.59904216443475, 0.28859823563494647, 0.7674245963145344, 0.6734320125739199, 0.8358961582021293, 0.1278944980813791, 0.32892270249662825, 0.9994117515238167, 0.2748503728085788, 0.13231884641928615, 0.152733684127751, 0.12164926074182669, 0.004708519614945228, 0.13599225835448792, 0.04636373949501893, 0.7489037896811741, 0.1300737763136739, 0.17833795696757992, 0.8247752518504372, 0.2293416386472028, 0.21127843338889984, 0.5204785157035939, 0.66835495099628, 0.4618149125544022, 0.3243119583258681, 0.7214644670213985, 0.025730511989276006, 0.6783005003880366, 0.12086112375658808, 0.8668057325103551, 0.465699400135522, 0.25431440714112497, 0.601224618183125, 0.2756696607808077, 0.8677241812448981, 0.5229120639196303, 0.3588625725606581, 0.10489030408199729, 0.145370074327128, 0.4099113791795018, 0.34346251731248967, 0.33195425089621133, 0.6925659378298747, 0.8214267058724511, 0.6395818552267809, 0.012996991856642381, 0.22863063582424414, 0.5657988662721365, 0.08881816760648065, 0.9909062747517301, 0.9727991308339224, 0.1001980532599982, 0.9443303694629502, 0.5437134750574711, 0.618523687226761, 0.7689461236423459, 0.8383463243759587, 0.7669795487883346, 0.7163303031062132, 0.9147877365048254, 0.00916742495858558, 0.8649797821080121, 0.08254461321346307, 0.5717174667639048, 0.3612919486669549, 0.10269183699998607, 0.856170252560751, 0.8392477471922538, 0.9141862239886277, 0.7472826565281964, 0.6956248924546488, 0.3300836511452421, 0.8507411684663382, 0.08220864009230633, 0.18521836985376627, 0.6578017383746666, 0.6164739271434297, 0.5168526829805357, 0.563706227628832, 0.03290253280740729, 0.6712956609683466, 0.08652929832637823, 0.5233059551235584, 0.4827338203449718, 0.5556429328203337, 0.6750631570593562, 0.9300959099470109, 0.7901133547760499, 0.5699562103201063, 0.34585373654468654, 0.10225368169478521, 0.4191114350338264, 0.3376823469743424, 0.2546184352619689, 0.6437318383948092, 0.5272924073170843, 0.06065015342875757, 0.06052848004242395, 0.31604221401667987, 0.23026042112827427, 0.7555469452656726, 0.9230693436674763, 0.5965757780315727, 0.019446216724195775, 0.49310479033666943, 0.13767317257879663, 0.2658706300125008, 0.75846410603671, 0.6949572665178546, 0.1009921940192533, 0.3076455142936667, 0.030694637126833624, 0.6916941701401589, 0.5558703990358123, 0.6537938562147828, 0.6218080294062088, 0.6684133569586286, 0.2573530351066925, 0.9652501891969015, 0.27379036441845084, 0.9266914417145697, 0.5472116273483846, 0.983904571433771, 0.7393887945780903, 0.6250565740269571, 0.01493636592868397, 0.7480495230489366, 0.4440399021020963, 0.9406867172369323, 0.16292525044826434, 0.5993850659558388, 0.7034376226585782, 0.16069585241785644, 0.9355514225624162, 0.5471106504266037, 0.03202362468581732, 0.6740656478079871, 0.3054475182566211, 0.023027495740441606, 0.6687744371598181, 0.6894380179819721, 0.07028313769832595, 0.3181064489358033, 0.37687523925177757, 0.8345402185155993, 0.8890938824861102, 0.3661960250187075, 0.11320371566309062, 0.012345639204901948, 0.45620364220023735, 0.9863471102445412, 0.7927600813077142, 0.3413334472421451, 0.2154392002402331, 0.6577143433354523, 0.7164613346963555, 0.9624827713555196, 0.40470284350457264, 0.006054646410836861, 0.08005836912163244, 0.8434024609903421, 0.2605004970007677, 0.19381520207755087, 0.8029667327035998, 0.1078348520334288, 0.7307046097024742, 0.5119920114761488, 0.5802415451499903, 0.38637879954297627, 0.6967613584230771, 0.33595865235648514, 0.5703025482477648, 0.6523141199817346, 0.4889621434302891, 0.6532408016740751, 0.8726115328569455, 0.8004824941033981, 0.26596715313396624, 0.060593946128281284, 0.651821711151133, 0.46276645660954174, 0.26462631731906283, 0.5122731657741786, 0.30464511285062745, 0.051377527642223586, 0.75890920140524, 0.8936538417932276, 0.3998646191334463, 0.6721740669101257, 0.9884427397133946, 0.16243066378108684, 0.8730740317004928, 0.4526085130457964, 0.8707066652838036, 0.6585593519322788, 0.17997987003170746, 0.7338516600669385, 0.4947525179344886, 0.1249084056622326, 0.43940727045132166, 0.1606232612266434, 0.5059657241807274, 0.8176927252174433, 0.16114633041528292, 0.9076189171848021, 0.7600718914252349, 0.09687948585605877, 0.5866969616073279, 0.6294904156566966, 0.5496720655434102, 0.078397626219877, 0.6699886114890231, 0.19546186922290942, 0.5018464533453871, 0.2978417101475691, 0.8147108695666009, 0.1973223489108823, 0.21803249855713336, 0.5975796298431924, 0.7842650831668737, 0.3319117431163955, 0.8829920548745368, 0.678953760611371, 0.9843747601507309, 0.8009751399211374, 0.12873292151176285, 0.6238286782121438, 0.16437011202435292, 0.3761485053800281, 0.16198690850955122, 0.2083977397416218, 0.6860760523274652, 0.4401460021482616, 0.04431446766682323, 0.31842399662410537, 0.7443956008150224, 0.394759027331255, 0.07423924630801282, 0.5507555438466094, 0.8591206268679163, 0.7873653791233723, 0.570110235394329, 0.4474425105769011, 0.12873511119804248, 0.6522300644991719, 0.20234554588881715, 0.8955304685391263, 0.30481930136797364, 0.8726703627668914, 0.9387153262686994, 0.3560721350696411, 0.3506570719585538, 0.6048353635473503, 0.216636482012165, 0.6607899077293174, 0.36027411347542826, 0.4643263475248145, 0.9263218708345267, 0.9413345828909285, 0.986770743387643, 0.5455490456135147, 0.6574936294468984, 0.34181572958055284, 0.4097477226004037, 0.8723559545757555, 0.4499898306796599, 0.8277222092489199, 0.0711046371219568, 0.3947735458557078, 0.41965094950938664, 0.08804325454435624, 0.36832732185715367, 0.6019164392366131, 0.7304859995137676, 0.12893319704978579, 0.9568719327453399, 0.843090068487194, 0.729153817702277, 0.2345967432043986, 0.1534072811916346, 0.32361097550187023, 0.6318431032136599, 0.3829659254329313, 0.9607568878919616, 0.9232513853001716, 0.21989071601615673, 0.23687844250548684, 0.38950226505479935, 0.5101796144564072, 0.7844530130407942, 0.08553045425615569, 0.8714091164842563, 0.6896032161622081, 0.5914664315622487, 0.44783539625757185, 0.6208108641893034, 0.5769945511476979, 0.286085460800962, 0.5429395572159247, 0.7708532064541372, 0.7341984740184186, 0.7100773146164755, 0.12505033624361006, 0.18226141910181481, 0.018586873369839663, 0.5488769992194589, 0.2756057843848523, 0.6188606246220253, 0.19447371957571913, 0.016783115567560625, 0.6284084835786106, 0.2996406064368087, 0.28513056157305183, 0.4957791758574539, 0.2205187391425274, 0.08435140415414366, 0.6673155369870338, 0.018672936162905995, 0.527843411063244, 0.9859128901421279, 0.43633098018048533, 0.5988922105142098, 0.8967755484043831, 0.7177493411668449, 0.6398705129123496, 0.455545303858891, 0.40627050923391717, 0.4891913031271625, 0.19544010536303835, 0.9160324614593488, 0.574279712140287, 0.2505096651583234, 0.07789332868062093, 0.12760740942201976, 0.4144325052232849, 0.36200792785864977, 0.5538042965143479, 0.16982498688580205, 0.48032854804847724, 0.7575957235180123, 0.8032622947898336, 0.0526294766650911, 0.2175342457536361, 0.025734951081849866, 0.907361322855013, 0.9619362254747686, 0.4461000436677388, 0.8900264757967357, 0.35344374091973074, 0.23929908033270808, 0.8579840501380238, 0.26045178336954966, 0.7109891799471348, 0.46403349588146836, 0.8521546397652949, 0.9876232750373896, 0.6502352910282135, 0.5064689526800338, 0.47688860124898036, 0.8460497336926643, 0.1630865317928012, 0.6027016367218718, 0.7287556231306259, 0.15396126340457783, 0.9091767394592077, 0.2580786157203986, 0.4790625443907356, 0.12070076736248614, 0.528649046116995, 0.6779954801254576, 0.599238315012255, 0.8425836049688383, 0.168094557733508, 0.5123108567259639, 0.0765854174538948, 0.623379104626125, 0.5801555420828856, 0.6623618596073311, 0.9317044727705777, 0.7970236721565831, 0.8569858475454342, 0.775065603803739, 0.5959954978913273, 0.19296330719739319, 0.9344734527514964, 0.2120955573314648, 0.6113705485694652, 0.6027362097930553, 0.39847799168098463, 0.8922080759278891, 0.0858381454424969, 0.9803273965258641, 0.7078613873222007, 0.7829367656285722, 0.11455523829668812, 0.16129627764568566, 0.987938735259576, 0.7847345616173133, 0.4018285910773902, 0.9411973032444303, 0.9583747701615647, 0.5630894231334792, 0.3143629080704945, 0.592662751894017, 0.03016702618944689, 0.024403263925998786, 0.7874267687697722, 0.4439701266329197, 0.2596084125913375, 0.3488469881873364, 0.2899828616793727, 0.7057169617249616, 0.22992418980815643, 0.5326508014260098, 0.25566583542769017, 0.15071008806460273, 0.4233737889476308, 0.0009475322790136742, 0.34056103243582003, 0.9788363801212023, 0.9286000507855152, 0.17627498305150702, 0.4591667035785295, 0.6119459338928004, 0.0844015822420856, 0.1334551913903944, 0.23381999981463553, 0.3339350514004066, 0.062117253627921754, 0.8668979354589361, 0.2717873367998117, 0.8779865013421887, 0.053362979417561185, 0.35814456959173924, 0.695493959035453, 0.022391955060086066, 0.933615952888406, 0.5028638613190605, 0.07587445963626704, 0.6762344512612556, 0.9348590242358412, 0.9179690604450822, 0.7052632598857532, 0.14649486365725528, 0.8762039922468162, 0.06544587137665636, 0.46837989054088724, 0.5439054503669631, 0.6888334283159842, 0.8952825801216827, 0.24705845021420703, 0.704118418119566, 0.597008342050394, 0.2900694928659817, 0.6433370347427801, 0.702006928121536, 0.12592704289597445, 0.6681501468038605, 0.697143566249103, 0.19700056327647697, 0.7879048595874429, 0.8843210204460574, 0.41770339469396656, 0.7611439343650591, 0.4617253420266646, 0.4038499438743389, 0.615107118252974, 0.5851516596418053, 0.6378327035814194, 0.877830292850119, 0.49619150047682936, 0.20294289239382168, 0.7292685360581507, 0.4550398465165276, 0.15406050976506436]\n",
            "[[0.22103912674625525, 0.8111900042630228, 0.42951426299031836, 0.7469652375524678, 0.7979359623540457, 0.6536373836439845, 0.07201120040511122, 0.8507641059773695, 0.48162443572602454, 0.9673584540752799, 0.5370721778609542, 0.2764988643505668, 0.614662018594901, 0.5623430429941367, 0.6409426282518532, 0.6198776417819745, 0.4870239703384154, 0.7702083954520236, 0.9580320447061101, 0.09508488900109491, 0.0993189241309731, 0.14947398785596466, 0.6497678523724131, 0.4523989180580935, 0.9941709438671489, 0.4192831902086438, 0.2029946712451045, 0.48892695493552474, 0.2644535556546841, 0.3674780738563771, 0.19279225122364307, 0.7186995808470215, 0.40631395955456406, 0.6651365550099916, 0.6289820247829946, 0.8891568314826679, 0.8175679382448483, 0.5085360703394999, 0.6681534105703026, 0.24730396148493516, 0.8394046025844185, 0.27963056952858434, 0.9019917096696914, 0.6834029912109786, 0.610150012036229, 0.36274289198627496, 0.6956694768121002, 0.11076053096730687, 0.07574478456719702, 0.583495386153743, 0.8277043337022649, 0.9734881492836075, 0.13962006598358323, 0.5789186884418022, 0.5135143954252981, 0.5846192258056293, 0.2956574183726247, 0.21263604710587247, 0.5006929119688778, 0.8320486480813896, 0.6436249858334131, 0.341636178361036, 0.3090157136987345, 0.800856286686457, 0.2858944450290434, 0.9850760390000141, 0.7727628371970848, 0.5829471249523998, 0.6893849840226755, 0.8539113470299947, 0.6817899176634633, 0.1869551187741385, 0.834004872024807, 0.04507918431278135, 0.0807645698654208, 0.6237559570143847, 0.6781090105631913, 0.9019495342564238, 0.5630189778591199, 0.20892778029088055, 0.41188266542599106, 0.4967412560463169, 0.8872491981654159, 0.3453791370846869, 0.37878942410077154, 0.26057425003205437, 0.06218089006605487, 0.01905489496171331, 0.005009704353606037, 0.6490689056464117, 0.2362353880372574, 0.775045266137764, 0.01455085875790807, 0.5169296141405103, 0.7308704593180905, 0.10445077661523305, 0.7770435000172137, 0.0211390435983273, 0.0591450562277438, 0.9804737696038958, 0.9710647934103506, 0.817108378881203, 0.9171830111192955, 0.12071918309984675, 0.2343843522030835, 0.05414219758256833, 0.9570273544675788, 0.38493939673991806, 0.2898676153740851, 0.6279346410369557, 0.37010949677099814, 0.47134895528929965, 0.9549225056562952, 0.9411034090368441, 0.6656714861948121, 0.31572258695204203, 0.4091088422739917, 0.4960831701523588, 0.9323220264688661, 0.015505005371738934, 0.4106736667677826, 0.9729279068357328, 0.7469782623404646, 0.31590482166836176, 0.7052336406976385, 0.9442269024421602, 0.08260776797383251, 0.9171048497829355, 0.2950348134291719, 0.5219823412642809, 0.09553136027320897, 0.29202720409741634, 0.5994402299577554, 0.5924591581573991, 0.9875130963835378, 0.685457906805213, 0.5035387787625176, 0.07108706475058524, 0.06131590208239257, 0.23196669187230157, 0.0533009098625864, 0.9876828047737938, 0.8953601624699701, 0.3643445769898639, 0.18665018002595635, 0.011803174933725735, 0.2232431719852871, 0.016560122064046334, 0.8894922119973248, 0.779472417666516, 0.45116311810297915, 0.5169640513302226, 0.46935549768021245, 0.3812485454656478, 0.9901701523424052, 0.6594793486438643, 0.3000330350120316, 0.5368206389310268, 0.20369558796677623, 0.9821657899112646, 0.7751729478745217, 0.6886873039166755, 0.7420468613029666, 0.6659855140556318, 0.4170239817366883, 0.07810473747013591, 0.9240279527845319, 0.9153305868950286, 0.6165639616892881, 0.8754919861072094, 0.346639181441425, 0.7027465713134695, 0.7449489678937677, 0.20080808285059304, 0.2782208022525273, 0.8735417402742093, 0.1560052818509593, 0.7212017974242486, 0.08571680092500011, 0.03633706664835168, 0.7051284857810262, 0.593333395197892, 0.9995679719606146, 0.1381655062792001, 0.021603109693013045, 0.24646902517004832, 0.02890837972359661, 0.15479451039041303, 0.21786961611018474, 0.35658058111784996, 0.4415058510232239, 0.38036001599739766, 0.43400692200540314, 0.6780964448753074, 0.2865734047899279, 0.4421532268735634, 0.4220215585399284, 0.7487449729109392, 0.4604820098159915, 0.8727003718090691, 0.1842651439754106, 0.8392959031084398, 0.22506068096280474, 0.10348533975660767, 0.6807268363575099, 0.21747279158158073, 0.8518740724310474, 0.36559384507665704, 0.2090236430408462, 0.13717321150895045, 0.46694225378084453, 0.7908302876204146, 0.7770817970378033, 0.6896815071828815, 0.6161942450359076, 0.792385251927385, 0.07061286900104591, 0.8687341847450756, 0.4282204919561875, 0.471552100163942, 0.9994986018623357, 0.8938430576829656, 0.12127859880914149, 0.29063503117151845, 0.6040632383385101, 0.32673524221192096, 0.9908454427980734, 0.5881233376837314, 0.5488418348700916, 0.7846754709363577, 0.3468396799285749, 0.8151695358916028, 0.6544542685309493, 0.930015242140173, 0.8691119072388134, 0.35859306055117346, 0.48287578849535184, 0.19660509443758079, 0.44006694751806685, 0.7532966686889837, 0.9120784652915193, 0.6325592665206765, 0.6452115920057965, 0.44174974076918605, 0.36078982358392664, 0.6204647349639503, 0.8009028078178388, 0.9756871724997992, 0.6303547262502435, 0.24771378000268995, 0.8264678964591243, 0.12683119155037836, 0.24171420821043588, 0.2632308328011228, 0.2559153116466616, 0.7414070971630438, 0.3917549386063237, 0.7301910766276495, 0.9992934880527558, 0.86677171817462, 0.48624936987003575, 0.554644609007801, 0.19575099428466636, 0.2490922687057815, 0.5450130163117278, 0.3410091211808073, 0.17257836751973177, 0.0130586252442082, 0.3033779082405218, 0.959777907131499, 0.6158814393900229, 0.37569349705060473, 0.9710972299744295, 0.9557095144353198, 0.3544806836328279, 0.49040251420613346, 0.5141298709898726, 0.6287371724816011, 0.6048901442679656, 0.17107964417507693, 0.2182260476488711, 0.3599134913403147, 0.9211701291823132, 0.7677246611945072, 0.1995693358401579, 0.15572752420555225, 0.03204891819408362, 0.8697084179816341, 0.6513459620409767, 0.6067803537313825, 0.9573588412649686, 0.33656973251217426, 0.6876552685291011, 0.13626343078634684, 0.5825800047805135, 0.9330741531278789, 0.4306294286038338, 0.8226001267038058, 0.7702112108331253, 0.5703709626262959, 0.5438655140492347, 0.742767200237467, 0.710586767870287, 0.4259762742433153, 0.6755377784147991, 0.03759148041042992, 0.7929390934376143, 0.06294252811410361, 0.5484829688494517, 0.9632136372768376, 0.33846393515975937, 0.607290652070273, 0.30848765191964445, 0.1073811642730228, 0.9584768199830103, 0.8031335111996362, 0.35421756701207374, 0.4842800285351938, 0.4123758924011125, 0.3997718524091364, 0.7786678638284608, 0.18075269387836634, 0.14916408458902897, 0.5700134390784323, 0.34764019553362624, 0.8805846500437261, 0.6986137384067126, 0.9017113089224322, 0.05480974161006069, 0.7411104344790785, 0.6080612489444649, 0.24291820071835268, 0.2155241163586068, 0.9529384841642207, 0.026777464776063464, 0.18418683348837472, 0.08678759576402428, 0.7969954255585435, 0.5607063039701555, 0.8766789546366938, 0.010705858992222539, 0.20366471074053993, 0.9346879068943649, 0.12376756979416748, 0.18816644254078496, 0.6836261490297284, 0.34341427395776714, 0.5579773026590239, 0.09716056442960197, 0.23308956084568844, 0.4513090911410641, 0.657813251301382, 0.13834062575498707, 0.16185836051344937, 0.644467998553535, 0.6107212154371839, 0.5781828025111444, 0.8850194782483777, 0.7279084537304513, 0.28779158260032245, 0.26882077846096974, 0.5207235858433491, 0.6151814323057649, 0.589604528074291, 0.9878209642278217, 0.6420837543254991, 0.7011127847587115, 0.6578810697887715, 0.13682901330106245, 0.12188908410747512, 0.43580436889210505, 0.2946556472749299, 0.11677954143161429, 0.8253759333005345, 0.6143964680795835, 0.4558300236674818, 0.515769495511947, 0.20052909339083913, 0.024950832018199143, 0.2268883861691915, 0.38857073174147827, 0.5999170199460846, 0.5855362503198795, 0.3101834802754173, 0.6917583609472449, 0.5907092941575802, 0.6441143220899739, 0.7189236189471642, 0.7007224572599257, 0.08746186351500562, 0.5327886592429653, 0.5494532531782015, 0.15762740277249077, 0.4526539114314545, 0.1690920850668447, 0.49911627255976965, 0.07069850798349109, 0.5609965856884815, 0.4507069477643614, 0.2054976440990196, 0.8709508128772104, 0.7947338330020295, 0.7960225083790698, 0.6420126951190118, 0.37970498352504145, 0.6570545854209047, 0.04736675905858567, 0.6711037372980048, 0.9795313461861574, 0.8966446088023423, 0.9726812379954103, 0.058810444328967315, 0.1325971762992637, 0.26826772386539577, 0.6739340687742597, 0.5312429839000495, 0.358622744174219, 0.9486283550682314, 0.7121530071737475, 0.9786254622404259, 0.7129600594250382, 0.7515492601450531, 0.10200514994395571, 0.5698239588079591, 0.983569103722505, 0.5119337345597044, 0.5009154467203285, 0.16668414511122853, 0.8574322216594626, 0.6651008231184512, 0.18255519588476576, 0.18304941371337635, 0.6359209174932966, 0.5329863930073735, 0.10367744188977535, 0.03587424942706263, 0.5754736090801489, 0.2873388592198154, 0.3954227920392791, 0.23062657719188373, 0.5215567890237832, 0.8194098459240976, 0.9785270700133194, 0.8347161549937182, 0.06870265345159099, 0.9949844734836858, 0.7970226883767024, 0.20310231013618651, 0.36928098467294135, 0.6227581739767474, 0.5000814145182936, 0.3481340616047258, 0.14183888800076416, 0.342124377247377, 0.950347038983807, 0.6675454597076419, 0.16081901305641977, 0.15310931878395118, 0.5473807637460003, 0.7595840356693971, 0.4513347037085841, 0.30083838907152105, 0.33160963195553095, 0.7045723615759838, 0.5896939954567444, 0.1232792822215486, 0.269596288202709, 0.5171761367584721, 0.008640791801551861, 0.08679968913029756, 0.09734295601496556, 0.5578938926497711, 0.3434813326758551, 0.48620592580884636, 0.3270761520465172, 0.2692332005664314, 0.2381802414757922, 0.7335478183601795, 0.626188084370463, 0.009453860225817734, 0.428047788531457, 0.04331064652336902, 0.9225045139148663, 0.6616081623649904, 0.8253346252386404, 0.2508016594471587, 0.11058353747504723, 0.6626260730461645, 0.21813687788438252, 0.23731616223398788, 0.180753040553027, 0.7493318097937411, 0.059108821298735514, 0.5766626761685948, 0.21507436801817503, 0.22367511928438555, 0.17425019733900227, 0.5959972077224781, 0.32715944657755613, 0.2719808515921194, 0.1273395761444026, 0.728145335839452, 0.5356466113101257, 0.2026627530863987, 0.8569716968683811, 0.737217351587602, 0.9368327215081078, 0.99179850993871, 0.8500096294911198, 0.5908812164536726, 0.8782513534882442, 0.16476103929892072, 0.1421575576454439, 0.14236377947700596, 0.3589686603547835, 0.25288387339480056, 0.5598050536534493, 0.0818707990426012, 0.16272579906760787, 0.22731808100684125, 0.40295210014179517, 0.6762832822084294, 0.4424045070278999, 0.8549775110935559, 0.18654747650022763, 0.9505770350509355, 0.6720087242325685, 0.7648104603377346, 0.6977642945269018, 0.16587348759202203, 0.7174038640074459, 0.39924587794369504, 0.8292828226003399, 0.7853898692982674, 0.2883733476155529, 0.7090457286572698, 0.7086833849657413, 0.7331065696014712, 0.9055414660749451, 0.7812110405724049, 0.20025089539930296, 0.2567061094942362, 0.056021855936944975, 0.5170861897042364, 0.6652194881836918, 0.07112056839300673, 0.9157776974344926, 0.2753582473748233, 0.5036291564693604, 0.5165881412698529, 0.2697407776735131, 0.13784431692761923, 0.9812120289113027, 0.4270307642333936, 0.954511241894639, 0.8148547816573318, 0.18581235220755554, 0.46613683475660117, 0.41965970514113204, 0.011785734663164571, 0.9143626110706748, 0.6596818197938866, 0.4741504571013745, 0.8291505197506557, 0.6954695387804356, 0.2798145491817077, 0.346540262655479, 0.5001415883082466, 0.47489008480216344, 0.08693989419805659, 0.15166999476602672, 0.5141591205103709, 0.48328992765405165, 0.6329449128149843, 0.4292406710313277, 0.5103353670972127, 0.4377099021194132, 0.6781133632467223, 0.6777379704645072, 0.6696534292184478, 0.9607401486615063, 0.976723126647542, 0.10714873752465315, 0.8061467892369041, 0.782945060379704, 0.35390144389782363, 0.6649520468863779, 0.7174883166946048, 0.262349844080354, 0.8942379423244092, 0.5726713111864563, 0.3117423182069621, 0.9303987291852291, 0.21915724471031106, 0.7219253855992978, 0.3361332226082434, 0.26053592347767396, 0.07117734437591172, 0.6778364228496393, 0.4535052407282476, 0.18300668220120164, 0.530599566425599, 0.6840360138213053, 0.9734922765171731, 0.4176591802345907, 0.3617768518065033, 0.25525802357735894, 0.40269138011972405, 0.013530449346710327, 0.8294163482519165, 0.8763069303792506, 0.6114436440850204, 0.8892160951805484, 0.8680965061953393, 0.9173796415317592, 0.9282746462416748, 0.9125142398733772, 0.45873309481867297, 0.40107202157297583, 0.6471578398742169, 0.5578612002655083, 0.4657346557048171, 0.5217979292888757, 0.6300881121608799, 0.9483479644871142, 0.5692805328603099, 0.7105152355120767, 0.7254531863867859, 0.725307946688445, 0.9811993597182497, 0.9575050265055409, 0.006018028511397766, 0.07031161337918879, 0.9267913420623413, 0.47804904716610197, 0.047983920091837606, 0.7641751731247141, 0.4837975045062406, 0.5761497708263816, 0.3160717451616616, 0.5370682768377494, 0.5398958040464561, 0.2142621633008407, 0.04414129252837329, 0.5641437240984544, 0.568702295478442, 0.8088863826788268, 0.8511566502509789, 0.29641266960793655, 0.9588645044768901, 0.4562495392759608, 0.041623544799176404, 0.13001203319726884, 0.38386061378005554, 0.045271691540960446, 0.3345454299747196, 0.1344247541668332, 0.6370005858090193, 0.6537072494316049, 0.27898082842206595, 0.582616653956922, 0.27595218070890837, 0.4450464004066246, 0.23539120616618026, 0.9864554479973738, 0.5815885707497914, 0.6940145445282838, 0.6574758366717591, 0.6208987453989206, 0.34046117130014664, 0.5885927201671275, 0.09571836212910101, 0.43237753568099146, 0.29751743683948395, 0.03554056914702752, 0.926205565237621, 0.807332802766471, 0.5847089492292887, 0.48475210031206906, 0.6979859947747417, 0.9033441517211286, 0.7215728511833184, 0.40870623911775095, 0.0923692486336889, 0.9045573978647629, 0.8772250100456854, 0.011124156763532067, 0.4587322725686268, 0.7411865259023391, 0.8350685633215226, 0.8706409233092064, 0.9074873988449815, 0.20210691363161915, 0.6070184336991323, 0.09222595655065535, 0.9549366911225088, 0.012615225716585354, 0.6216505184906639, 0.4989245761674743, 0.8541636221451864, 0.14546480163688646, 0.6384724659291385, 0.9064461043132799, 0.4283772285635471, 0.8418022674617376, 0.05879634634239983, 0.2145765860753479, 0.46573683844193936, 0.19243657473393838, 0.6931590926006685, 0.08422812267766178, 0.843302524671135, 0.019604440950580226, 0.508352460707797, 0.291705342130406, 0.819146536951836, 0.6311148256617425, 0.4084791890966373, 0.0924042065091728, 0.9530699637428814, 0.4438640625123845, 0.8849251574585029, 0.585570920843536, 0.4277917004899534, 0.9416165657588469, 0.6345885755655489, 0.728774268485828, 0.0093229383098411, 0.47573173469606356, 0.9715701884139287, 0.11447464655712813, 0.6826679937889525, 0.7209358649762365, 0.39161598216382143, 0.5288818498743052, 0.9786237542436418, 0.8501722929588108, 0.44436504734299975, 0.4800500460512599, 0.0145744808568683, 0.8833251324884653, 0.40101814842680217, 0.6321006412430001, 0.9281024134641103, 0.37102163544909317, 0.04381304485304893, 0.8947599351387711, 0.37277296643549473, 0.980252266856829, 0.19200630828856802, 0.3073531356890662, 0.9077475361304695, 0.5154858846152509, 0.04601784043177504, 0.06053868881550306, 0.06319622513871725, 0.6635219301572614, 0.39008719916349865, 0.5407945209488421, 0.9415793885440003, 0.9722197411147567, 0.3928150492386464, 0.16995164535562568, 0.7273724666877166, 0.9759506885957581, 0.7342154660022836, 0.39154698804549637, 0.7232483465737544, 0.38811892787927393, 0.42884773644527174, 0.3736304592681554, 0.9600100669479835, 0.06205548596891375, 0.10896864027189346, 0.9170096228485567, 0.8459010992699757, 0.30761449503892613, 0.8478300050877966, 0.9647192815246142, 0.5040867066448698, 0.0068913303921501745, 0.7565473646221516, 0.5325709896275236, 0.10684238666255397, 0.007417179192884871, 0.6430672917148806, 0.7237712763512668, 0.4589398964304189, 0.29410508761055987, 0.064002253159258, 0.8711615334617334, 0.537506683684734, 0.1222249452192733, 0.22544605865175638, 0.8013125029639416, 0.06918388714211765, 0.2818624562395676, 0.3388567087686565, 0.7596190644280962, 0.4712190659273652, 0.7177028480104558, 0.8837427199664432, 0.12255851492611702, 0.8129831563469956, 0.9420051180631097, 0.35642612208360114, 0.3621235958290091, 0.023247078452731995, 0.11957180385869748, 0.6274224465076244, 0.4225483855131913, 0.030309789169150836, 0.4455336069056497, 0.9465406465182978, 0.5022244553673474, 0.1226055551310834, 0.2049912036478969, 0.4021391939526052, 0.23124621798219536, 0.3903776422877223, 0.3332994207567346, 0.03580285942154671, 0.8867295314570874, 0.06857182393828254, 0.07115850835559856, 0.6378989374324618, 0.803880671623502, 0.22809229229220962, 0.753433859268805, 0.38841429438375075, 0.7918651200160481, 0.24360837779052613, 0.9056613738832264, 0.9385736606966754, 0.06622278043418717, 0.967273550043943, 0.8760773395552374, 0.048464755935895276, 0.25523539413061247, 0.5477424561911155, 0.6978624280399159, 0.6066213271536468, 0.016765486949806174, 0.20305201441181453, 0.46435402919592195, 0.03359031987695815, 0.7564039395020755, 0.18214331732560318, 0.3004878570663939, 0.2657149727139655, 0.9130666952288345, 0.10589508942731674, 0.7397472723760098, 0.7515084091075538, 0.764323700984563, 0.18536982532202495, 0.967836929909838, 0.5688021183055577, 0.5251070320784863, 0.8194794875707416, 0.711514479874814, 0.8011950259106132, 0.9161378723435716, 0.9112289666638912, 0.8958606575827054, 0.0049562508779309455, 0.037795321528270254, 0.2785606664539091, 0.7184321601778852, 0.2079440650879505, 0.8499124375902902, 0.2867232389543165, 0.4678808977142527, 0.2682457105288091, 0.13140954010224037, 0.20275686931053472, 0.32159569189149895, 0.12155759786578979, 0.16189159416291143, 0.8192989919803969, 0.35073647862303703, 0.4407366474475377, 0.3810116987201465, 0.9782518588640291, 0.5586230977544117, 0.7249555636359053, 0.06434237964775802, 0.6914896868262008, 0.6275892876706441, 0.735358085403119, 0.6306383056822153, 0.6498757854170806, 0.6322161667688184, 0.19198329779294887, 0.6516687656677962, 0.1081386637660976, 0.030249537292159845, 0.5864263281338024, 0.16889123177987675, 0.9528610414052628, 0.12506080644293738, 0.03990107152024336, 0.7597030025714054, 0.9663534063530033, 0.6958525100171216, 0.8827013476022744, 0.5919202863964861, 0.3216486856637837, 0.1551124077118442, 0.5677654654677756, 0.16876764393362365, 0.9625577025427577, 0.2727327233325769, 0.46829282643751713, 0.20074437955503766, 0.469872344966789, 0.5272350034814809, 0.9121498088157578, 0.42210111241428994, 0.5463807456266974, 0.3929346499696592, 0.8958374590713014, 0.4996363128870627, 0.18980236328096078, 0.8950813876208525, 0.015511744462498922, 0.3509555848533771, 0.43241454565843884, 0.09778093434233415, 0.4243881796740231, 0.7912184913983528, 0.010416734692408558, 0.8420202126201568, 0.5472014312988326, 0.796885383187479, 0.8461032834464772, 0.22369625782029112, 0.93831951618534, 0.151474949289095, 0.7254517676017, 0.9490667115626363, 0.6722300086994522, 0.550210442775023, 0.4835651210996895, 0.9352702379496065, 0.17636778411700205, 0.3615385021658286, 0.6411899253588674, 0.8048528948448236, 0.16070364446189422, 0.5713507443988888, 0.7769015539229669, 0.9679866863535502, 0.47001194584228023, 0.20396903177724524, 0.31565061032176267, 0.6847516518461938, 0.00573523061898229, 0.07196189694525801, 0.9618589893086418, 0.9644619415322963, 0.5181671190358292, 0.5412639482459853, 0.6211936455635816, 0.007105032475052786, 0.2879010055269955, 0.2606533191176159, 0.067745421169767, 0.5730938484939184, 0.31078426086384425, 0.7445326874434041, 0.18800641833380438, 0.06335038297630347, 0.9401895599300039, 0.9289774595609444, 0.2461191478177579, 0.2917131640729026, 0.7033682308571989, 0.6758715800520104, 0.394369928863613, 0.5788317015691845, 0.34360501826008183, 0.6418215005897883, 0.9095510885886179, 0.39246493182534103, 0.24396164757455419, 0.8729946864394247, 0.7735259195306786, 0.25659893596343086, 0.17521588187163017, 0.0008856033840343169, 0.38034141055397164, 0.07724366284648432, 0.6210018632265993, 0.08251691719265741, 0.5430373213637285, 0.6769126876902418, 0.8817843868028564, 0.36545247723282837, 0.8566117155825118], [0.4055057177472672, 0.540045522818006, 0.1959702396477363, 0.27280587997751804, 0.9746538345333309, 0.16721002546677177, 0.8031746101604121, 0.9800612767667516, 0.8685064062104575, 0.3373572627810294, 0.633749868259051, 0.15030479872604197, 0.8264145482495696, 0.8467083632554836, 0.25616804916934, 0.8858682159789822, 0.5306600797809359, 0.7541434343538285, 0.41553043150134905, 0.5439582994560272, 0.09555926878631915, 0.8495627788031032, 0.002668148089704836, 0.0435272853209171, 0.9707842425064103, 0.5092085297526171, 0.7206345467088658, 0.006570943213655034, 0.19631190610244076, 0.2815015784845093, 0.430481604376999, 0.9747430518999523, 0.1574593266298353, 0.6375538314964084, 0.3884288618390256, 0.08867534613014039, 0.46675018027569204, 0.8640245781323402, 0.9212569444117925, 0.5566200853873888, 0.5664937266312985, 0.23176312360986462, 0.31456905384375355, 0.42470847183529636, 0.2103302784187473, 0.8041627033344675, 0.41550689873528845, 0.32959075087048006, 0.2861655378537983, 0.10970540657252026, 0.06485484723119805, 0.06535100492194967, 0.8034450750658653, 0.019230615303481247, 0.9435492189581067, 0.6311330548062037, 0.14893672301781202, 0.7991606906934796, 0.6009625384418708, 0.9442624245473048, 0.17461970983984465, 0.8892760419973478, 0.09153028563367072, 0.901457536528665, 0.28720265433463643, 0.09342105934025446, 0.4690141484976975, 0.7076530877050373, 0.23374937022728426, 0.31100580910126774, 0.7821832134438473, 0.018439358038855325, 0.6219912061468679, 0.5290209002965053, 0.24873732475489618, 0.544403088960513, 0.8229385628270711, 0.11003405422843071, 0.7713029538130239, 0.35978301202151464, 0.04984609880337332, 0.9813722924224052, 0.5330980289577988, 0.6206286191732318, 0.1878825636635304, 0.5201289824794763, 0.3934390927784509, 0.30303838445933284, 0.9090779913771625, 0.1291876904618542, 0.6109534421363754, 0.5698168589036103, 0.3800238562416637, 0.8158383490611885, 0.1155624973776862, 0.3164788357976397, 0.01273638727251769, 0.8500178954669334, 0.7876636331762776, 0.3444417750363983, 0.3318848829617971, 0.1153485839460705, 0.2878866448348206, 0.9940422113116326, 0.3022116068728975, 0.5892874234857602, 0.7681474795888017, 0.5140759398282464, 0.9387449698665191, 0.22399414132364726, 0.23815128926954043, 0.7028405596947731, 0.6550845839661035, 0.5883852500219756, 0.9814861574994144, 0.501694097575286, 0.10796841619096842, 0.003573816444024991, 0.09292711702533418, 0.32536937944120337, 0.7793332036139573, 0.38115453308267744, 0.7895576495931481, 0.01972152946496586, 0.9214957623437761, 0.12794759906319142, 0.40836273856072525, 0.34704958236520844, 0.08398038492672488, 0.5355551733270691, 0.695740065833924, 0.39893851767589517, 0.5938052708604427, 0.11289813679614724, 0.2250248757141261, 0.42225144390615865, 0.8074643390685879, 0.7309170745584107, 0.20868535503475882, 0.7121908231618779, 0.22250543473014295, 0.08104961003172317, 0.6446382329974544, 0.42327663243371416, 0.30887069418263435, 0.14127014147130024, 0.6054226920203846, 0.5640035780824466, 0.7515837298389368, 0.03805255803639096, 0.12230377108970092, 0.26880688600818214, 0.30747145510588836, 0.9550009774556644, 0.09541741321685848, 0.563730560878936, 0.3068577073977342, 0.08338324857118784, 0.5071156884344864, 0.25877549044333603, 0.8589491238481342, 0.9320272847321576, 0.8050798825545973, 0.41335389759643826, 0.18228156752860358, 0.49989873216960334, 0.5074208812231241, 0.6141041869942317, 0.22647544672866615, 0.6922659514797994, 0.7294991640996245, 0.6981114470777301, 0.5337183291974442, 0.008650054220412429, 0.38084443511003696, 0.8180006580654364, 0.6586494868397075, 0.582499636815135, 0.8242196181528032, 0.8116330539407889, 0.28117495662711034, 0.8986296546887724, 0.4803503451180263, 0.5006181786088557, 0.3874385810737363, 0.7756922579544675, 0.6677232850908776, 0.39793834727244226, 0.5241384793961864, 0.04769137433709181, 0.5837534160120866, 0.5669823090588064, 0.4058652746399328, 0.5956528234112007, 0.4079418773304222, 0.24245898917338193, 0.48663602360047953, 0.30045915859324945, 0.5202181061627159, 0.9940473878915692, 0.33707970453495795, 0.11247412715327598, 0.5453874950837865, 0.4385938747694089, 0.7338871136782762, 0.46422585860593146, 0.223755343449761, 0.4197080943746103, 0.4510403313151743, 0.2302071023532678, 0.33913287233904066, 0.38410681874275954, 0.918133711749088, 0.5787535288183369, 0.0467133120798614, 0.6978184034347928, 0.45398449743884006, 0.4177753291138431, 0.479144519788989, 0.16052113865844053, 0.24151586468537234, 0.565134003474222, 0.5000305896633088, 0.7614065785092615, 0.37081421986754015, 0.2726818402554909, 0.29815157153243954, 0.7251547150825699, 0.7320022448867902, 0.059009219138427826, 0.4100180530631933, 0.6408786644414314, 0.7432032767245745, 0.11280610461871432, 0.5731231795578383, 0.1904104474191909, 0.36303232751485714, 0.1689353426534812, 0.4895764633506451, 0.4056663018765271, 0.1974206343580931, 0.6003932987219419, 0.7639165383511827, 0.17302010375660437, 0.06292608176055892, 0.6879342784009939, 0.6901449728711063, 0.7161919081451227, 0.2937747095046368, 0.9894369786277837, 0.604930673154014, 0.0502090593985457, 0.5088519928939145, 0.7532633305714537, 0.9469461946383955, 0.9101518130588184, 0.4723037441951299, 0.16814901776842794, 0.6278903178179956, 0.5751278142357528, 0.7490112289494966, 0.3754193438439961, 0.865929127459765, 0.6031195534394088, 0.17663028000110126, 0.6521227002462701, 0.8748441869465848, 0.1508979610038046, 0.9696492852770874, 0.5064264558202611, 0.7903988729706393, 0.0706490745766909, 0.36437939026481425, 0.9976918517736766, 0.3350818311272272, 0.31614301493921293, 0.41763099865726183, 0.057646004885071545, 0.01255715239952171, 0.4992921321404311, 0.6360727200822568, 0.4292544063287821, 0.7368354698846795, 0.7799234867532088, 0.2928587460240598, 0.3134289998082842, 0.4701904026798829, 0.03820786612938154, 0.5605077655399152, 0.22090452222260548, 0.7378911869495607, 0.7121759267481149, 0.7878656733053162, 0.7525697998051731, 0.546664260443336, 0.7541901238080317, 0.3380714708736495, 0.6863926287187522, 0.5845437057234648, 0.9628909297546804, 0.3641246925875098, 0.33308555921571403, 0.34918067130353714, 0.21183141068758982, 0.8017455946840437, 0.8273830668497958, 0.7883586968331897, 0.8238263536626754, 0.6917475504793746, 0.3808924634605756, 0.12197857550653823, 0.3857805639191024, 0.0002075928032139185, 0.24317507436299413, 0.7752505195182566, 0.5662053613705728, 0.015076830080796855, 0.6419138708490008, 0.3379567220587629, 0.1266794509343917, 0.2267860260794371, 0.40883087126313833, 0.9851868325256066, 0.7404263270881029, 0.012517633429031338, 0.24011627309528738, 0.05169680304915425, 0.2887414685492966, 0.15880138063431315, 0.3836237915691231, 0.2094231490169033, 0.0077374440486817075, 0.7790025270524741, 0.26712489924560257, 0.3285101030063603, 0.09065734238918854, 0.39412990195631514, 0.4334640208099664, 0.6481012645379922, 0.8306486616227736, 0.40554133169091655, 0.2988647680460893, 0.9041225460784164, 0.15401491407137546, 0.9479993292120584, 0.8399824191216682, 0.12835329817964647, 0.11156304019005236, 0.9271417924073274, 0.6274954520495261, 0.9744358387638166, 0.5946406501035733, 0.30974595700634544, 0.6686831302605744, 0.4102957191558564, 0.6515894288916502, 0.7466676705666713, 0.8866339768512363, 0.47999268364602654, 0.3197226010110955, 0.047207219641273945, 0.034225792341118666, 0.35970514897957706, 0.011661077559119937, 0.4545488698486463, 0.20701863235610085, 0.22503452000960433, 0.8384626420315261, 0.17046726115139466, 0.2964419938052242, 0.38478885810665286, 0.4657637524252338, 0.10956618330663936, 0.3400567171584331, 0.10067955889319569, 0.14901133762232188, 0.019728476507586468, 0.9757765739282772, 0.2587124603997467, 0.6350501820120514, 0.35483489031129056, 0.21671557267316732, 0.23092801579708933, 0.13192303498322056, 0.568470406609896, 0.5922879933665711, 0.6905704757064411, 0.8016916243554838, 0.908739730249292, 0.6197419851478809, 0.060592119675525136, 0.8302432120558507, 0.3018137009667978, 0.07243941837112156, 0.17447443700837373, 0.583058867130113, 0.7414026119553448, 0.015332362195816351, 0.5573696042952909, 0.9219193937426192, 0.8565110715984848, 0.5762916011048869, 0.22105990571995005, 0.0453347873876796, 0.5967519183542027, 0.09647934285603454, 0.6739002294186389, 0.6395592720598583, 0.5954369379260598, 0.7114075142719615, 0.9797037175732025, 0.7263184182746878, 0.9490216041668723, 0.9603457011287264, 0.21182019159478027, 0.7062639976395686, 0.040057814770971145, 0.6877475311699709, 0.41007106927657555, 0.7701333354811399, 0.7774421899584301, 0.05782581463529024, 0.12075411392991853, 0.8439377747939596, 0.7220329104090943, 0.7972826520556636, 0.4706723866231566, 0.6065279550004709, 0.18429493458735935, 0.851498400196005, 0.6199178942774727, 0.5615287500613256, 0.9784901349501195, 0.568381572255151, 0.33988437006049543, 0.6291365935008809, 0.3239596625012111, 0.06870654480951621, 0.7990516587585772, 0.9950727387006517, 0.7566767770489196, 0.09424706189378129, 0.05723003307679053, 0.8807316208449966, 0.4080283542903348, 0.5874356684227202, 0.46390267336184765, 0.0811650465890218, 0.4845767110438747, 0.026400120558444984, 0.6798681057472789, 0.13130095671371056, 0.419481542454171, 0.20915332665407171, 0.6261962984293302, 0.022686425460725745, 0.9179994354582636, 0.8738932703737466, 0.7226118723622271, 0.4106385008342479, 0.3686945887620914, 0.26059448169438393, 0.3982710143673187, 0.14462509324451922, 0.05154394884644331, 0.6872891323969436, 0.38207105813442155, 0.14822073239900924, 0.5881340985559261, 0.05045441476274315, 0.25838742130465, 0.46995236576354193, 0.29508558542830754, 0.361012259383761, 0.9912486641391711, 0.6241820688151645, 0.9869860268858404, 0.8800138179297025, 0.40722659665091043, 0.3148137183346289, 0.21332789058190582, 0.0960916968031329, 0.8912640063185698, 0.27938615493745544, 0.6904250011015677, 0.1860240445089888, 0.8074927086795785, 0.23199207908777675, 0.6123267113158924, 0.25138889120217056, 0.43235780610203134, 0.7811742486109365, 0.41203346421127995, 0.8494515231093275, 0.6845013903958128, 0.19862833990946327, 0.1038196607037426, 0.0900989478854014, 0.38775996140400604, 0.3068725047573603, 0.7397879295980329, 0.24430973722743354, 0.030025505065355285, 0.6750201092429109, 0.6904418782582364, 0.5103979403773896, 0.5125052553765873, 0.6977125317723105, 0.24688926246934284, 0.1379993235311826, 0.03516346401491244, 0.803280011099384, 0.5767747944922766, 0.39501369779197504, 0.8418012872879421, 0.7810073117840765, 0.9429958600321756, 0.2501739195069158, 0.3580571585710429, 0.6760485384810102, 0.8027287937657581, 0.11135509685754463, 0.8957472606610739, 0.8300296628512012, 0.35340401762379325, 0.59904216443475, 0.28859823563494647, 0.7674245963145344, 0.6734320125739199, 0.8358961582021293, 0.1278944980813791, 0.32892270249662825, 0.9994117515238167, 0.2748503728085788, 0.13231884641928615, 0.152733684127751, 0.12164926074182669, 0.004708519614945228, 0.13599225835448792, 0.04636373949501893, 0.7489037896811741, 0.1300737763136739, 0.17833795696757992, 0.8247752518504372, 0.2293416386472028, 0.21127843338889984, 0.5204785157035939, 0.66835495099628, 0.4618149125544022, 0.3243119583258681, 0.7214644670213985, 0.025730511989276006, 0.6783005003880366, 0.12086112375658808, 0.8668057325103551, 0.465699400135522, 0.25431440714112497, 0.601224618183125, 0.2756696607808077, 0.8677241812448981, 0.5229120639196303, 0.3588625725606581, 0.10489030408199729, 0.145370074327128, 0.4099113791795018, 0.34346251731248967, 0.33195425089621133, 0.6925659378298747, 0.8214267058724511, 0.6395818552267809, 0.012996991856642381, 0.22863063582424414, 0.5657988662721365, 0.08881816760648065, 0.9909062747517301, 0.9727991308339224, 0.1001980532599982, 0.9443303694629502, 0.5437134750574711, 0.618523687226761, 0.7689461236423459, 0.8383463243759587, 0.7669795487883346, 0.7163303031062132, 0.9147877365048254, 0.00916742495858558, 0.8649797821080121, 0.08254461321346307, 0.5717174667639048, 0.3612919486669549, 0.10269183699998607, 0.856170252560751, 0.8392477471922538, 0.9141862239886277, 0.7472826565281964, 0.6956248924546488, 0.3300836511452421, 0.8507411684663382, 0.08220864009230633, 0.18521836985376627, 0.6578017383746666, 0.6164739271434297, 0.5168526829805357, 0.563706227628832, 0.03290253280740729, 0.6712956609683466, 0.08652929832637823, 0.5233059551235584, 0.4827338203449718, 0.5556429328203337, 0.6750631570593562, 0.9300959099470109, 0.7901133547760499, 0.5699562103201063, 0.34585373654468654, 0.10225368169478521, 0.4191114350338264, 0.3376823469743424, 0.2546184352619689, 0.6437318383948092, 0.5272924073170843, 0.06065015342875757, 0.06052848004242395, 0.31604221401667987, 0.23026042112827427, 0.7555469452656726, 0.9230693436674763, 0.5965757780315727, 0.019446216724195775, 0.49310479033666943, 0.13767317257879663, 0.2658706300125008, 0.75846410603671, 0.6949572665178546, 0.1009921940192533, 0.3076455142936667, 0.030694637126833624, 0.6916941701401589, 0.5558703990358123, 0.6537938562147828, 0.6218080294062088, 0.6684133569586286, 0.2573530351066925, 0.9652501891969015, 0.27379036441845084, 0.9266914417145697, 0.5472116273483846, 0.983904571433771, 0.7393887945780903, 0.6250565740269571, 0.01493636592868397, 0.7480495230489366, 0.4440399021020963, 0.9406867172369323, 0.16292525044826434, 0.5993850659558388, 0.7034376226585782, 0.16069585241785644, 0.9355514225624162, 0.5471106504266037, 0.03202362468581732, 0.6740656478079871, 0.3054475182566211, 0.023027495740441606, 0.6687744371598181, 0.6894380179819721, 0.07028313769832595, 0.3181064489358033, 0.37687523925177757, 0.8345402185155993, 0.8890938824861102, 0.3661960250187075, 0.11320371566309062, 0.012345639204901948, 0.45620364220023735, 0.9863471102445412, 0.7927600813077142, 0.3413334472421451, 0.2154392002402331, 0.6577143433354523, 0.7164613346963555, 0.9624827713555196, 0.40470284350457264, 0.006054646410836861, 0.08005836912163244, 0.8434024609903421, 0.2605004970007677, 0.19381520207755087, 0.8029667327035998, 0.1078348520334288, 0.7307046097024742, 0.5119920114761488, 0.5802415451499903, 0.38637879954297627, 0.6967613584230771, 0.33595865235648514, 0.5703025482477648, 0.6523141199817346, 0.4889621434302891, 0.6532408016740751, 0.8726115328569455, 0.8004824941033981, 0.26596715313396624, 0.060593946128281284, 0.651821711151133, 0.46276645660954174, 0.26462631731906283, 0.5122731657741786, 0.30464511285062745, 0.051377527642223586, 0.75890920140524, 0.8936538417932276, 0.3998646191334463, 0.6721740669101257, 0.9884427397133946, 0.16243066378108684, 0.8730740317004928, 0.4526085130457964, 0.8707066652838036, 0.6585593519322788, 0.17997987003170746, 0.7338516600669385, 0.4947525179344886, 0.1249084056622326, 0.43940727045132166, 0.1606232612266434, 0.5059657241807274, 0.8176927252174433, 0.16114633041528292, 0.9076189171848021, 0.7600718914252349, 0.09687948585605877, 0.5866969616073279, 0.6294904156566966, 0.5496720655434102, 0.078397626219877, 0.6699886114890231, 0.19546186922290942, 0.5018464533453871, 0.2978417101475691, 0.8147108695666009, 0.1973223489108823, 0.21803249855713336, 0.5975796298431924, 0.7842650831668737, 0.3319117431163955, 0.8829920548745368, 0.678953760611371, 0.9843747601507309, 0.8009751399211374, 0.12873292151176285, 0.6238286782121438, 0.16437011202435292, 0.3761485053800281, 0.16198690850955122, 0.2083977397416218, 0.6860760523274652, 0.4401460021482616, 0.04431446766682323, 0.31842399662410537, 0.7443956008150224, 0.394759027331255, 0.07423924630801282, 0.5507555438466094, 0.8591206268679163, 0.7873653791233723, 0.570110235394329, 0.4474425105769011, 0.12873511119804248, 0.6522300644991719, 0.20234554588881715, 0.8955304685391263, 0.30481930136797364, 0.8726703627668914, 0.9387153262686994, 0.3560721350696411, 0.3506570719585538, 0.6048353635473503, 0.216636482012165, 0.6607899077293174, 0.36027411347542826, 0.4643263475248145, 0.9263218708345267, 0.9413345828909285, 0.986770743387643, 0.5455490456135147, 0.6574936294468984, 0.34181572958055284, 0.4097477226004037, 0.8723559545757555, 0.4499898306796599, 0.8277222092489199, 0.0711046371219568, 0.3947735458557078, 0.41965094950938664, 0.08804325454435624, 0.36832732185715367, 0.6019164392366131, 0.7304859995137676, 0.12893319704978579, 0.9568719327453399, 0.843090068487194, 0.729153817702277, 0.2345967432043986, 0.1534072811916346, 0.32361097550187023, 0.6318431032136599, 0.3829659254329313, 0.9607568878919616, 0.9232513853001716, 0.21989071601615673, 0.23687844250548684, 0.38950226505479935, 0.5101796144564072, 0.7844530130407942, 0.08553045425615569, 0.8714091164842563, 0.6896032161622081, 0.5914664315622487, 0.44783539625757185, 0.6208108641893034, 0.5769945511476979, 0.286085460800962, 0.5429395572159247, 0.7708532064541372, 0.7341984740184186, 0.7100773146164755, 0.12505033624361006, 0.18226141910181481, 0.018586873369839663, 0.5488769992194589, 0.2756057843848523, 0.6188606246220253, 0.19447371957571913, 0.016783115567560625, 0.6284084835786106, 0.2996406064368087, 0.28513056157305183, 0.4957791758574539, 0.2205187391425274, 0.08435140415414366, 0.6673155369870338, 0.018672936162905995, 0.527843411063244, 0.9859128901421279, 0.43633098018048533, 0.5988922105142098, 0.8967755484043831, 0.7177493411668449, 0.6398705129123496, 0.455545303858891, 0.40627050923391717, 0.4891913031271625, 0.19544010536303835, 0.9160324614593488, 0.574279712140287, 0.2505096651583234, 0.07789332868062093, 0.12760740942201976, 0.4144325052232849, 0.36200792785864977, 0.5538042965143479, 0.16982498688580205, 0.48032854804847724, 0.7575957235180123, 0.8032622947898336, 0.0526294766650911, 0.2175342457536361, 0.025734951081849866, 0.907361322855013, 0.9619362254747686, 0.4461000436677388, 0.8900264757967357, 0.35344374091973074, 0.23929908033270808, 0.8579840501380238, 0.26045178336954966, 0.7109891799471348, 0.46403349588146836, 0.8521546397652949, 0.9876232750373896, 0.6502352910282135, 0.5064689526800338, 0.47688860124898036, 0.8460497336926643, 0.1630865317928012, 0.6027016367218718, 0.7287556231306259, 0.15396126340457783, 0.9091767394592077, 0.2580786157203986, 0.4790625443907356, 0.12070076736248614, 0.528649046116995, 0.6779954801254576, 0.599238315012255, 0.8425836049688383, 0.168094557733508, 0.5123108567259639, 0.0765854174538948, 0.623379104626125, 0.5801555420828856, 0.6623618596073311, 0.9317044727705777, 0.7970236721565831, 0.8569858475454342, 0.775065603803739, 0.5959954978913273, 0.19296330719739319, 0.9344734527514964, 0.2120955573314648, 0.6113705485694652, 0.6027362097930553, 0.39847799168098463, 0.8922080759278891, 0.0858381454424969, 0.9803273965258641, 0.7078613873222007, 0.7829367656285722, 0.11455523829668812, 0.16129627764568566, 0.987938735259576, 0.7847345616173133, 0.4018285910773902, 0.9411973032444303, 0.9583747701615647, 0.5630894231334792, 0.3143629080704945, 0.592662751894017, 0.03016702618944689, 0.024403263925998786, 0.7874267687697722, 0.4439701266329197, 0.2596084125913375, 0.3488469881873364, 0.2899828616793727, 0.7057169617249616, 0.22992418980815643, 0.5326508014260098, 0.25566583542769017, 0.15071008806460273, 0.4233737889476308, 0.0009475322790136742, 0.34056103243582003, 0.9788363801212023, 0.9286000507855152, 0.17627498305150702, 0.4591667035785295, 0.6119459338928004, 0.0844015822420856, 0.1334551913903944, 0.23381999981463553, 0.3339350514004066, 0.062117253627921754, 0.8668979354589361, 0.2717873367998117, 0.8779865013421887, 0.053362979417561185, 0.35814456959173924, 0.695493959035453, 0.022391955060086066, 0.933615952888406, 0.5028638613190605, 0.07587445963626704, 0.6762344512612556, 0.9348590242358412, 0.9179690604450822, 0.7052632598857532, 0.14649486365725528, 0.8762039922468162, 0.06544587137665636, 0.46837989054088724, 0.5439054503669631, 0.6888334283159842, 0.8952825801216827, 0.24705845021420703, 0.704118418119566, 0.597008342050394, 0.2900694928659817, 0.6433370347427801, 0.702006928121536, 0.12592704289597445, 0.6681501468038605, 0.697143566249103, 0.19700056327647697, 0.7879048595874429, 0.8843210204460574, 0.41770339469396656, 0.7611439343650591, 0.4617253420266646, 0.4038499438743389, 0.615107118252974, 0.5851516596418053, 0.6378327035814194, 0.877830292850119, 0.49619150047682936, 0.20294289239382168, 0.7292685360581507, 0.4550398465165276, 0.15406050976506436]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jp2fPHvKB9wB",
        "outputId": "28c891b2-b552-4b88-b403-b35de60366f0"
      },
      "source": [
        "scaler = preprocessing.StandardScaler().fit(valores)\n",
        "datos_escalados = scaler.transform(valores)\n",
        "\n",
        "print('Media: ',scaler.mean_)\n",
        "\n",
        "datos_escalados"
      ],
      "id": "jp2fPHvKB9wB",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Media:  [0.31327242 0.67561776 0.31274225 0.50988556 0.8862949  0.4104237\n",
            " 0.43759291 0.91541269 0.67506542 0.65235786 0.58541102 0.21340183\n",
            " 0.72053828 0.7045257  0.44855534 0.75287293 0.50884203 0.76217591\n",
            " 0.68678124 0.31952159 0.0974391  0.49951838 0.326218   0.2479631\n",
            " 0.98247759 0.46424586 0.46181461 0.24774895 0.23038273 0.32448983\n",
            " 0.31163693 0.84672132 0.28188664 0.65134519 0.50870544 0.48891609\n",
            " 0.64215906 0.68628032 0.79470518 0.40196202 0.70294916 0.25569685\n",
            " 0.60828038 0.55405573 0.41024015 0.5834528  0.55558819 0.22017564\n",
            " 0.18095516 0.3466004  0.44627959 0.51941958 0.47153257 0.29907465\n",
            " 0.72853181 0.60787614 0.22229707 0.50589837 0.55082773 0.88815554\n",
            " 0.40912235 0.61545611 0.200273   0.85115691 0.28654855 0.53924855\n",
            " 0.62088849 0.64530011 0.46156718 0.58245858 0.73198657 0.10269724\n",
            " 0.72799804 0.28705004 0.16475095 0.58407952 0.75052379 0.50599179\n",
            " 0.66716097 0.2843554  0.23086438 0.73905677 0.71017361 0.48300388\n",
            " 0.28333599 0.39035162 0.22780999 0.16104664 0.45704385 0.3891283\n",
            " 0.42359442 0.67243106 0.19728736 0.66638398 0.42321648 0.21046481\n",
            " 0.39488994 0.43557847 0.42340434 0.66245777 0.65147484 0.46622848\n",
            " 0.60253483 0.5573807  0.26829798 0.32171481 0.86258742 0.44950767\n",
            " 0.61430629 0.42596439 0.30413039 0.58709476 0.80500354 0.76474433\n",
            " 0.82357882 0.40870834 0.25853863 0.24982849 0.51262457 0.17043719\n",
            " 0.59500344 0.67704122 0.76826796 0.16781318 0.8133647  0.53608725\n",
            " 0.24548525 0.63207722 0.1895076  0.52876876 0.39563571 0.34548286\n",
            " 0.59662275 0.35267865 0.60626899 0.55385468 0.65550156 0.40100207\n",
            " 0.13500063 0.47207876 0.13790317 0.53436621 0.7699992  0.3938106\n",
            " 0.24776044 0.07653666 0.41433293 0.29028185 0.82053797 0.40876249\n",
            " 0.28673344 0.39288547 0.38841348 0.66812476 0.54279378 0.61160495\n",
            " 0.30344537 0.31010194 0.35540564 0.62047064 0.81706104 0.81035729\n",
            " 0.77356337 0.53966971 0.29965277 0.28900173 0.71572442 0.76471739\n",
            " 0.4215197  0.78387897 0.53806917 0.70042901 0.63933365 0.10472907\n",
            " 0.32953262 0.8457712  0.40732738 0.65185072 0.45496821 0.42398506\n",
            " 0.49315172 0.74598152 0.73995916 0.31939184 0.20452085 0.51108064\n",
            " 0.34831583 0.27636643 0.37100405 0.20213598 0.51262963 0.47367116\n",
            " 0.4199361  0.63687463 0.34725764 0.34230611 0.45432879 0.52460207\n",
            " 0.49035006 0.93337388 0.26067242 0.47588502 0.38522409 0.27103961\n",
            " 0.70730698 0.34084933 0.53781471 0.39265097 0.33003199 0.18369016\n",
            " 0.40303756 0.58746855 0.84760775 0.63421752 0.33145378 0.74510183\n",
            " 0.26229868 0.64325476 0.45368251 0.31603662 0.62050723 0.72948853\n",
            " 0.31065459 0.5260208  0.48743873 0.29970854 0.64449851 0.65663903\n",
            " 0.64042204 0.42184235 0.37842887 0.7280241  0.69882877 0.52141067\n",
            " 0.72111754 0.27450175 0.42295406 0.18277022 0.46482171 0.57948149\n",
            " 0.55474955 0.61647628 0.70456407 0.30738492 0.21185795 0.65419951\n",
            " 0.74552389 0.84593954 0.46206472 0.61857538 0.71569928 0.08852013\n",
            " 0.3752831  0.50824708 0.60143075 0.82577946 0.43202934 0.44917005\n",
            " 0.8135919  0.72094977 0.6176303  0.46503198 0.53084006 0.42610591\n",
            " 0.36082165 0.49656591 0.52371128 0.08197829 0.6365136  0.73310218\n",
            " 0.70314016 0.22317129 0.66773831 0.97670068 0.34478126 0.40327276\n",
            " 0.46588043 0.34319159 0.30872365 0.33518589 0.42714938 0.39458395\n",
            " 0.8290028  0.77382407 0.24621404 0.23457826 0.25111966 0.45395814\n",
            " 0.60592686 0.41384244 0.84762501 0.52437283 0.73776047 0.44441662\n",
            " 0.56462213 0.84363214 0.38435045 0.75449638 0.67737746 0.76663095\n",
            " 0.4539951  0.53792638 0.52988372 0.31890384 0.73864169 0.43248727\n",
            " 0.7906489  0.44338444 0.62011526 0.67205305 0.23022126 0.49653561\n",
            " 0.15434762 0.17527812 0.86686367 0.68466944 0.1846472  0.56309695\n",
            " 0.37516631 0.26322565 0.50272694 0.29479178 0.56717546 0.65521988\n",
            " 0.18007891 0.56035046 0.37515527 0.59522639 0.10680556 0.56236711\n",
            " 0.4087422  0.12532782 0.49726332 0.61003169 0.17764378 0.13742209\n",
            " 0.24045875 0.61522972 0.60440378 0.85366381 0.2081236  0.25126474\n",
            " 0.91940523 0.13889124 0.56808289 0.76180428 0.23588379 0.33477017\n",
            " 0.51215118 0.43029251 0.71287246 0.62622695 0.22404329 0.41527075\n",
            " 0.52738186 0.63115532 0.66242524 0.88582673 0.60395057 0.30375709\n",
            " 0.158014   0.27747469 0.48744329 0.3006328  0.72118492 0.42455119\n",
            " 0.46307365 0.74817186 0.15364814 0.20916554 0.41029661 0.3802097\n",
            " 0.11317286 0.58271633 0.35753801 0.30242068 0.26774899 0.58815283\n",
            " 0.14183165 0.43096928 0.37170281 0.4083163  0.40823213 0.22105326\n",
            " 0.63011438 0.59149864 0.6673424  0.76030762 0.80473109 0.35360192\n",
            " 0.29669039 0.68984823 0.22972055 0.26254666 0.17178326 0.54108757\n",
            " 0.40605056 0.28816447 0.50403828 0.56370852 0.86373094 0.68551272\n",
            " 0.50854121 0.34367374 0.48822845 0.37676696 0.36063349 0.6553315\n",
            " 0.78748414 0.80402606 0.97619248 0.39256443 0.54080939 0.61430671\n",
            " 0.44287713 0.61875349 0.19934028 0.81818794 0.56111204 0.8743794\n",
            " 0.74520112 0.40468754 0.11137963 0.70688087 0.85280101 0.65460819\n",
            " 0.48579392 0.38660605 0.52086358 0.75829961 0.40123655 0.37228908\n",
            " 0.80720553 0.55068398 0.22178091 0.33250542 0.44971664 0.1780227\n",
            " 0.59723723 0.61284966 0.63911678 0.45682845 0.51787855 0.85772389\n",
            " 0.2383655  0.79121007 0.63046268 0.14213368 0.42692885 0.32457915\n",
            " 0.58997476 0.23971751 0.28066022 0.27563885 0.78827167 0.34511594\n",
            " 0.53940922 0.51350129 0.63499632 0.58511127 0.41001465 0.28071644\n",
            " 0.36494032 0.42459873 0.32061897 0.40528421 0.32583367 0.33269843\n",
            " 0.29838745 0.06862705 0.17786519 0.51392313 0.31928346 0.42360909\n",
            " 0.65916241 0.44670763 0.61258313 0.80678082 0.51670734 0.16213379\n",
            " 0.32068784 0.06970117 0.90688426 0.47049716 0.75787981 0.21841285\n",
            " 0.45903812 0.44730908 0.41523179 0.24435253 0.30655542 0.76525303\n",
            " 0.23557114 0.7130571  0.44978788 0.21115173 0.13903493 0.34304808\n",
            " 0.3574597  0.28942668 0.43356375 0.48622754 0.28283606 0.43884143\n",
            " 0.77370679 0.62380765 0.72466899 0.84475552 0.54844945 0.36444027\n",
            " 0.45670741 0.48402053 0.35946618 0.26868874 0.60038497 0.51694559\n",
            " 0.75140046 0.16602236 0.26039148 0.45168331 0.60284045 0.39381919\n",
            " 0.66907588 0.84250359 0.26997575 0.7748096  0.48030348 0.76611753\n",
            " 0.68559815 0.50088482 0.42264918 0.36408429 0.91434729 0.53012012\n",
            " 0.2103461  0.43088971 0.41516632 0.36890754 0.52076686 0.41378739\n",
            " 0.47457734 0.19338994 0.11717991 0.67093072 0.44728056 0.1411995\n",
            " 0.71812811 0.4718566  0.48272203 0.42045005 0.49560262 0.08178741\n",
            " 0.82975626 0.27394594 0.91065849 0.64027709 0.22006338 0.53368073\n",
            " 0.34766468 0.43975496 0.71863734 0.5092722  0.28952038 0.4872603\n",
            " 0.55269046 0.31163853 0.33924726 0.59635376 0.6481584  0.36326087\n",
            " 0.08233349 0.37139488 0.5245444  0.36088154 0.71007347 0.74156725\n",
            " 0.26895398 0.81122187 0.61072572 0.64408856 0.86484314 0.90753473\n",
            " 0.43706414 0.76123855 0.8488664  0.18153443 0.76496591 0.40001646\n",
            " 0.41703366 0.62776495 0.33768157 0.58395629 0.88482324 0.56667173\n",
            " 0.73460402 0.51587906 0.29530979 0.46095926 0.38002253 0.31936181\n",
            " 0.42040421 0.57353675 0.60044435 0.76859925 0.22528086 0.51653626\n",
            " 0.17089366 0.46299867 0.24813213 0.69252964 0.77568504 0.77076978\n",
            " 0.83966472 0.71902636 0.63161669 0.51526416 0.66581284 0.39820772\n",
            " 0.32784523 0.64544484 0.5425768  0.2631924  0.2911632  0.47306516\n",
            " 0.58930419 0.66241374 0.81679229 0.66101448 0.37237708 0.73715208\n",
            " 0.5475891  0.13594433 0.41438786 0.8108743  0.28952062 0.17781472\n",
            " 0.39743491 0.58774584 0.56601008 0.4849328  0.57943815 0.60415458\n",
            " 0.2358076  0.50469574 0.41896704 0.74769687 0.67804901 0.91753061\n",
            " 0.51790073 0.79196054 0.23559295 0.39483653 0.28702597 0.66227367\n",
            " 0.10409847 0.46696525 0.41893119 0.39884822 0.79462934 0.41304574\n",
            " 0.30732014 0.47500891 0.37524696 0.12920935 0.82761494 0.63551329\n",
            " 0.38214884 0.48779114 0.49888699 0.58750069 0.7388433  0.23095719\n",
            " 0.27279063 0.15493154 0.24587211 0.95627634 0.80004644 0.4630212\n",
            " 0.35009565 0.67785017 0.80990274 0.84202781 0.40670454 0.04921195\n",
            " 0.49230788 0.86031374 0.13581233 0.32627374 0.77207663 0.47145171\n",
            " 0.80067277 0.70973971 0.39117423 0.49669862 0.39449366 0.64544767\n",
            " 0.29145889 0.63698232 0.49394336 0.75370221 0.50903817 0.71947748\n",
            " 0.58620663 0.24448559 0.74681199 0.2607814  0.23960145 0.489005\n",
            " 0.24854084 0.37226831 0.42156866 0.86847818 0.20973453 0.59026326\n",
            " 0.64007404 0.4907886  0.75209443 0.43054385 0.48155544 0.80581466\n",
            " 0.31192197 0.80938841 0.54016172 0.27635005 0.69051192 0.39760592\n",
            " 0.61737    0.41350783 0.31843903 0.93959455 0.43727327 0.38977374\n",
            " 0.65381641 0.5105532  0.53927696 0.52851069 0.76008045 0.31991346\n",
            " 0.49094825 0.1562081  0.849018   0.29917025 0.42506657 0.76284102\n",
            " 0.57764336 0.18786239 0.888876   0.52586336 0.98231351 0.49649072\n",
            " 0.21804303 0.76578811 0.339928   0.21108317 0.1112628  0.13579698\n",
            " 0.67479899 0.4151166  0.29255449 0.63000169 0.85830767 0.39378704\n",
            " 0.12209545 0.63906401 0.91753566 0.76079042 0.48082861 0.58534543\n",
            " 0.25842702 0.5405389  0.287988   0.92777027 0.18343739 0.4908195\n",
            " 0.92786247 0.60098662 0.32913578 0.72633268 0.59067788 0.58243831\n",
            " 0.18358272 0.61043686 0.72944643 0.52408848 0.49709396 0.59430817\n",
            " 0.69063245 0.40037781 0.35192641 0.4681791  0.66057568 0.68261445\n",
            " 0.09666479 0.3101098  0.61048173 0.07861357 0.32509489 0.47038657\n",
            " 0.74505253 0.30007613 0.83728739 0.86341639 0.42585617 0.52378995\n",
            " 0.5477062  0.34001855 0.49698335 0.2031065  0.54016435 0.77533692\n",
            " 0.32121955 0.13359412 0.41751794 0.72836013 0.64333873 0.104068\n",
            " 0.53820016 0.54587121 0.41135632 0.41910652 0.47705514 0.30639871\n",
            " 0.5864075  0.30575569 0.42100586 0.68604871 0.75697899 0.17657131\n",
            " 0.46784764 0.20350058 0.67037106 0.25960708 0.762261   0.56652369\n",
            " 0.04150295 0.79784102 0.58785897 0.16679766 0.37550728 0.3841306\n",
            " 0.39110692 0.63696843 0.01771921 0.36544771 0.72513346 0.23496065\n",
            " 0.67764808 0.53945943 0.5091186  0.45279274 0.684306   0.2560828\n",
            " 0.61446929 0.47347426 0.84017808 0.37982477 0.6091733  0.32334772\n",
            " 0.32635722 0.616956   0.5367612  0.67749966 0.54298143 0.69577876\n",
            " 0.82672819 0.40410927 0.0452124  0.24804746 0.37208356 0.55765269\n",
            " 0.90592433 0.36641164 0.67895369 0.31084473 0.18535431 0.53037046\n",
            " 0.29102374 0.41627339 0.31296255 0.83572682 0.66917988 0.54548597\n",
            " 0.44374033 0.72757023 0.70233642 0.44402105 0.33352201 0.71012265\n",
            " 0.39077528 0.82226741 0.44435846 0.56446916 0.37645847 0.36031617\n",
            " 0.66483212 0.35368849 0.43641657 0.37726044 0.34060104 0.51472323\n",
            " 0.37421996 0.31002831 0.71103243 0.94902894 0.74643809 0.8698436\n",
            " 0.68349295 0.45882209 0.17403786 0.75111946 0.1904316  0.78696413\n",
            " 0.43773447 0.43338541 0.54647623 0.27785525 0.7537812  0.8100056\n",
            " 0.60251894 0.33046799 0.27711546 0.9418881  0.64218544 0.29581548\n",
            " 0.91813935 0.48694326 0.4570225  0.37338873 0.34522184 0.2272776\n",
            " 0.40781088 0.39892175 0.64299517 0.40340492 0.57286619 0.56804307\n",
            " 0.46470661 0.58412185 0.34206288 0.4905588  0.5498884  0.5478019\n",
            " 0.27557899 0.41206308 0.95705331 0.55248392 0.26890674 0.55017831\n",
            " 0.70839941 0.12255261 0.35240297 0.50536078 0.65096087 0.2660646\n",
            " 0.53543348 0.29371897 0.78136908 0.02954911 0.21505323 0.82867647\n",
            " 0.49342695 0.72589154 0.5220639  0.34853405 0.34166974 0.61138001\n",
            " 0.58931119 0.38650434 0.35979436 0.59349413 0.40498928 0.32819315\n",
            " 0.30362792 0.81451149 0.91213002 0.2465888  0.49791579 0.65018829\n",
            " 0.48297054 0.51885348 0.64041931 0.23476603 0.65498582 0.80334733\n",
            " 0.29473275 0.51593325 0.87865785 0.59561466 0.50887144 0.31847061\n",
            " 0.20236777 0.49772426 0.33119766 0.62941728 0.48017361 0.51961441\n",
            " 0.43992779 0.80552646 0.41024616 0.50533611]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.,  1.,  1., ...,  1., -1.,  1.],\n",
              "       [ 1., -1., -1., ..., -1.,  1., -1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wgqZg_mWCBgp",
        "outputId": "1348501b-1c7f-4bdb-d5f9-ad6040b814c5"
      },
      "source": [
        "\n",
        "enc = OneHotEncoder()\n",
        "valor2 = OneHotEncoder().fit_transform(valores).toarray()\n",
        "enc.fit(valor2)\n",
        "\n"
      ],
      "id": "wgqZg_mWCBgp",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\n",
              "              handle_unknown='error', sparse=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTewP1k4CE_L"
      },
      "source": [
        "#X_train, X_test, y_train, y_test = train_test_split(datos_escalados,valor2,test_size=0.2, \\\n",
        " #                                                  stratify = valor2, random_state = 73)\n",
        "#X_train, xtest, ytrain, ytest = train_test_split(datos_escalados, valor2.iloc[:,1], test_size=0.2,\n",
        " # random_state=73, stratify=y.iloc[:,1])\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(datos_escalados, valor2, test_size=0.2, random_state=80)\n"
      ],
      "id": "pTewP1k4CE_L",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vZe8h3y5IWsG",
        "outputId": "ded28767-1b31-4e6e-d922-cf6042fa25d7"
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes = (10, 7, 4), activation = 'relu', solver = 'sgd', verbose = 10, \\\n",
        "                   random_state = 80, max_iter = 10000)\n",
        "\n",
        "mlp"
      ],
      "id": "vZe8h3y5IWsG",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(10, 7, 4), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=10000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=80, shuffle=True, solver='sgd',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=10,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CQhXuSB1JW_t",
        "outputId": "7a7b4c42-4dd5-47b2-beff-0cc97bf4061c"
      },
      "source": [
        "mlp.fit(xtrain, ytrain)"
      ],
      "id": "CQhXuSB1JW_t",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1387.63940718\n",
            "Iteration 2, loss = 1386.52768555\n",
            "Iteration 3, loss = 1384.72261513\n",
            "Iteration 4, loss = 1380.96362767\n",
            "Iteration 5, loss = 1369.57054993\n",
            "Iteration 6, loss = 1325.08318683\n",
            "Iteration 7, loss = 1118.01199721\n",
            "Iteration 8, loss = 300.42313706\n",
            "Iteration 9, loss = 0.00723436\n",
            "Iteration 10, loss = 0.00345945\n",
            "Iteration 11, loss = 0.00402197\n",
            "Iteration 12, loss = 0.00461528\n",
            "Iteration 13, loss = 0.00521975\n",
            "Iteration 14, loss = 0.00582088\n",
            "Iteration 15, loss = 0.00640814\n",
            "Iteration 16, loss = 0.00697415\n",
            "Iteration 17, loss = 0.00751389\n",
            "Iteration 18, loss = 0.00802424\n",
            "Iteration 19, loss = 0.00850347\n",
            "Iteration 20, loss = 0.00895090\n",
            "Iteration 21, loss = 0.00936664\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(10, 7, 4), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=10000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=80, shuffle=True, solver='sgd',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=10,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dIn3yZSJLV5v",
        "outputId": "1938574c-7940-4187-c0f0-bc32dbb4df70"
      },
      "source": [
        "y_pred = mlp.predict(xtest)\n",
        "y_pred"
      ],
      "id": "dIn3yZSJLV5v",
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, ..., 1, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mRmIsktPMMD"
      },
      "source": [
        "#PrecisiÃ³n de la RNA con la funcion de activacion relu y el solver sgd\n"
      ],
      "id": "8mRmIsktPMMD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DNUz3oDvLqbm",
        "outputId": "e6a7c005-b417-4811-f65e-d2fe2d264433"
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cm = multilabel_confusion_matrix(y_pred, ytest)\n",
        "\n",
        "acc = accuracy_score(ytest, y_pred)\n",
        "\n",
        "print('PrecisiÃ³n de la RNA MLPClassifier: % 2.3f' % acc)"
      ],
      "id": "DNUz3oDvLqbm",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PrecisiÃ³n de la RNA MLPClassifier:  0.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ll-gL43qPBlF",
        "outputId": "1d07e797-1cac-4b3d-b897-7c7180e106bf"
      },
      "source": [
        " cm"
      ],
      "id": "ll-gL43qPBlF",
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 1],\n",
              "        [0, 0]],\n",
              "\n",
              "       [[0, 0],\n",
              "        [1, 0]],\n",
              "\n",
              "       [[0, 0],\n",
              "        [1, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0, 0],\n",
              "        [1, 0]],\n",
              "\n",
              "       [[0, 0],\n",
              "        [1, 0]],\n",
              "\n",
              "       [[0, 0],\n",
              "        [0, 1]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDPLE7ZVPYQG"
      },
      "source": [
        "#PrecisiÃ³n de la RNA con la funcion de activacion tanh y el solver adam"
      ],
      "id": "vDPLE7ZVPYQG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h9NGzS3rO638",
        "outputId": "7dd4792f-dac3-4f77-e336-c03130914472"
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes = (10, 7, 4), activation = 'tanh', solver = 'adam', verbose = 10, \\\n",
        "                   random_state = 80, max_iter = 10000)\n",
        "\n",
        "mlp"
      ],
      "id": "h9NGzS3rO638",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(10, 7, 4), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=10000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=80, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=10,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3JV93U6GPAc5",
        "outputId": "ad70afc1-ffa5-46fb-d4a0-ed60650fb41e"
      },
      "source": [
        "mlp.fit(xtrain, ytrain)"
      ],
      "id": "3JV93U6GPAc5",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 4828, loss = 5.51936903\n",
            "Iteration 4829, loss = 5.51673977\n",
            "Iteration 4830, loss = 5.51411228\n",
            "Iteration 4831, loss = 5.51148654\n",
            "Iteration 4832, loss = 5.50886256\n",
            "Iteration 4833, loss = 5.50624033\n",
            "Iteration 4834, loss = 5.50361986\n",
            "Iteration 4835, loss = 5.50100113\n",
            "Iteration 4836, loss = 5.49838416\n",
            "Iteration 4837, loss = 5.49576894\n",
            "Iteration 4838, loss = 5.49315547\n",
            "Iteration 4839, loss = 5.49054374\n",
            "Iteration 4840, loss = 5.48793376\n",
            "Iteration 4841, loss = 5.48532552\n",
            "Iteration 4842, loss = 5.48271903\n",
            "Iteration 4843, loss = 5.48011427\n",
            "Iteration 4844, loss = 5.47751126\n",
            "Iteration 4845, loss = 5.47490999\n",
            "Iteration 4846, loss = 5.47231045\n",
            "Iteration 4847, loss = 5.46971265\n",
            "Iteration 4848, loss = 5.46711659\n",
            "Iteration 4849, loss = 5.46452226\n",
            "Iteration 4850, loss = 5.46192966\n",
            "Iteration 4851, loss = 5.45933879\n",
            "Iteration 4852, loss = 5.45674965\n",
            "Iteration 4853, loss = 5.45416224\n",
            "Iteration 4854, loss = 5.45157656\n",
            "Iteration 4855, loss = 5.44899260\n",
            "Iteration 4856, loss = 5.44641037\n",
            "Iteration 4857, loss = 5.44382986\n",
            "Iteration 4858, loss = 5.44125107\n",
            "Iteration 4859, loss = 5.43867401\n",
            "Iteration 4860, loss = 5.43609866\n",
            "Iteration 4861, loss = 5.43352503\n",
            "Iteration 4862, loss = 5.43095312\n",
            "Iteration 4863, loss = 5.42838292\n",
            "Iteration 4864, loss = 5.42581444\n",
            "Iteration 4865, loss = 5.42324767\n",
            "Iteration 4866, loss = 5.42068261\n",
            "Iteration 4867, loss = 5.41811926\n",
            "Iteration 4868, loss = 5.41555762\n",
            "Iteration 4869, loss = 5.41299769\n",
            "Iteration 4870, loss = 5.41043946\n",
            "Iteration 4871, loss = 5.40788294\n",
            "Iteration 4872, loss = 5.40532812\n",
            "Iteration 4873, loss = 5.40277501\n",
            "Iteration 4874, loss = 5.40022360\n",
            "Iteration 4875, loss = 5.39767388\n",
            "Iteration 4876, loss = 5.39512587\n",
            "Iteration 4877, loss = 5.39257955\n",
            "Iteration 4878, loss = 5.39003493\n",
            "Iteration 4879, loss = 5.38749200\n",
            "Iteration 4880, loss = 5.38495077\n",
            "Iteration 4881, loss = 5.38241123\n",
            "Iteration 4882, loss = 5.37987338\n",
            "Iteration 4883, loss = 5.37733722\n",
            "Iteration 4884, loss = 5.37480275\n",
            "Iteration 4885, loss = 5.37226996\n",
            "Iteration 4886, loss = 5.36973886\n",
            "Iteration 4887, loss = 5.36720945\n",
            "Iteration 4888, loss = 5.36468172\n",
            "Iteration 4889, loss = 5.36215567\n",
            "Iteration 4890, loss = 5.35963130\n",
            "Iteration 4891, loss = 5.35710862\n",
            "Iteration 4892, loss = 5.35458761\n",
            "Iteration 4893, loss = 5.35206827\n",
            "Iteration 4894, loss = 5.34955062\n",
            "Iteration 4895, loss = 5.34703463\n",
            "Iteration 4896, loss = 5.34452032\n",
            "Iteration 4897, loss = 5.34200769\n",
            "Iteration 4898, loss = 5.33949672\n",
            "Iteration 4899, loss = 5.33698742\n",
            "Iteration 4900, loss = 5.33447979\n",
            "Iteration 4901, loss = 5.33197383\n",
            "Iteration 4902, loss = 5.32946954\n",
            "Iteration 4903, loss = 5.32696690\n",
            "Iteration 4904, loss = 5.32446594\n",
            "Iteration 4905, loss = 5.32196663\n",
            "Iteration 4906, loss = 5.31946898\n",
            "Iteration 4907, loss = 5.31697300\n",
            "Iteration 4908, loss = 5.31447867\n",
            "Iteration 4909, loss = 5.31198600\n",
            "Iteration 4910, loss = 5.30949498\n",
            "Iteration 4911, loss = 5.30700562\n",
            "Iteration 4912, loss = 5.30451791\n",
            "Iteration 4913, loss = 5.30203186\n",
            "Iteration 4914, loss = 5.29954745\n",
            "Iteration 4915, loss = 5.29706470\n",
            "Iteration 4916, loss = 5.29458359\n",
            "Iteration 4917, loss = 5.29210413\n",
            "Iteration 4918, loss = 5.28962632\n",
            "Iteration 4919, loss = 5.28715015\n",
            "Iteration 4920, loss = 5.28467562\n",
            "Iteration 4921, loss = 5.28220274\n",
            "Iteration 4922, loss = 5.27973150\n",
            "Iteration 4923, loss = 5.27726190\n",
            "Iteration 4924, loss = 5.27479393\n",
            "Iteration 4925, loss = 5.27232760\n",
            "Iteration 4926, loss = 5.26986291\n",
            "Iteration 4927, loss = 5.26739986\n",
            "Iteration 4928, loss = 5.26493843\n",
            "Iteration 4929, loss = 5.26247864\n",
            "Iteration 4930, loss = 5.26002048\n",
            "Iteration 4931, loss = 5.25756396\n",
            "Iteration 4932, loss = 5.25510906\n",
            "Iteration 4933, loss = 5.25265578\n",
            "Iteration 4934, loss = 5.25020414\n",
            "Iteration 4935, loss = 5.24775411\n",
            "Iteration 4936, loss = 5.24530572\n",
            "Iteration 4937, loss = 5.24285894\n",
            "Iteration 4938, loss = 5.24041379\n",
            "Iteration 4939, loss = 5.23797025\n",
            "Iteration 4940, loss = 5.23552834\n",
            "Iteration 4941, loss = 5.23308804\n",
            "Iteration 4942, loss = 5.23064936\n",
            "Iteration 4943, loss = 5.22821230\n",
            "Iteration 4944, loss = 5.22577685\n",
            "Iteration 4945, loss = 5.22334301\n",
            "Iteration 4946, loss = 5.22091079\n",
            "Iteration 4947, loss = 5.21848017\n",
            "Iteration 4948, loss = 5.21605117\n",
            "Iteration 4949, loss = 5.21362377\n",
            "Iteration 4950, loss = 5.21119798\n",
            "Iteration 4951, loss = 5.20877379\n",
            "Iteration 4952, loss = 5.20635122\n",
            "Iteration 4953, loss = 5.20393024\n",
            "Iteration 4954, loss = 5.20151086\n",
            "Iteration 4955, loss = 5.19909309\n",
            "Iteration 4956, loss = 5.19667692\n",
            "Iteration 4957, loss = 5.19426234\n",
            "Iteration 4958, loss = 5.19184937\n",
            "Iteration 4959, loss = 5.18943799\n",
            "Iteration 4960, loss = 5.18702820\n",
            "Iteration 4961, loss = 5.18462001\n",
            "Iteration 4962, loss = 5.18221341\n",
            "Iteration 4963, loss = 5.17980840\n",
            "Iteration 4964, loss = 5.17740498\n",
            "Iteration 4965, loss = 5.17500316\n",
            "Iteration 4966, loss = 5.17260292\n",
            "Iteration 4967, loss = 5.17020426\n",
            "Iteration 4968, loss = 5.16780720\n",
            "Iteration 4969, loss = 5.16541171\n",
            "Iteration 4970, loss = 5.16301781\n",
            "Iteration 4971, loss = 5.16062549\n",
            "Iteration 4972, loss = 5.15823476\n",
            "Iteration 4973, loss = 5.15584560\n",
            "Iteration 4974, loss = 5.15345802\n",
            "Iteration 4975, loss = 5.15107202\n",
            "Iteration 4976, loss = 5.14868760\n",
            "Iteration 4977, loss = 5.14630475\n",
            "Iteration 4978, loss = 5.14392347\n",
            "Iteration 4979, loss = 5.14154377\n",
            "Iteration 4980, loss = 5.13916564\n",
            "Iteration 4981, loss = 5.13678908\n",
            "Iteration 4982, loss = 5.13441408\n",
            "Iteration 4983, loss = 5.13204066\n",
            "Iteration 4984, loss = 5.12966880\n",
            "Iteration 4985, loss = 5.12729851\n",
            "Iteration 4986, loss = 5.12492979\n",
            "Iteration 4987, loss = 5.12256262\n",
            "Iteration 4988, loss = 5.12019702\n",
            "Iteration 4989, loss = 5.11783298\n",
            "Iteration 4990, loss = 5.11547050\n",
            "Iteration 4991, loss = 5.11310958\n",
            "Iteration 4992, loss = 5.11075022\n",
            "Iteration 4993, loss = 5.10839241\n",
            "Iteration 4994, loss = 5.10603616\n",
            "Iteration 4995, loss = 5.10368147\n",
            "Iteration 4996, loss = 5.10132832\n",
            "Iteration 4997, loss = 5.09897673\n",
            "Iteration 4998, loss = 5.09662669\n",
            "Iteration 4999, loss = 5.09427820\n",
            "Iteration 5000, loss = 5.09193126\n",
            "Iteration 5001, loss = 5.08958586\n",
            "Iteration 5002, loss = 5.08724201\n",
            "Iteration 5003, loss = 5.08489971\n",
            "Iteration 5004, loss = 5.08255895\n",
            "Iteration 5005, loss = 5.08021973\n",
            "Iteration 5006, loss = 5.07788205\n",
            "Iteration 5007, loss = 5.07554592\n",
            "Iteration 5008, loss = 5.07321132\n",
            "Iteration 5009, loss = 5.07087827\n",
            "Iteration 5010, loss = 5.06854675\n",
            "Iteration 5011, loss = 5.06621676\n",
            "Iteration 5012, loss = 5.06388831\n",
            "Iteration 5013, loss = 5.06156139\n",
            "Iteration 5014, loss = 5.05923601\n",
            "Iteration 5015, loss = 5.05691216\n",
            "Iteration 5016, loss = 5.05458984\n",
            "Iteration 5017, loss = 5.05226905\n",
            "Iteration 5018, loss = 5.04994978\n",
            "Iteration 5019, loss = 5.04763204\n",
            "Iteration 5020, loss = 5.04531583\n",
            "Iteration 5021, loss = 5.04300115\n",
            "Iteration 5022, loss = 5.04068798\n",
            "Iteration 5023, loss = 5.03837634\n",
            "Iteration 5024, loss = 5.03606622\n",
            "Iteration 5025, loss = 5.03375762\n",
            "Iteration 5026, loss = 5.03145054\n",
            "Iteration 5027, loss = 5.02914498\n",
            "Iteration 5028, loss = 5.02684094\n",
            "Iteration 5029, loss = 5.02453841\n",
            "Iteration 5030, loss = 5.02223739\n",
            "Iteration 5031, loss = 5.01993789\n",
            "Iteration 5032, loss = 5.01763990\n",
            "Iteration 5033, loss = 5.01534342\n",
            "Iteration 5034, loss = 5.01304846\n",
            "Iteration 5035, loss = 5.01075500\n",
            "Iteration 5036, loss = 5.00846305\n",
            "Iteration 5037, loss = 5.00617260\n",
            "Iteration 5038, loss = 5.00388367\n",
            "Iteration 5039, loss = 5.00159623\n",
            "Iteration 5040, loss = 4.99931030\n",
            "Iteration 5041, loss = 4.99702588\n",
            "Iteration 5042, loss = 4.99474295\n",
            "Iteration 5043, loss = 4.99246152\n",
            "Iteration 5044, loss = 4.99018160\n",
            "Iteration 5045, loss = 4.98790317\n",
            "Iteration 5046, loss = 4.98562624\n",
            "Iteration 5047, loss = 4.98335080\n",
            "Iteration 5048, loss = 4.98107686\n",
            "Iteration 5049, loss = 4.97880442\n",
            "Iteration 5050, loss = 4.97653346\n",
            "Iteration 5051, loss = 4.97426400\n",
            "Iteration 5052, loss = 4.97199602\n",
            "Iteration 5053, loss = 4.96972954\n",
            "Iteration 5054, loss = 4.96746455\n",
            "Iteration 5055, loss = 4.96520104\n",
            "Iteration 5056, loss = 4.96293902\n",
            "Iteration 5057, loss = 4.96067848\n",
            "Iteration 5058, loss = 4.95841943\n",
            "Iteration 5059, loss = 4.95616186\n",
            "Iteration 5060, loss = 4.95390577\n",
            "Iteration 5061, loss = 4.95165116\n",
            "Iteration 5062, loss = 4.94939803\n",
            "Iteration 5063, loss = 4.94714638\n",
            "Iteration 5064, loss = 4.94489621\n",
            "Iteration 5065, loss = 4.94264751\n",
            "Iteration 5066, loss = 4.94040029\n",
            "Iteration 5067, loss = 4.93815454\n",
            "Iteration 5068, loss = 4.93591027\n",
            "Iteration 5069, loss = 4.93366746\n",
            "Iteration 5070, loss = 4.93142613\n",
            "Iteration 5071, loss = 4.92918627\n",
            "Iteration 5072, loss = 4.92694788\n",
            "Iteration 5073, loss = 4.92471095\n",
            "Iteration 5074, loss = 4.92247549\n",
            "Iteration 5075, loss = 4.92024150\n",
            "Iteration 5076, loss = 4.91800897\n",
            "Iteration 5077, loss = 4.91577791\n",
            "Iteration 5078, loss = 4.91354830\n",
            "Iteration 5079, loss = 4.91132016\n",
            "Iteration 5080, loss = 4.90909348\n",
            "Iteration 5081, loss = 4.90686826\n",
            "Iteration 5082, loss = 4.90464449\n",
            "Iteration 5083, loss = 4.90242219\n",
            "Iteration 5084, loss = 4.90020134\n",
            "Iteration 5085, loss = 4.89798194\n",
            "Iteration 5086, loss = 4.89576400\n",
            "Iteration 5087, loss = 4.89354751\n",
            "Iteration 5088, loss = 4.89133247\n",
            "Iteration 5089, loss = 4.88911888\n",
            "Iteration 5090, loss = 4.88690674\n",
            "Iteration 5091, loss = 4.88469605\n",
            "Iteration 5092, loss = 4.88248681\n",
            "Iteration 5093, loss = 4.88027901\n",
            "Iteration 5094, loss = 4.87807266\n",
            "Iteration 5095, loss = 4.87586776\n",
            "Iteration 5096, loss = 4.87366430\n",
            "Iteration 5097, loss = 4.87146227\n",
            "Iteration 5098, loss = 4.86926170\n",
            "Iteration 5099, loss = 4.86706256\n",
            "Iteration 5100, loss = 4.86486486\n",
            "Iteration 5101, loss = 4.86266859\n",
            "Iteration 5102, loss = 4.86047377\n",
            "Iteration 5103, loss = 4.85828038\n",
            "Iteration 5104, loss = 4.85608842\n",
            "Iteration 5105, loss = 4.85389790\n",
            "Iteration 5106, loss = 4.85170882\n",
            "Iteration 5107, loss = 4.84952116\n",
            "Iteration 5108, loss = 4.84733494\n",
            "Iteration 5109, loss = 4.84515014\n",
            "Iteration 5110, loss = 4.84296677\n",
            "Iteration 5111, loss = 4.84078483\n",
            "Iteration 5112, loss = 4.83860432\n",
            "Iteration 5113, loss = 4.83642523\n",
            "Iteration 5114, loss = 4.83424757\n",
            "Iteration 5115, loss = 4.83207133\n",
            "Iteration 5116, loss = 4.82989652\n",
            "Iteration 5117, loss = 4.82772312\n",
            "Iteration 5118, loss = 4.82555115\n",
            "Iteration 5119, loss = 4.82338059\n",
            "Iteration 5120, loss = 4.82121145\n",
            "Iteration 5121, loss = 4.81904373\n",
            "Iteration 5122, loss = 4.81687743\n",
            "Iteration 5123, loss = 4.81471254\n",
            "Iteration 5124, loss = 4.81254907\n",
            "Iteration 5125, loss = 4.81038701\n",
            "Iteration 5126, loss = 4.80822636\n",
            "Iteration 5127, loss = 4.80606712\n",
            "Iteration 5128, loss = 4.80390929\n",
            "Iteration 5129, loss = 4.80175287\n",
            "Iteration 5130, loss = 4.79959786\n",
            "Iteration 5131, loss = 4.79744426\n",
            "Iteration 5132, loss = 4.79529206\n",
            "Iteration 5133, loss = 4.79314127\n",
            "Iteration 5134, loss = 4.79099188\n",
            "Iteration 5135, loss = 4.78884390\n",
            "Iteration 5136, loss = 4.78669732\n",
            "Iteration 5137, loss = 4.78455213\n",
            "Iteration 5138, loss = 4.78240835\n",
            "Iteration 5139, loss = 4.78026597\n",
            "Iteration 5140, loss = 4.77812498\n",
            "Iteration 5141, loss = 4.77598539\n",
            "Iteration 5142, loss = 4.77384720\n",
            "Iteration 5143, loss = 4.77171040\n",
            "Iteration 5144, loss = 4.76957500\n",
            "Iteration 5145, loss = 4.76744099\n",
            "Iteration 5146, loss = 4.76530837\n",
            "Iteration 5147, loss = 4.76317714\n",
            "Iteration 5148, loss = 4.76104731\n",
            "Iteration 5149, loss = 4.75891886\n",
            "Iteration 5150, loss = 4.75679179\n",
            "Iteration 5151, loss = 4.75466612\n",
            "Iteration 5152, loss = 4.75254183\n",
            "Iteration 5153, loss = 4.75041893\n",
            "Iteration 5154, loss = 4.74829741\n",
            "Iteration 5155, loss = 4.74617727\n",
            "Iteration 5156, loss = 4.74405852\n",
            "Iteration 5157, loss = 4.74194114\n",
            "Iteration 5158, loss = 4.73982515\n",
            "Iteration 5159, loss = 4.73771053\n",
            "Iteration 5160, loss = 4.73559730\n",
            "Iteration 5161, loss = 4.73348544\n",
            "Iteration 5162, loss = 4.73137495\n",
            "Iteration 5163, loss = 4.72926584\n",
            "Iteration 5164, loss = 4.72715811\n",
            "Iteration 5165, loss = 4.72505174\n",
            "Iteration 5166, loss = 4.72294675\n",
            "Iteration 5167, loss = 4.72084313\n",
            "Iteration 5168, loss = 4.71874088\n",
            "Iteration 5169, loss = 4.71664000\n",
            "Iteration 5170, loss = 4.71454049\n",
            "Iteration 5171, loss = 4.71244235\n",
            "Iteration 5172, loss = 4.71034557\n",
            "Iteration 5173, loss = 4.70825015\n",
            "Iteration 5174, loss = 4.70615610\n",
            "Iteration 5175, loss = 4.70406341\n",
            "Iteration 5176, loss = 4.70197209\n",
            "Iteration 5177, loss = 4.69988213\n",
            "Iteration 5178, loss = 4.69779352\n",
            "Iteration 5179, loss = 4.69570628\n",
            "Iteration 5180, loss = 4.69362039\n",
            "Iteration 5181, loss = 4.69153586\n",
            "Iteration 5182, loss = 4.68945269\n",
            "Iteration 5183, loss = 4.68737087\n",
            "Iteration 5184, loss = 4.68529041\n",
            "Iteration 5185, loss = 4.68321130\n",
            "Iteration 5186, loss = 4.68113354\n",
            "Iteration 5187, loss = 4.67905714\n",
            "Iteration 5188, loss = 4.67698208\n",
            "Iteration 5189, loss = 4.67490837\n",
            "Iteration 5190, loss = 4.67283602\n",
            "Iteration 5191, loss = 4.67076501\n",
            "Iteration 5192, loss = 4.66869534\n",
            "Iteration 5193, loss = 4.66662703\n",
            "Iteration 5194, loss = 4.66456005\n",
            "Iteration 5195, loss = 4.66249442\n",
            "Iteration 5196, loss = 4.66043014\n",
            "Iteration 5197, loss = 4.65836719\n",
            "Iteration 5198, loss = 4.65630559\n",
            "Iteration 5199, loss = 4.65424532\n",
            "Iteration 5200, loss = 4.65218640\n",
            "Iteration 5201, loss = 4.65012881\n",
            "Iteration 5202, loss = 4.64807256\n",
            "Iteration 5203, loss = 4.64601765\n",
            "Iteration 5204, loss = 4.64396407\n",
            "Iteration 5205, loss = 4.64191182\n",
            "Iteration 5206, loss = 4.63986091\n",
            "Iteration 5207, loss = 4.63781133\n",
            "Iteration 5208, loss = 4.63576308\n",
            "Iteration 5209, loss = 4.63371616\n",
            "Iteration 5210, loss = 4.63167057\n",
            "Iteration 5211, loss = 4.62962631\n",
            "Iteration 5212, loss = 4.62758338\n",
            "Iteration 5213, loss = 4.62554177\n",
            "Iteration 5214, loss = 4.62350149\n",
            "Iteration 5215, loss = 4.62146254\n",
            "Iteration 5216, loss = 4.61942490\n",
            "Iteration 5217, loss = 4.61738859\n",
            "Iteration 5218, loss = 4.61535360\n",
            "Iteration 5219, loss = 4.61331994\n",
            "Iteration 5220, loss = 4.61128759\n",
            "Iteration 5221, loss = 4.60925656\n",
            "Iteration 5222, loss = 4.60722685\n",
            "Iteration 5223, loss = 4.60519846\n",
            "Iteration 5224, loss = 4.60317138\n",
            "Iteration 5225, loss = 4.60114562\n",
            "Iteration 5226, loss = 4.59912117\n",
            "Iteration 5227, loss = 4.59709803\n",
            "Iteration 5228, loss = 4.59507621\n",
            "Iteration 5229, loss = 4.59305570\n",
            "Iteration 5230, loss = 4.59103650\n",
            "Iteration 5231, loss = 4.58901861\n",
            "Iteration 5232, loss = 4.58700202\n",
            "Iteration 5233, loss = 4.58498675\n",
            "Iteration 5234, loss = 4.58297278\n",
            "Iteration 5235, loss = 4.58096012\n",
            "Iteration 5236, loss = 4.57894876\n",
            "Iteration 5237, loss = 4.57693871\n",
            "Iteration 5238, loss = 4.57492996\n",
            "Iteration 5239, loss = 4.57292251\n",
            "Iteration 5240, loss = 4.57091636\n",
            "Iteration 5241, loss = 4.56891152\n",
            "Iteration 5242, loss = 4.56690797\n",
            "Iteration 5243, loss = 4.56490572\n",
            "Iteration 5244, loss = 4.56290477\n",
            "Iteration 5245, loss = 4.56090511\n",
            "Iteration 5246, loss = 4.55890675\n",
            "Iteration 5247, loss = 4.55690969\n",
            "Iteration 5248, loss = 4.55491392\n",
            "Iteration 5249, loss = 4.55291944\n",
            "Iteration 5250, loss = 4.55092625\n",
            "Iteration 5251, loss = 4.54893436\n",
            "Iteration 5252, loss = 4.54694376\n",
            "Iteration 5253, loss = 4.54495444\n",
            "Iteration 5254, loss = 4.54296641\n",
            "Iteration 5255, loss = 4.54097967\n",
            "Iteration 5256, loss = 4.53899422\n",
            "Iteration 5257, loss = 4.53701005\n",
            "Iteration 5258, loss = 4.53502717\n",
            "Iteration 5259, loss = 4.53304557\n",
            "Iteration 5260, loss = 4.53106525\n",
            "Iteration 5261, loss = 4.52908622\n",
            "Iteration 5262, loss = 4.52710846\n",
            "Iteration 5263, loss = 4.52513199\n",
            "Iteration 5264, loss = 4.52315680\n",
            "Iteration 5265, loss = 4.52118288\n",
            "Iteration 5266, loss = 4.51921024\n",
            "Iteration 5267, loss = 4.51723888\n",
            "Iteration 5268, loss = 4.51526879\n",
            "Iteration 5269, loss = 4.51329998\n",
            "Iteration 5270, loss = 4.51133244\n",
            "Iteration 5271, loss = 4.50936618\n",
            "Iteration 5272, loss = 4.50740118\n",
            "Iteration 5273, loss = 4.50543746\n",
            "Iteration 5274, loss = 4.50347501\n",
            "Iteration 5275, loss = 4.50151382\n",
            "Iteration 5276, loss = 4.49955391\n",
            "Iteration 5277, loss = 4.49759526\n",
            "Iteration 5278, loss = 4.49563788\n",
            "Iteration 5279, loss = 4.49368177\n",
            "Iteration 5280, loss = 4.49172692\n",
            "Iteration 5281, loss = 4.48977333\n",
            "Iteration 5282, loss = 4.48782101\n",
            "Iteration 5283, loss = 4.48586994\n",
            "Iteration 5284, loss = 4.48392014\n",
            "Iteration 5285, loss = 4.48197160\n",
            "Iteration 5286, loss = 4.48002432\n",
            "Iteration 5287, loss = 4.47807830\n",
            "Iteration 5288, loss = 4.47613354\n",
            "Iteration 5289, loss = 4.47419003\n",
            "Iteration 5290, loss = 4.47224778\n",
            "Iteration 5291, loss = 4.47030678\n",
            "Iteration 5292, loss = 4.46836704\n",
            "Iteration 5293, loss = 4.46642855\n",
            "Iteration 5294, loss = 4.46449131\n",
            "Iteration 5295, loss = 4.46255533\n",
            "Iteration 5296, loss = 4.46062059\n",
            "Iteration 5297, loss = 4.45868710\n",
            "Iteration 5298, loss = 4.45675487\n",
            "Iteration 5299, loss = 4.45482388\n",
            "Iteration 5300, loss = 4.45289414\n",
            "Iteration 5301, loss = 4.45096564\n",
            "Iteration 5302, loss = 4.44903839\n",
            "Iteration 5303, loss = 4.44711238\n",
            "Iteration 5304, loss = 4.44518762\n",
            "Iteration 5305, loss = 4.44326410\n",
            "Iteration 5306, loss = 4.44134182\n",
            "Iteration 5307, loss = 4.43942079\n",
            "Iteration 5308, loss = 4.43750099\n",
            "Iteration 5309, loss = 4.43558243\n",
            "Iteration 5310, loss = 4.43366511\n",
            "Iteration 5311, loss = 4.43174903\n",
            "Iteration 5312, loss = 4.42983418\n",
            "Iteration 5313, loss = 4.42792057\n",
            "Iteration 5314, loss = 4.42600819\n",
            "Iteration 5315, loss = 4.42409705\n",
            "Iteration 5316, loss = 4.42218714\n",
            "Iteration 5317, loss = 4.42027847\n",
            "Iteration 5318, loss = 4.41837102\n",
            "Iteration 5319, loss = 4.41646481\n",
            "Iteration 5320, loss = 4.41455982\n",
            "Iteration 5321, loss = 4.41265606\n",
            "Iteration 5322, loss = 4.41075353\n",
            "Iteration 5323, loss = 4.40885223\n",
            "Iteration 5324, loss = 4.40695216\n",
            "Iteration 5325, loss = 4.40505331\n",
            "Iteration 5326, loss = 4.40315568\n",
            "Iteration 5327, loss = 4.40125928\n",
            "Iteration 5328, loss = 4.39936410\n",
            "Iteration 5329, loss = 4.39747014\n",
            "Iteration 5330, loss = 4.39557740\n",
            "Iteration 5331, loss = 4.39368588\n",
            "Iteration 5332, loss = 4.39179559\n",
            "Iteration 5333, loss = 4.38990651\n",
            "Iteration 5334, loss = 4.38801864\n",
            "Iteration 5335, loss = 4.38613200\n",
            "Iteration 5336, loss = 4.38424657\n",
            "Iteration 5337, loss = 4.38236235\n",
            "Iteration 5338, loss = 4.38047935\n",
            "Iteration 5339, loss = 4.37859757\n",
            "Iteration 5340, loss = 4.37671699\n",
            "Iteration 5341, loss = 4.37483763\n",
            "Iteration 5342, loss = 4.37295948\n",
            "Iteration 5343, loss = 4.37108253\n",
            "Iteration 5344, loss = 4.36920680\n",
            "Iteration 5345, loss = 4.36733227\n",
            "Iteration 5346, loss = 4.36545895\n",
            "Iteration 5347, loss = 4.36358684\n",
            "Iteration 5348, loss = 4.36171594\n",
            "Iteration 5349, loss = 4.35984624\n",
            "Iteration 5350, loss = 4.35797774\n",
            "Iteration 5351, loss = 4.35611044\n",
            "Iteration 5352, loss = 4.35424435\n",
            "Iteration 5353, loss = 4.35237946\n",
            "Iteration 5354, loss = 4.35051577\n",
            "Iteration 5355, loss = 4.34865328\n",
            "Iteration 5356, loss = 4.34679199\n",
            "Iteration 5357, loss = 4.34493189\n",
            "Iteration 5358, loss = 4.34307300\n",
            "Iteration 5359, loss = 4.34121530\n",
            "Iteration 5360, loss = 4.33935879\n",
            "Iteration 5361, loss = 4.33750348\n",
            "Iteration 5362, loss = 4.33564937\n",
            "Iteration 5363, loss = 4.33379644\n",
            "Iteration 5364, loss = 4.33194471\n",
            "Iteration 5365, loss = 4.33009417\n",
            "Iteration 5366, loss = 4.32824482\n",
            "Iteration 5367, loss = 4.32639667\n",
            "Iteration 5368, loss = 4.32454969\n",
            "Iteration 5369, loss = 4.32270391\n",
            "Iteration 5370, loss = 4.32085932\n",
            "Iteration 5371, loss = 4.31901591\n",
            "Iteration 5372, loss = 4.31717369\n",
            "Iteration 5373, loss = 4.31533265\n",
            "Iteration 5374, loss = 4.31349279\n",
            "Iteration 5375, loss = 4.31165412\n",
            "Iteration 5376, loss = 4.30981663\n",
            "Iteration 5377, loss = 4.30798032\n",
            "Iteration 5378, loss = 4.30614520\n",
            "Iteration 5379, loss = 4.30431125\n",
            "Iteration 5380, loss = 4.30247848\n",
            "Iteration 5381, loss = 4.30064689\n",
            "Iteration 5382, loss = 4.29881648\n",
            "Iteration 5383, loss = 4.29698724\n",
            "Iteration 5384, loss = 4.29515918\n",
            "Iteration 5385, loss = 4.29333229\n",
            "Iteration 5386, loss = 4.29150658\n",
            "Iteration 5387, loss = 4.28968204\n",
            "Iteration 5388, loss = 4.28785867\n",
            "Iteration 5389, loss = 4.28603648\n",
            "Iteration 5390, loss = 4.28421545\n",
            "Iteration 5391, loss = 4.28239560\n",
            "Iteration 5392, loss = 4.28057691\n",
            "Iteration 5393, loss = 4.27875940\n",
            "Iteration 5394, loss = 4.27694305\n",
            "Iteration 5395, loss = 4.27512786\n",
            "Iteration 5396, loss = 4.27331385\n",
            "Iteration 5397, loss = 4.27150099\n",
            "Iteration 5398, loss = 4.26968931\n",
            "Iteration 5399, loss = 4.26787878\n",
            "Iteration 5400, loss = 4.26606942\n",
            "Iteration 5401, loss = 4.26426122\n",
            "Iteration 5402, loss = 4.26245418\n",
            "Iteration 5403, loss = 4.26064830\n",
            "Iteration 5404, loss = 4.25884358\n",
            "Iteration 5405, loss = 4.25704002\n",
            "Iteration 5406, loss = 4.25523762\n",
            "Iteration 5407, loss = 4.25343637\n",
            "Iteration 5408, loss = 4.25163628\n",
            "Iteration 5409, loss = 4.24983735\n",
            "Iteration 5410, loss = 4.24803957\n",
            "Iteration 5411, loss = 4.24624294\n",
            "Iteration 5412, loss = 4.24444747\n",
            "Iteration 5413, loss = 4.24265315\n",
            "Iteration 5414, loss = 4.24085998\n",
            "Iteration 5415, loss = 4.23906796\n",
            "Iteration 5416, loss = 4.23727709\n",
            "Iteration 5417, loss = 4.23548737\n",
            "Iteration 5418, loss = 4.23369880\n",
            "Iteration 5419, loss = 4.23191137\n",
            "Iteration 5420, loss = 4.23012510\n",
            "Iteration 5421, loss = 4.22833996\n",
            "Iteration 5422, loss = 4.22655598\n",
            "Iteration 5423, loss = 4.22477313\n",
            "Iteration 5424, loss = 4.22299143\n",
            "Iteration 5425, loss = 4.22121088\n",
            "Iteration 5426, loss = 4.21943146\n",
            "Iteration 5427, loss = 4.21765319\n",
            "Iteration 5428, loss = 4.21587605\n",
            "Iteration 5429, loss = 4.21410006\n",
            "Iteration 5430, loss = 4.21232520\n",
            "Iteration 5431, loss = 4.21055148\n",
            "Iteration 5432, loss = 4.20877890\n",
            "Iteration 5433, loss = 4.20700746\n",
            "Iteration 5434, loss = 4.20523715\n",
            "Iteration 5435, loss = 4.20346797\n",
            "Iteration 5436, loss = 4.20169993\n",
            "Iteration 5437, loss = 4.19993302\n",
            "Iteration 5438, loss = 4.19816725\n",
            "Iteration 5439, loss = 4.19640261\n",
            "Iteration 5440, loss = 4.19463909\n",
            "Iteration 5441, loss = 4.19287671\n",
            "Iteration 5442, loss = 4.19111546\n",
            "Iteration 5443, loss = 4.18935533\n",
            "Iteration 5444, loss = 4.18759633\n",
            "Iteration 5445, loss = 4.18583846\n",
            "Iteration 5446, loss = 4.18408172\n",
            "Iteration 5447, loss = 4.18232610\n",
            "Iteration 5448, loss = 4.18057160\n",
            "Iteration 5449, loss = 4.17881823\n",
            "Iteration 5450, loss = 4.17706599\n",
            "Iteration 5451, loss = 4.17531486\n",
            "Iteration 5452, loss = 4.17356486\n",
            "Iteration 5453, loss = 4.17181597\n",
            "Iteration 5454, loss = 4.17006821\n",
            "Iteration 5455, loss = 4.16832156\n",
            "Iteration 5456, loss = 4.16657604\n",
            "Iteration 5457, loss = 4.16483163\n",
            "Iteration 5458, loss = 4.16308834\n",
            "Iteration 5459, loss = 4.16134616\n",
            "Iteration 5460, loss = 4.15960510\n",
            "Iteration 5461, loss = 4.15786516\n",
            "Iteration 5462, loss = 4.15612632\n",
            "Iteration 5463, loss = 4.15438861\n",
            "Iteration 5464, loss = 4.15265200\n",
            "Iteration 5465, loss = 4.15091650\n",
            "Iteration 5466, loss = 4.14918212\n",
            "Iteration 5467, loss = 4.14744884\n",
            "Iteration 5468, loss = 4.14571668\n",
            "Iteration 5469, loss = 4.14398562\n",
            "Iteration 5470, loss = 4.14225567\n",
            "Iteration 5471, loss = 4.14052683\n",
            "Iteration 5472, loss = 4.13879909\n",
            "Iteration 5473, loss = 4.13707246\n",
            "Iteration 5474, loss = 4.13534693\n",
            "Iteration 5475, loss = 4.13362251\n",
            "Iteration 5476, loss = 4.13189919\n",
            "Iteration 5477, loss = 4.13017697\n",
            "Iteration 5478, loss = 4.12845585\n",
            "Iteration 5479, loss = 4.12673584\n",
            "Iteration 5480, loss = 4.12501692\n",
            "Iteration 5481, loss = 4.12329911\n",
            "Iteration 5482, loss = 4.12158239\n",
            "Iteration 5483, loss = 4.11986677\n",
            "Iteration 5484, loss = 4.11815224\n",
            "Iteration 5485, loss = 4.11643882\n",
            "Iteration 5486, loss = 4.11472649\n",
            "Iteration 5487, loss = 4.11301525\n",
            "Iteration 5488, loss = 4.11130511\n",
            "Iteration 5489, loss = 4.10959606\n",
            "Iteration 5490, loss = 4.10788810\n",
            "Iteration 5491, loss = 4.10618123\n",
            "Iteration 5492, loss = 4.10447546\n",
            "Iteration 5493, loss = 4.10277077\n",
            "Iteration 5494, loss = 4.10106718\n",
            "Iteration 5495, loss = 4.09936467\n",
            "Iteration 5496, loss = 4.09766326\n",
            "Iteration 5497, loss = 4.09596292\n",
            "Iteration 5498, loss = 4.09426368\n",
            "Iteration 5499, loss = 4.09256552\n",
            "Iteration 5500, loss = 4.09086845\n",
            "Iteration 5501, loss = 4.08917246\n",
            "Iteration 5502, loss = 4.08747755\n",
            "Iteration 5503, loss = 4.08578373\n",
            "Iteration 5504, loss = 4.08409099\n",
            "Iteration 5505, loss = 4.08239933\n",
            "Iteration 5506, loss = 4.08070875\n",
            "Iteration 5507, loss = 4.07901925\n",
            "Iteration 5508, loss = 4.07733083\n",
            "Iteration 5509, loss = 4.07564349\n",
            "Iteration 5510, loss = 4.07395722\n",
            "Iteration 5511, loss = 4.07227203\n",
            "Iteration 5512, loss = 4.07058792\n",
            "Iteration 5513, loss = 4.06890489\n",
            "Iteration 5514, loss = 4.06722292\n",
            "Iteration 5515, loss = 4.06554204\n",
            "Iteration 5516, loss = 4.06386222\n",
            "Iteration 5517, loss = 4.06218348\n",
            "Iteration 5518, loss = 4.06050581\n",
            "Iteration 5519, loss = 4.05882921\n",
            "Iteration 5520, loss = 4.05715368\n",
            "Iteration 5521, loss = 4.05547922\n",
            "Iteration 5522, loss = 4.05380583\n",
            "Iteration 5523, loss = 4.05213350\n",
            "Iteration 5524, loss = 4.05046225\n",
            "Iteration 5525, loss = 4.04879206\n",
            "Iteration 5526, loss = 4.04712293\n",
            "Iteration 5527, loss = 4.04545488\n",
            "Iteration 5528, loss = 4.04378788\n",
            "Iteration 5529, loss = 4.04212195\n",
            "Iteration 5530, loss = 4.04045708\n",
            "Iteration 5531, loss = 4.03879328\n",
            "Iteration 5532, loss = 4.03713053\n",
            "Iteration 5533, loss = 4.03546885\n",
            "Iteration 5534, loss = 4.03380823\n",
            "Iteration 5535, loss = 4.03214866\n",
            "Iteration 5536, loss = 4.03049016\n",
            "Iteration 5537, loss = 4.02883271\n",
            "Iteration 5538, loss = 4.02717632\n",
            "Iteration 5539, loss = 4.02552099\n",
            "Iteration 5540, loss = 4.02386671\n",
            "Iteration 5541, loss = 4.02221349\n",
            "Iteration 5542, loss = 4.02056132\n",
            "Iteration 5543, loss = 4.01891020\n",
            "Iteration 5544, loss = 4.01726014\n",
            "Iteration 5545, loss = 4.01561113\n",
            "Iteration 5546, loss = 4.01396317\n",
            "Iteration 5547, loss = 4.01231626\n",
            "Iteration 5548, loss = 4.01067040\n",
            "Iteration 5549, loss = 4.00902559\n",
            "Iteration 5550, loss = 4.00738183\n",
            "Iteration 5551, loss = 4.00573912\n",
            "Iteration 5552, loss = 4.00409745\n",
            "Iteration 5553, loss = 4.00245683\n",
            "Iteration 5554, loss = 4.00081725\n",
            "Iteration 5555, loss = 3.99917873\n",
            "Iteration 5556, loss = 3.99754124\n",
            "Iteration 5557, loss = 3.99590480\n",
            "Iteration 5558, loss = 3.99426940\n",
            "Iteration 5559, loss = 3.99263504\n",
            "Iteration 5560, loss = 3.99100173\n",
            "Iteration 5561, loss = 3.98936945\n",
            "Iteration 5562, loss = 3.98773822\n",
            "Iteration 5563, loss = 3.98610802\n",
            "Iteration 5564, loss = 3.98447887\n",
            "Iteration 5565, loss = 3.98285075\n",
            "Iteration 5566, loss = 3.98122366\n",
            "Iteration 5567, loss = 3.97959762\n",
            "Iteration 5568, loss = 3.97797261\n",
            "Iteration 5569, loss = 3.97634863\n",
            "Iteration 5570, loss = 3.97472569\n",
            "Iteration 5571, loss = 3.97310379\n",
            "Iteration 5572, loss = 3.97148291\n",
            "Iteration 5573, loss = 3.96986307\n",
            "Iteration 5574, loss = 3.96824426\n",
            "Iteration 5575, loss = 3.96662648\n",
            "Iteration 5576, loss = 3.96500973\n",
            "Iteration 5577, loss = 3.96339401\n",
            "Iteration 5578, loss = 3.96177932\n",
            "Iteration 5579, loss = 3.96016566\n",
            "Iteration 5580, loss = 3.95855302\n",
            "Iteration 5581, loss = 3.95694141\n",
            "Iteration 5582, loss = 3.95533083\n",
            "Iteration 5583, loss = 3.95372127\n",
            "Iteration 5584, loss = 3.95211273\n",
            "Iteration 5585, loss = 3.95050522\n",
            "Iteration 5586, loss = 3.94889874\n",
            "Iteration 5587, loss = 3.94729327\n",
            "Iteration 5588, loss = 3.94568883\n",
            "Iteration 5589, loss = 3.94408541\n",
            "Iteration 5590, loss = 3.94248301\n",
            "Iteration 5591, loss = 3.94088163\n",
            "Iteration 5592, loss = 3.93928126\n",
            "Iteration 5593, loss = 3.93768192\n",
            "Iteration 5594, loss = 3.93608359\n",
            "Iteration 5595, loss = 3.93448628\n",
            "Iteration 5596, loss = 3.93288999\n",
            "Iteration 5597, loss = 3.93129471\n",
            "Iteration 5598, loss = 3.92970045\n",
            "Iteration 5599, loss = 3.92810720\n",
            "Iteration 5600, loss = 3.92651496\n",
            "Iteration 5601, loss = 3.92492374\n",
            "Iteration 5602, loss = 3.92333353\n",
            "Iteration 5603, loss = 3.92174433\n",
            "Iteration 5604, loss = 3.92015614\n",
            "Iteration 5605, loss = 3.91856896\n",
            "Iteration 5606, loss = 3.91698279\n",
            "Iteration 5607, loss = 3.91539763\n",
            "Iteration 5608, loss = 3.91381348\n",
            "Iteration 5609, loss = 3.91223033\n",
            "Iteration 5610, loss = 3.91064819\n",
            "Iteration 5611, loss = 3.90906706\n",
            "Iteration 5612, loss = 3.90748693\n",
            "Iteration 5613, loss = 3.90590781\n",
            "Iteration 5614, loss = 3.90432969\n",
            "Iteration 5615, loss = 3.90275257\n",
            "Iteration 5616, loss = 3.90117646\n",
            "Iteration 5617, loss = 3.89960135\n",
            "Iteration 5618, loss = 3.89802724\n",
            "Iteration 5619, loss = 3.89645413\n",
            "Iteration 5620, loss = 3.89488202\n",
            "Iteration 5621, loss = 3.89331091\n",
            "Iteration 5622, loss = 3.89174079\n",
            "Iteration 5623, loss = 3.89017168\n",
            "Iteration 5624, loss = 3.88860356\n",
            "Iteration 5625, loss = 3.88703644\n",
            "Iteration 5626, loss = 3.88547031\n",
            "Iteration 5627, loss = 3.88390518\n",
            "Iteration 5628, loss = 3.88234105\n",
            "Iteration 5629, loss = 3.88077791\n",
            "Iteration 5630, loss = 3.87921576\n",
            "Iteration 5631, loss = 3.87765460\n",
            "Iteration 5632, loss = 3.87609444\n",
            "Iteration 5633, loss = 3.87453526\n",
            "Iteration 5634, loss = 3.87297708\n",
            "Iteration 5635, loss = 3.87141989\n",
            "Iteration 5636, loss = 3.86986368\n",
            "Iteration 5637, loss = 3.86830847\n",
            "Iteration 5638, loss = 3.86675424\n",
            "Iteration 5639, loss = 3.86520100\n",
            "Iteration 5640, loss = 3.86364874\n",
            "Iteration 5641, loss = 3.86209747\n",
            "Iteration 5642, loss = 3.86054719\n",
            "Iteration 5643, loss = 3.85899789\n",
            "Iteration 5644, loss = 3.85744957\n",
            "Iteration 5645, loss = 3.85590224\n",
            "Iteration 5646, loss = 3.85435589\n",
            "Iteration 5647, loss = 3.85281052\n",
            "Iteration 5648, loss = 3.85126614\n",
            "Iteration 5649, loss = 3.84972273\n",
            "Iteration 5650, loss = 3.84818030\n",
            "Iteration 5651, loss = 3.84663886\n",
            "Iteration 5652, loss = 3.84509839\n",
            "Iteration 5653, loss = 3.84355889\n",
            "Iteration 5654, loss = 3.84202038\n",
            "Iteration 5655, loss = 3.84048284\n",
            "Iteration 5656, loss = 3.83894628\n",
            "Iteration 5657, loss = 3.83741069\n",
            "Iteration 5658, loss = 3.83587608\n",
            "Iteration 5659, loss = 3.83434244\n",
            "Iteration 5660, loss = 3.83280978\n",
            "Iteration 5661, loss = 3.83127809\n",
            "Iteration 5662, loss = 3.82974737\n",
            "Iteration 5663, loss = 3.82821762\n",
            "Iteration 5664, loss = 3.82668884\n",
            "Iteration 5665, loss = 3.82516103\n",
            "Iteration 5666, loss = 3.82363419\n",
            "Iteration 5667, loss = 3.82210832\n",
            "Iteration 5668, loss = 3.82058342\n",
            "Iteration 5669, loss = 3.81905948\n",
            "Iteration 5670, loss = 3.81753651\n",
            "Iteration 5671, loss = 3.81601451\n",
            "Iteration 5672, loss = 3.81449347\n",
            "Iteration 5673, loss = 3.81297340\n",
            "Iteration 5674, loss = 3.81145429\n",
            "Iteration 5675, loss = 3.80993615\n",
            "Iteration 5676, loss = 3.80841896\n",
            "Iteration 5677, loss = 3.80690274\n",
            "Iteration 5678, loss = 3.80538749\n",
            "Iteration 5679, loss = 3.80387319\n",
            "Iteration 5680, loss = 3.80235985\n",
            "Iteration 5681, loss = 3.80084747\n",
            "Iteration 5682, loss = 3.79933605\n",
            "Iteration 5683, loss = 3.79782559\n",
            "Iteration 5684, loss = 3.79631609\n",
            "Iteration 5685, loss = 3.79480755\n",
            "Iteration 5686, loss = 3.79329996\n",
            "Iteration 5687, loss = 3.79179332\n",
            "Iteration 5688, loss = 3.79028764\n",
            "Iteration 5689, loss = 3.78878292\n",
            "Iteration 5690, loss = 3.78727915\n",
            "Iteration 5691, loss = 3.78577633\n",
            "Iteration 5692, loss = 3.78427447\n",
            "Iteration 5693, loss = 3.78277356\n",
            "Iteration 5694, loss = 3.78127359\n",
            "Iteration 5695, loss = 3.77977458\n",
            "Iteration 5696, loss = 3.77827652\n",
            "Iteration 5697, loss = 3.77677941\n",
            "Iteration 5698, loss = 3.77528325\n",
            "Iteration 5699, loss = 3.77378803\n",
            "Iteration 5700, loss = 3.77229377\n",
            "Iteration 5701, loss = 3.77080045\n",
            "Iteration 5702, loss = 3.76930807\n",
            "Iteration 5703, loss = 3.76781665\n",
            "Iteration 5704, loss = 3.76632616\n",
            "Iteration 5705, loss = 3.76483662\n",
            "Iteration 5706, loss = 3.76334803\n",
            "Iteration 5707, loss = 3.76186037\n",
            "Iteration 5708, loss = 3.76037366\n",
            "Iteration 5709, loss = 3.75888790\n",
            "Iteration 5710, loss = 3.75740307\n",
            "Iteration 5711, loss = 3.75591918\n",
            "Iteration 5712, loss = 3.75443624\n",
            "Iteration 5713, loss = 3.75295423\n",
            "Iteration 5714, loss = 3.75147316\n",
            "Iteration 5715, loss = 3.74999303\n",
            "Iteration 5716, loss = 3.74851383\n",
            "Iteration 5717, loss = 3.74703558\n",
            "Iteration 5718, loss = 3.74555826\n",
            "Iteration 5719, loss = 3.74408187\n",
            "Iteration 5720, loss = 3.74260642\n",
            "Iteration 5721, loss = 3.74113191\n",
            "Iteration 5722, loss = 3.73965832\n",
            "Iteration 5723, loss = 3.73818567\n",
            "Iteration 5724, loss = 3.73671396\n",
            "Iteration 5725, loss = 3.73524317\n",
            "Iteration 5726, loss = 3.73377332\n",
            "Iteration 5727, loss = 3.73230439\n",
            "Iteration 5728, loss = 3.73083640\n",
            "Iteration 5729, loss = 3.72936934\n",
            "Iteration 5730, loss = 3.72790320\n",
            "Iteration 5731, loss = 3.72643799\n",
            "Iteration 5732, loss = 3.72497371\n",
            "Iteration 5733, loss = 3.72351036\n",
            "Iteration 5734, loss = 3.72204793\n",
            "Iteration 5735, loss = 3.72058643\n",
            "Iteration 5736, loss = 3.71912585\n",
            "Iteration 5737, loss = 3.71766620\n",
            "Iteration 5738, loss = 3.71620747\n",
            "Iteration 5739, loss = 3.71474967\n",
            "Iteration 5740, loss = 3.71329278\n",
            "Iteration 5741, loss = 3.71183682\n",
            "Iteration 5742, loss = 3.71038178\n",
            "Iteration 5743, loss = 3.70892766\n",
            "Iteration 5744, loss = 3.70747446\n",
            "Iteration 5745, loss = 3.70602218\n",
            "Iteration 5746, loss = 3.70457082\n",
            "Iteration 5747, loss = 3.70312038\n",
            "Iteration 5748, loss = 3.70167085\n",
            "Iteration 5749, loss = 3.70022225\n",
            "Iteration 5750, loss = 3.69877455\n",
            "Iteration 5751, loss = 3.69732778\n",
            "Iteration 5752, loss = 3.69588192\n",
            "Iteration 5753, loss = 3.69443697\n",
            "Iteration 5754, loss = 3.69299294\n",
            "Iteration 5755, loss = 3.69154982\n",
            "Iteration 5756, loss = 3.69010761\n",
            "Iteration 5757, loss = 3.68866632\n",
            "Iteration 5758, loss = 3.68722593\n",
            "Iteration 5759, loss = 3.68578646\n",
            "Iteration 5760, loss = 3.68434790\n",
            "Iteration 5761, loss = 3.68291025\n",
            "Iteration 5762, loss = 3.68147351\n",
            "Iteration 5763, loss = 3.68003767\n",
            "Iteration 5764, loss = 3.67860274\n",
            "Iteration 5765, loss = 3.67716873\n",
            "Iteration 5766, loss = 3.67573561\n",
            "Iteration 5767, loss = 3.67430341\n",
            "Iteration 5768, loss = 3.67287211\n",
            "Iteration 5769, loss = 3.67144171\n",
            "Iteration 5770, loss = 3.67001222\n",
            "Iteration 5771, loss = 3.66858363\n",
            "Iteration 5772, loss = 3.66715594\n",
            "Iteration 5773, loss = 3.66572916\n",
            "Iteration 5774, loss = 3.66430328\n",
            "Iteration 5775, loss = 3.66287830\n",
            "Iteration 5776, loss = 3.66145422\n",
            "Iteration 5777, loss = 3.66003104\n",
            "Iteration 5778, loss = 3.65860876\n",
            "Iteration 5779, loss = 3.65718738\n",
            "Iteration 5780, loss = 3.65576690\n",
            "Iteration 5781, loss = 3.65434732\n",
            "Iteration 5782, loss = 3.65292863\n",
            "Iteration 5783, loss = 3.65151084\n",
            "Iteration 5784, loss = 3.65009394\n",
            "Iteration 5785, loss = 3.64867795\n",
            "Iteration 5786, loss = 3.64726284\n",
            "Iteration 5787, loss = 3.64584863\n",
            "Iteration 5788, loss = 3.64443531\n",
            "Iteration 5789, loss = 3.64302289\n",
            "Iteration 5790, loss = 3.64161136\n",
            "Iteration 5791, loss = 3.64020072\n",
            "Iteration 5792, loss = 3.63879097\n",
            "Iteration 5793, loss = 3.63738211\n",
            "Iteration 5794, loss = 3.63597414\n",
            "Iteration 5795, loss = 3.63456706\n",
            "Iteration 5796, loss = 3.63316087\n",
            "Iteration 5797, loss = 3.63175557\n",
            "Iteration 5798, loss = 3.63035116\n",
            "Iteration 5799, loss = 3.62894763\n",
            "Iteration 5800, loss = 3.62754499\n",
            "Iteration 5801, loss = 3.62614324\n",
            "Iteration 5802, loss = 3.62474237\n",
            "Iteration 5803, loss = 3.62334238\n",
            "Iteration 5804, loss = 3.62194328\n",
            "Iteration 5805, loss = 3.62054506\n",
            "Iteration 5806, loss = 3.61914773\n",
            "Iteration 5807, loss = 3.61775128\n",
            "Iteration 5808, loss = 3.61635571\n",
            "Iteration 5809, loss = 3.61496102\n",
            "Iteration 5810, loss = 3.61356721\n",
            "Iteration 5811, loss = 3.61217428\n",
            "Iteration 5812, loss = 3.61078224\n",
            "Iteration 5813, loss = 3.60939107\n",
            "Iteration 5814, loss = 3.60800078\n",
            "Iteration 5815, loss = 3.60661136\n",
            "Iteration 5816, loss = 3.60522283\n",
            "Iteration 5817, loss = 3.60383517\n",
            "Iteration 5818, loss = 3.60244838\n",
            "Iteration 5819, loss = 3.60106247\n",
            "Iteration 5820, loss = 3.59967744\n",
            "Iteration 5821, loss = 3.59829328\n",
            "Iteration 5822, loss = 3.59691000\n",
            "Iteration 5823, loss = 3.59552758\n",
            "Iteration 5824, loss = 3.59414604\n",
            "Iteration 5825, loss = 3.59276538\n",
            "Iteration 5826, loss = 3.59138558\n",
            "Iteration 5827, loss = 3.59000665\n",
            "Iteration 5828, loss = 3.58862860\n",
            "Iteration 5829, loss = 3.58725141\n",
            "Iteration 5830, loss = 3.58587510\n",
            "Iteration 5831, loss = 3.58449965\n",
            "Iteration 5832, loss = 3.58312507\n",
            "Iteration 5833, loss = 3.58175135\n",
            "Iteration 5834, loss = 3.58037851\n",
            "Iteration 5835, loss = 3.57900653\n",
            "Iteration 5836, loss = 3.57763541\n",
            "Iteration 5837, loss = 3.57626517\n",
            "Iteration 5838, loss = 3.57489578\n",
            "Iteration 5839, loss = 3.57352726\n",
            "Iteration 5840, loss = 3.57215960\n",
            "Iteration 5841, loss = 3.57079281\n",
            "Iteration 5842, loss = 3.56942688\n",
            "Iteration 5843, loss = 3.56806181\n",
            "Iteration 5844, loss = 3.56669760\n",
            "Iteration 5845, loss = 3.56533425\n",
            "Iteration 5846, loss = 3.56397176\n",
            "Iteration 5847, loss = 3.56261013\n",
            "Iteration 5848, loss = 3.56124936\n",
            "Iteration 5849, loss = 3.55988945\n",
            "Iteration 5850, loss = 3.55853039\n",
            "Iteration 5851, loss = 3.55717220\n",
            "Iteration 5852, loss = 3.55581486\n",
            "Iteration 5853, loss = 3.55445837\n",
            "Iteration 5854, loss = 3.55310274\n",
            "Iteration 5855, loss = 3.55174797\n",
            "Iteration 5856, loss = 3.55039405\n",
            "Iteration 5857, loss = 3.54904098\n",
            "Iteration 5858, loss = 3.54768877\n",
            "Iteration 5859, loss = 3.54633741\n",
            "Iteration 5860, loss = 3.54498690\n",
            "Iteration 5861, loss = 3.54363724\n",
            "Iteration 5862, loss = 3.54228844\n",
            "Iteration 5863, loss = 3.54094048\n",
            "Iteration 5864, loss = 3.53959338\n",
            "Iteration 5865, loss = 3.53824712\n",
            "Iteration 5866, loss = 3.53690171\n",
            "Iteration 5867, loss = 3.53555716\n",
            "Iteration 5868, loss = 3.53421344\n",
            "Iteration 5869, loss = 3.53287058\n",
            "Iteration 5870, loss = 3.53152856\n",
            "Iteration 5871, loss = 3.53018739\n",
            "Iteration 5872, loss = 3.52884706\n",
            "Iteration 5873, loss = 3.52750758\n",
            "Iteration 5874, loss = 3.52616895\n",
            "Iteration 5875, loss = 3.52483115\n",
            "Iteration 5876, loss = 3.52349420\n",
            "Iteration 5877, loss = 3.52215809\n",
            "Iteration 5878, loss = 3.52082283\n",
            "Iteration 5879, loss = 3.51948840\n",
            "Iteration 5880, loss = 3.51815482\n",
            "Iteration 5881, loss = 3.51682208\n",
            "Iteration 5882, loss = 3.51549018\n",
            "Iteration 5883, loss = 3.51415911\n",
            "Iteration 5884, loss = 3.51282889\n",
            "Iteration 5885, loss = 3.51149950\n",
            "Iteration 5886, loss = 3.51017095\n",
            "Iteration 5887, loss = 3.50884324\n",
            "Iteration 5888, loss = 3.50751637\n",
            "Iteration 5889, loss = 3.50619033\n",
            "Iteration 5890, loss = 3.50486513\n",
            "Iteration 5891, loss = 3.50354076\n",
            "Iteration 5892, loss = 3.50221722\n",
            "Iteration 5893, loss = 3.50089452\n",
            "Iteration 5894, loss = 3.49957266\n",
            "Iteration 5895, loss = 3.49825162\n",
            "Iteration 5896, loss = 3.49693142\n",
            "Iteration 5897, loss = 3.49561205\n",
            "Iteration 5898, loss = 3.49429351\n",
            "Iteration 5899, loss = 3.49297581\n",
            "Iteration 5900, loss = 3.49165893\n",
            "Iteration 5901, loss = 3.49034288\n",
            "Iteration 5902, loss = 3.48902766\n",
            "Iteration 5903, loss = 3.48771327\n",
            "Iteration 5904, loss = 3.48639970\n",
            "Iteration 5905, loss = 3.48508697\n",
            "Iteration 5906, loss = 3.48377506\n",
            "Iteration 5907, loss = 3.48246398\n",
            "Iteration 5908, loss = 3.48115372\n",
            "Iteration 5909, loss = 3.47984429\n",
            "Iteration 5910, loss = 3.47853568\n",
            "Iteration 5911, loss = 3.47722790\n",
            "Iteration 5912, loss = 3.47592094\n",
            "Iteration 5913, loss = 3.47461480\n",
            "Iteration 5914, loss = 3.47330949\n",
            "Iteration 5915, loss = 3.47200500\n",
            "Iteration 5916, loss = 3.47070132\n",
            "Iteration 5917, loss = 3.46939847\n",
            "Iteration 5918, loss = 3.46809645\n",
            "Iteration 5919, loss = 3.46679524\n",
            "Iteration 5920, loss = 3.46549485\n",
            "Iteration 5921, loss = 3.46419527\n",
            "Iteration 5922, loss = 3.46289652\n",
            "Iteration 5923, loss = 3.46159858\n",
            "Iteration 5924, loss = 3.46030147\n",
            "Iteration 5925, loss = 3.45900516\n",
            "Iteration 5926, loss = 3.45770968\n",
            "Iteration 5927, loss = 3.45641501\n",
            "Iteration 5928, loss = 3.45512115\n",
            "Iteration 5929, loss = 3.45382811\n",
            "Iteration 5930, loss = 3.45253589\n",
            "Iteration 5931, loss = 3.45124447\n",
            "Iteration 5932, loss = 3.44995387\n",
            "Iteration 5933, loss = 3.44866408\n",
            "Iteration 5934, loss = 3.44737511\n",
            "Iteration 5935, loss = 3.44608694\n",
            "Iteration 5936, loss = 3.44479959\n",
            "Iteration 5937, loss = 3.44351304\n",
            "Iteration 5938, loss = 3.44222731\n",
            "Iteration 5939, loss = 3.44094239\n",
            "Iteration 5940, loss = 3.43965827\n",
            "Iteration 5941, loss = 3.43837496\n",
            "Iteration 5942, loss = 3.43709246\n",
            "Iteration 5943, loss = 3.43581077\n",
            "Iteration 5944, loss = 3.43452988\n",
            "Iteration 5945, loss = 3.43324980\n",
            "Iteration 5946, loss = 3.43197052\n",
            "Iteration 5947, loss = 3.43069205\n",
            "Iteration 5948, loss = 3.42941439\n",
            "Iteration 5949, loss = 3.42813753\n",
            "Iteration 5950, loss = 3.42686147\n",
            "Iteration 5951, loss = 3.42558621\n",
            "Iteration 5952, loss = 3.42431176\n",
            "Iteration 5953, loss = 3.42303811\n",
            "Iteration 5954, loss = 3.42176526\n",
            "Iteration 5955, loss = 3.42049321\n",
            "Iteration 5956, loss = 3.41922196\n",
            "Iteration 5957, loss = 3.41795151\n",
            "Iteration 5958, loss = 3.41668186\n",
            "Iteration 5959, loss = 3.41541301\n",
            "Iteration 5960, loss = 3.41414496\n",
            "Iteration 5961, loss = 3.41287770\n",
            "Iteration 5962, loss = 3.41161125\n",
            "Iteration 5963, loss = 3.41034558\n",
            "Iteration 5964, loss = 3.40908072\n",
            "Iteration 5965, loss = 3.40781665\n",
            "Iteration 5966, loss = 3.40655338\n",
            "Iteration 5967, loss = 3.40529090\n",
            "Iteration 5968, loss = 3.40402921\n",
            "Iteration 5969, loss = 3.40276832\n",
            "Iteration 5970, loss = 3.40150822\n",
            "Iteration 5971, loss = 3.40024891\n",
            "Iteration 5972, loss = 3.39899040\n",
            "Iteration 5973, loss = 3.39773268\n",
            "Iteration 5974, loss = 3.39647574\n",
            "Iteration 5975, loss = 3.39521960\n",
            "Iteration 5976, loss = 3.39396425\n",
            "Iteration 5977, loss = 3.39270969\n",
            "Iteration 5978, loss = 3.39145592\n",
            "Iteration 5979, loss = 3.39020293\n",
            "Iteration 5980, loss = 3.38895074\n",
            "Iteration 5981, loss = 3.38769933\n",
            "Iteration 5982, loss = 3.38644870\n",
            "Iteration 5983, loss = 3.38519887\n",
            "Iteration 5984, loss = 3.38394982\n",
            "Iteration 5985, loss = 3.38270155\n",
            "Iteration 5986, loss = 3.38145407\n",
            "Iteration 5987, loss = 3.38020738\n",
            "Iteration 5988, loss = 3.37896147\n",
            "Iteration 5989, loss = 3.37771634\n",
            "Iteration 5990, loss = 3.37647199\n",
            "Iteration 5991, loss = 3.37522843\n",
            "Iteration 5992, loss = 3.37398565\n",
            "Iteration 5993, loss = 3.37274365\n",
            "Iteration 5994, loss = 3.37150243\n",
            "Iteration 5995, loss = 3.37026199\n",
            "Iteration 5996, loss = 3.36902233\n",
            "Iteration 5997, loss = 3.36778345\n",
            "Iteration 5998, loss = 3.36654535\n",
            "Iteration 5999, loss = 3.36530803\n",
            "Iteration 6000, loss = 3.36407149\n",
            "Iteration 6001, loss = 3.36283572\n",
            "Iteration 6002, loss = 3.36160073\n",
            "Iteration 6003, loss = 3.36036651\n",
            "Iteration 6004, loss = 3.35913307\n",
            "Iteration 6005, loss = 3.35790041\n",
            "Iteration 6006, loss = 3.35666852\n",
            "Iteration 6007, loss = 3.35543741\n",
            "Iteration 6008, loss = 3.35420707\n",
            "Iteration 6009, loss = 3.35297750\n",
            "Iteration 6010, loss = 3.35174871\n",
            "Iteration 6011, loss = 3.35052068\n",
            "Iteration 6012, loss = 3.34929343\n",
            "Iteration 6013, loss = 3.34806696\n",
            "Iteration 6014, loss = 3.34684125\n",
            "Iteration 6015, loss = 3.34561631\n",
            "Iteration 6016, loss = 3.34439214\n",
            "Iteration 6017, loss = 3.34316874\n",
            "Iteration 6018, loss = 3.34194611\n",
            "Iteration 6019, loss = 3.34072425\n",
            "Iteration 6020, loss = 3.33950316\n",
            "Iteration 6021, loss = 3.33828283\n",
            "Iteration 6022, loss = 3.33706327\n",
            "Iteration 6023, loss = 3.33584448\n",
            "Iteration 6024, loss = 3.33462645\n",
            "Iteration 6025, loss = 3.33340919\n",
            "Iteration 6026, loss = 3.33219269\n",
            "Iteration 6027, loss = 3.33097696\n",
            "Iteration 6028, loss = 3.32976199\n",
            "Iteration 6029, loss = 3.32854778\n",
            "Iteration 6030, loss = 3.32733434\n",
            "Iteration 6031, loss = 3.32612166\n",
            "Iteration 6032, loss = 3.32490974\n",
            "Iteration 6033, loss = 3.32369858\n",
            "Iteration 6034, loss = 3.32248818\n",
            "Iteration 6035, loss = 3.32127855\n",
            "Iteration 6036, loss = 3.32006967\n",
            "Iteration 6037, loss = 3.31886155\n",
            "Iteration 6038, loss = 3.31765420\n",
            "Iteration 6039, loss = 3.31644760\n",
            "Iteration 6040, loss = 3.31524176\n",
            "Iteration 6041, loss = 3.31403667\n",
            "Iteration 6042, loss = 3.31283234\n",
            "Iteration 6043, loss = 3.31162877\n",
            "Iteration 6044, loss = 3.31042596\n",
            "Iteration 6045, loss = 3.30922390\n",
            "Iteration 6046, loss = 3.30802260\n",
            "Iteration 6047, loss = 3.30682205\n",
            "Iteration 6048, loss = 3.30562225\n",
            "Iteration 6049, loss = 3.30442321\n",
            "Iteration 6050, loss = 3.30322492\n",
            "Iteration 6051, loss = 3.30202739\n",
            "Iteration 6052, loss = 3.30083060\n",
            "Iteration 6053, loss = 3.29963457\n",
            "Iteration 6054, loss = 3.29843929\n",
            "Iteration 6055, loss = 3.29724476\n",
            "Iteration 6056, loss = 3.29605098\n",
            "Iteration 6057, loss = 3.29485795\n",
            "Iteration 6058, loss = 3.29366567\n",
            "Iteration 6059, loss = 3.29247413\n",
            "Iteration 6060, loss = 3.29128335\n",
            "Iteration 6061, loss = 3.29009331\n",
            "Iteration 6062, loss = 3.28890402\n",
            "Iteration 6063, loss = 3.28771548\n",
            "Iteration 6064, loss = 3.28652769\n",
            "Iteration 6065, loss = 3.28534064\n",
            "Iteration 6066, loss = 3.28415433\n",
            "Iteration 6067, loss = 3.28296877\n",
            "Iteration 6068, loss = 3.28178396\n",
            "Iteration 6069, loss = 3.28059988\n",
            "Iteration 6070, loss = 3.27941656\n",
            "Iteration 6071, loss = 3.27823397\n",
            "Iteration 6072, loss = 3.27705213\n",
            "Iteration 6073, loss = 3.27587103\n",
            "Iteration 6074, loss = 3.27469067\n",
            "Iteration 6075, loss = 3.27351105\n",
            "Iteration 6076, loss = 3.27233217\n",
            "Iteration 6077, loss = 3.27115404\n",
            "Iteration 6078, loss = 3.26997664\n",
            "Iteration 6079, loss = 3.26879998\n",
            "Iteration 6080, loss = 3.26762406\n",
            "Iteration 6081, loss = 3.26644888\n",
            "Iteration 6082, loss = 3.26527444\n",
            "Iteration 6083, loss = 3.26410073\n",
            "Iteration 6084, loss = 3.26292776\n",
            "Iteration 6085, loss = 3.26175553\n",
            "Iteration 6086, loss = 3.26058403\n",
            "Iteration 6087, loss = 3.25941327\n",
            "Iteration 6088, loss = 3.25824325\n",
            "Iteration 6089, loss = 3.25707395\n",
            "Iteration 6090, loss = 3.25590539\n",
            "Iteration 6091, loss = 3.25473757\n",
            "Iteration 6092, loss = 3.25357048\n",
            "Iteration 6093, loss = 3.25240412\n",
            "Iteration 6094, loss = 3.25123849\n",
            "Iteration 6095, loss = 3.25007360\n",
            "Iteration 6096, loss = 3.24890943\n",
            "Iteration 6097, loss = 3.24774600\n",
            "Iteration 6098, loss = 3.24658330\n",
            "Iteration 6099, loss = 3.24542132\n",
            "Iteration 6100, loss = 3.24426008\n",
            "Iteration 6101, loss = 3.24309956\n",
            "Iteration 6102, loss = 3.24193978\n",
            "Iteration 6103, loss = 3.24078072\n",
            "Iteration 6104, loss = 3.23962239\n",
            "Iteration 6105, loss = 3.23846478\n",
            "Iteration 6106, loss = 3.23730790\n",
            "Iteration 6107, loss = 3.23615175\n",
            "Iteration 6108, loss = 3.23499633\n",
            "Iteration 6109, loss = 3.23384163\n",
            "Iteration 6110, loss = 3.23268765\n",
            "Iteration 6111, loss = 3.23153440\n",
            "Iteration 6112, loss = 3.23038187\n",
            "Iteration 6113, loss = 3.22923006\n",
            "Iteration 6114, loss = 3.22807898\n",
            "Iteration 6115, loss = 3.22692862\n",
            "Iteration 6116, loss = 3.22577898\n",
            "Iteration 6117, loss = 3.22463006\n",
            "Iteration 6118, loss = 3.22348187\n",
            "Iteration 6119, loss = 3.22233439\n",
            "Iteration 6120, loss = 3.22118764\n",
            "Iteration 6121, loss = 3.22004160\n",
            "Iteration 6122, loss = 3.21889628\n",
            "Iteration 6123, loss = 3.21775169\n",
            "Iteration 6124, loss = 3.21660781\n",
            "Iteration 6125, loss = 3.21546464\n",
            "Iteration 6126, loss = 3.21432220\n",
            "Iteration 6127, loss = 3.21318047\n",
            "Iteration 6128, loss = 3.21203946\n",
            "Iteration 6129, loss = 3.21089916\n",
            "Iteration 6130, loss = 3.20975958\n",
            "Iteration 6131, loss = 3.20862072\n",
            "Iteration 6132, loss = 3.20748257\n",
            "Iteration 6133, loss = 3.20634513\n",
            "Iteration 6134, loss = 3.20520841\n",
            "Iteration 6135, loss = 3.20407240\n",
            "Iteration 6136, loss = 3.20293710\n",
            "Iteration 6137, loss = 3.20180252\n",
            "Iteration 6138, loss = 3.20066864\n",
            "Iteration 6139, loss = 3.19953548\n",
            "Iteration 6140, loss = 3.19840303\n",
            "Iteration 6141, loss = 3.19727129\n",
            "Iteration 6142, loss = 3.19614026\n",
            "Iteration 6143, loss = 3.19500994\n",
            "Iteration 6144, loss = 3.19388033\n",
            "Iteration 6145, loss = 3.19275142\n",
            "Iteration 6146, loss = 3.19162323\n",
            "Iteration 6147, loss = 3.19049574\n",
            "Iteration 6148, loss = 3.18936896\n",
            "Iteration 6149, loss = 3.18824289\n",
            "Iteration 6150, loss = 3.18711752\n",
            "Iteration 6151, loss = 3.18599286\n",
            "Iteration 6152, loss = 3.18486891\n",
            "Iteration 6153, loss = 3.18374565\n",
            "Iteration 6154, loss = 3.18262311\n",
            "Iteration 6155, loss = 3.18150127\n",
            "Iteration 6156, loss = 3.18038013\n",
            "Iteration 6157, loss = 3.17925970\n",
            "Iteration 6158, loss = 3.17813996\n",
            "Iteration 6159, loss = 3.17702093\n",
            "Iteration 6160, loss = 3.17590261\n",
            "Iteration 6161, loss = 3.17478498\n",
            "Iteration 6162, loss = 3.17366805\n",
            "Iteration 6163, loss = 3.17255183\n",
            "Iteration 6164, loss = 3.17143631\n",
            "Iteration 6165, loss = 3.17032148\n",
            "Iteration 6166, loss = 3.16920736\n",
            "Iteration 6167, loss = 3.16809393\n",
            "Iteration 6168, loss = 3.16698120\n",
            "Iteration 6169, loss = 3.16586917\n",
            "Iteration 6170, loss = 3.16475784\n",
            "Iteration 6171, loss = 3.16364720\n",
            "Iteration 6172, loss = 3.16253726\n",
            "Iteration 6173, loss = 3.16142802\n",
            "Iteration 6174, loss = 3.16031947\n",
            "Iteration 6175, loss = 3.15921162\n",
            "Iteration 6176, loss = 3.15810446\n",
            "Iteration 6177, loss = 3.15699800\n",
            "Iteration 6178, loss = 3.15589223\n",
            "Iteration 6179, loss = 3.15478715\n",
            "Iteration 6180, loss = 3.15368277\n",
            "Iteration 6181, loss = 3.15257908\n",
            "Iteration 6182, loss = 3.15147608\n",
            "Iteration 6183, loss = 3.15037377\n",
            "Iteration 6184, loss = 3.14927216\n",
            "Iteration 6185, loss = 3.14817123\n",
            "Iteration 6186, loss = 3.14707100\n",
            "Iteration 6187, loss = 3.14597145\n",
            "Iteration 6188, loss = 3.14487260\n",
            "Iteration 6189, loss = 3.14377443\n",
            "Iteration 6190, loss = 3.14267696\n",
            "Iteration 6191, loss = 3.14158017\n",
            "Iteration 6192, loss = 3.14048407\n",
            "Iteration 6193, loss = 3.13938865\n",
            "Iteration 6194, loss = 3.13829393\n",
            "Iteration 6195, loss = 3.13719988\n",
            "Iteration 6196, loss = 3.13610653\n",
            "Iteration 6197, loss = 3.13501386\n",
            "Iteration 6198, loss = 3.13392188\n",
            "Iteration 6199, loss = 3.13283058\n",
            "Iteration 6200, loss = 3.13173996\n",
            "Iteration 6201, loss = 3.13065003\n",
            "Iteration 6202, loss = 3.12956079\n",
            "Iteration 6203, loss = 3.12847222\n",
            "Iteration 6204, loss = 3.12738434\n",
            "Iteration 6205, loss = 3.12629714\n",
            "Iteration 6206, loss = 3.12521062\n",
            "Iteration 6207, loss = 3.12412479\n",
            "Iteration 6208, loss = 3.12303963\n",
            "Iteration 6209, loss = 3.12195515\n",
            "Iteration 6210, loss = 3.12087136\n",
            "Iteration 6211, loss = 3.11978824\n",
            "Iteration 6212, loss = 3.11870581\n",
            "Iteration 6213, loss = 3.11762405\n",
            "Iteration 6214, loss = 3.11654297\n",
            "Iteration 6215, loss = 3.11546256\n",
            "Iteration 6216, loss = 3.11438284\n",
            "Iteration 6217, loss = 3.11330379\n",
            "Iteration 6218, loss = 3.11222542\n",
            "Iteration 6219, loss = 3.11114773\n",
            "Iteration 6220, loss = 3.11007071\n",
            "Iteration 6221, loss = 3.10899436\n",
            "Iteration 6222, loss = 3.10791869\n",
            "Iteration 6223, loss = 3.10684370\n",
            "Iteration 6224, loss = 3.10576937\n",
            "Iteration 6225, loss = 3.10469573\n",
            "Iteration 6226, loss = 3.10362275\n",
            "Iteration 6227, loss = 3.10255045\n",
            "Iteration 6228, loss = 3.10147882\n",
            "Iteration 6229, loss = 3.10040786\n",
            "Iteration 6230, loss = 3.09933758\n",
            "Iteration 6231, loss = 3.09826796\n",
            "Iteration 6232, loss = 3.09719902\n",
            "Iteration 6233, loss = 3.09613074\n",
            "Iteration 6234, loss = 3.09506314\n",
            "Iteration 6235, loss = 3.09399620\n",
            "Iteration 6236, loss = 3.09292993\n",
            "Iteration 6237, loss = 3.09186433\n",
            "Iteration 6238, loss = 3.09079940\n",
            "Iteration 6239, loss = 3.08973514\n",
            "Iteration 6240, loss = 3.08867154\n",
            "Iteration 6241, loss = 3.08760862\n",
            "Iteration 6242, loss = 3.08654635\n",
            "Iteration 6243, loss = 3.08548476\n",
            "Iteration 6244, loss = 3.08442382\n",
            "Iteration 6245, loss = 3.08336356\n",
            "Iteration 6246, loss = 3.08230396\n",
            "Iteration 6247, loss = 3.08124502\n",
            "Iteration 6248, loss = 3.08018675\n",
            "Iteration 6249, loss = 3.07912914\n",
            "Iteration 6250, loss = 3.07807219\n",
            "Iteration 6251, loss = 3.07701590\n",
            "Iteration 6252, loss = 3.07596028\n",
            "Iteration 6253, loss = 3.07490532\n",
            "Iteration 6254, loss = 3.07385102\n",
            "Iteration 6255, loss = 3.07279738\n",
            "Iteration 6256, loss = 3.07174440\n",
            "Iteration 6257, loss = 3.07069209\n",
            "Iteration 6258, loss = 3.06964043\n",
            "Iteration 6259, loss = 3.06858943\n",
            "Iteration 6260, loss = 3.06753909\n",
            "Iteration 6261, loss = 3.06648940\n",
            "Iteration 6262, loss = 3.06544038\n",
            "Iteration 6263, loss = 3.06439201\n",
            "Iteration 6264, loss = 3.06334430\n",
            "Iteration 6265, loss = 3.06229725\n",
            "Iteration 6266, loss = 3.06125086\n",
            "Iteration 6267, loss = 3.06020511\n",
            "Iteration 6268, loss = 3.05916003\n",
            "Iteration 6269, loss = 3.05811560\n",
            "Iteration 6270, loss = 3.05707182\n",
            "Iteration 6271, loss = 3.05602870\n",
            "Iteration 6272, loss = 3.05498624\n",
            "Iteration 6273, loss = 3.05394442\n",
            "Iteration 6274, loss = 3.05290326\n",
            "Iteration 6275, loss = 3.05186275\n",
            "Iteration 6276, loss = 3.05082290\n",
            "Iteration 6277, loss = 3.04978369\n",
            "Iteration 6278, loss = 3.04874514\n",
            "Iteration 6279, loss = 3.04770724\n",
            "Iteration 6280, loss = 3.04666999\n",
            "Iteration 6281, loss = 3.04563338\n",
            "Iteration 6282, loss = 3.04459743\n",
            "Iteration 6283, loss = 3.04356213\n",
            "Iteration 6284, loss = 3.04252748\n",
            "Iteration 6285, loss = 3.04149347\n",
            "Iteration 6286, loss = 3.04046012\n",
            "Iteration 6287, loss = 3.03942741\n",
            "Iteration 6288, loss = 3.03839535\n",
            "Iteration 6289, loss = 3.03736393\n",
            "Iteration 6290, loss = 3.03633316\n",
            "Iteration 6291, loss = 3.03530304\n",
            "Iteration 6292, loss = 3.03427356\n",
            "Iteration 6293, loss = 3.03324473\n",
            "Iteration 6294, loss = 3.03221654\n",
            "Iteration 6295, loss = 3.03118900\n",
            "Iteration 6296, loss = 3.03016210\n",
            "Iteration 6297, loss = 3.02913585\n",
            "Iteration 6298, loss = 3.02811024\n",
            "Iteration 6299, loss = 3.02708527\n",
            "Iteration 6300, loss = 3.02606094\n",
            "Iteration 6301, loss = 3.02503726\n",
            "Iteration 6302, loss = 3.02401422\n",
            "Iteration 6303, loss = 3.02299182\n",
            "Iteration 6304, loss = 3.02197006\n",
            "Iteration 6305, loss = 3.02094894\n",
            "Iteration 6306, loss = 3.01992845\n",
            "Iteration 6307, loss = 3.01890861\n",
            "Iteration 6308, loss = 3.01788941\n",
            "Iteration 6309, loss = 3.01687085\n",
            "Iteration 6310, loss = 3.01585293\n",
            "Iteration 6311, loss = 3.01483564\n",
            "Iteration 6312, loss = 3.01381899\n",
            "Iteration 6313, loss = 3.01280298\n",
            "Iteration 6314, loss = 3.01178761\n",
            "Iteration 6315, loss = 3.01077287\n",
            "Iteration 6316, loss = 3.00975877\n",
            "Iteration 6317, loss = 3.00874530\n",
            "Iteration 6318, loss = 3.00773247\n",
            "Iteration 6319, loss = 3.00672027\n",
            "Iteration 6320, loss = 3.00570871\n",
            "Iteration 6321, loss = 3.00469778\n",
            "Iteration 6322, loss = 3.00368748\n",
            "Iteration 6323, loss = 3.00267782\n",
            "Iteration 6324, loss = 3.00166879\n",
            "Iteration 6325, loss = 3.00066040\n",
            "Iteration 6326, loss = 2.99965263\n",
            "Iteration 6327, loss = 2.99864550\n",
            "Iteration 6328, loss = 2.99763900\n",
            "Iteration 6329, loss = 2.99663313\n",
            "Iteration 6330, loss = 2.99562789\n",
            "Iteration 6331, loss = 2.99462328\n",
            "Iteration 6332, loss = 2.99361930\n",
            "Iteration 6333, loss = 2.99261595\n",
            "Iteration 6334, loss = 2.99161322\n",
            "Iteration 6335, loss = 2.99061113\n",
            "Iteration 6336, loss = 2.98960966\n",
            "Iteration 6337, loss = 2.98860883\n",
            "Iteration 6338, loss = 2.98760862\n",
            "Iteration 6339, loss = 2.98660903\n",
            "Iteration 6340, loss = 2.98561007\n",
            "Iteration 6341, loss = 2.98461174\n",
            "Iteration 6342, loss = 2.98361404\n",
            "Iteration 6343, loss = 2.98261696\n",
            "Iteration 6344, loss = 2.98162050\n",
            "Iteration 6345, loss = 2.98062467\n",
            "Iteration 6346, loss = 2.97962946\n",
            "Iteration 6347, loss = 2.97863488\n",
            "Iteration 6348, loss = 2.97764092\n",
            "Iteration 6349, loss = 2.97664758\n",
            "Iteration 6350, loss = 2.97565487\n",
            "Iteration 6351, loss = 2.97466278\n",
            "Iteration 6352, loss = 2.97367131\n",
            "Iteration 6353, loss = 2.97268046\n",
            "Iteration 6354, loss = 2.97169023\n",
            "Iteration 6355, loss = 2.97070062\n",
            "Iteration 6356, loss = 2.96971163\n",
            "Iteration 6357, loss = 2.96872327\n",
            "Iteration 6358, loss = 2.96773552\n",
            "Iteration 6359, loss = 2.96674839\n",
            "Iteration 6360, loss = 2.96576188\n",
            "Iteration 6361, loss = 2.96477599\n",
            "Iteration 6362, loss = 2.96379072\n",
            "Iteration 6363, loss = 2.96280606\n",
            "Iteration 6364, loss = 2.96182202\n",
            "Iteration 6365, loss = 2.96083860\n",
            "Iteration 6366, loss = 2.95985579\n",
            "Iteration 6367, loss = 2.95887360\n",
            "Iteration 6368, loss = 2.95789203\n",
            "Iteration 6369, loss = 2.95691107\n",
            "Iteration 6370, loss = 2.95593073\n",
            "Iteration 6371, loss = 2.95495100\n",
            "Iteration 6372, loss = 2.95397188\n",
            "Iteration 6373, loss = 2.95299338\n",
            "Iteration 6374, loss = 2.95201549\n",
            "Iteration 6375, loss = 2.95103821\n",
            "Iteration 6376, loss = 2.95006155\n",
            "Iteration 6377, loss = 2.94908550\n",
            "Iteration 6378, loss = 2.94811006\n",
            "Iteration 6379, loss = 2.94713523\n",
            "Iteration 6380, loss = 2.94616102\n",
            "Iteration 6381, loss = 2.94518741\n",
            "Iteration 6382, loss = 2.94421441\n",
            "Iteration 6383, loss = 2.94324203\n",
            "Iteration 6384, loss = 2.94227025\n",
            "Iteration 6385, loss = 2.94129908\n",
            "Iteration 6386, loss = 2.94032852\n",
            "Iteration 6387, loss = 2.93935857\n",
            "Iteration 6388, loss = 2.93838923\n",
            "Iteration 6389, loss = 2.93742050\n",
            "Iteration 6390, loss = 2.93645237\n",
            "Iteration 6391, loss = 2.93548485\n",
            "Iteration 6392, loss = 2.93451793\n",
            "Iteration 6393, loss = 2.93355162\n",
            "Iteration 6394, loss = 2.93258592\n",
            "Iteration 6395, loss = 2.93162082\n",
            "Iteration 6396, loss = 2.93065633\n",
            "Iteration 6397, loss = 2.92969244\n",
            "Iteration 6398, loss = 2.92872916\n",
            "Iteration 6399, loss = 2.92776648\n",
            "Iteration 6400, loss = 2.92680440\n",
            "Iteration 6401, loss = 2.92584293\n",
            "Iteration 6402, loss = 2.92488206\n",
            "Iteration 6403, loss = 2.92392179\n",
            "Iteration 6404, loss = 2.92296213\n",
            "Iteration 6405, loss = 2.92200306\n",
            "Iteration 6406, loss = 2.92104460\n",
            "Iteration 6407, loss = 2.92008673\n",
            "Iteration 6408, loss = 2.91912947\n",
            "Iteration 6409, loss = 2.91817281\n",
            "Iteration 6410, loss = 2.91721675\n",
            "Iteration 6411, loss = 2.91626128\n",
            "Iteration 6412, loss = 2.91530642\n",
            "Iteration 6413, loss = 2.91435215\n",
            "Iteration 6414, loss = 2.91339849\n",
            "Iteration 6415, loss = 2.91244542\n",
            "Iteration 6416, loss = 2.91149294\n",
            "Iteration 6417, loss = 2.91054107\n",
            "Iteration 6418, loss = 2.90958979\n",
            "Iteration 6419, loss = 2.90863911\n",
            "Iteration 6420, loss = 2.90768902\n",
            "Iteration 6421, loss = 2.90673953\n",
            "Iteration 6422, loss = 2.90579063\n",
            "Iteration 6423, loss = 2.90484233\n",
            "Iteration 6424, loss = 2.90389463\n",
            "Iteration 6425, loss = 2.90294752\n",
            "Iteration 6426, loss = 2.90200100\n",
            "Iteration 6427, loss = 2.90105507\n",
            "Iteration 6428, loss = 2.90010974\n",
            "Iteration 6429, loss = 2.89916500\n",
            "Iteration 6430, loss = 2.89822085\n",
            "Iteration 6431, loss = 2.89727730\n",
            "Iteration 6432, loss = 2.89633433\n",
            "Iteration 6433, loss = 2.89539196\n",
            "Iteration 6434, loss = 2.89445018\n",
            "Iteration 6435, loss = 2.89350899\n",
            "Iteration 6436, loss = 2.89256839\n",
            "Iteration 6437, loss = 2.89162838\n",
            "Iteration 6438, loss = 2.89068895\n",
            "Iteration 6439, loss = 2.88975012\n",
            "Iteration 6440, loss = 2.88881188\n",
            "Iteration 6441, loss = 2.88787422\n",
            "Iteration 6442, loss = 2.88693715\n",
            "Iteration 6443, loss = 2.88600067\n",
            "Iteration 6444, loss = 2.88506478\n",
            "Iteration 6445, loss = 2.88412947\n",
            "Iteration 6446, loss = 2.88319475\n",
            "Iteration 6447, loss = 2.88226061\n",
            "Iteration 6448, loss = 2.88132707\n",
            "Iteration 6449, loss = 2.88039410\n",
            "Iteration 6450, loss = 2.87946172\n",
            "Iteration 6451, loss = 2.87852993\n",
            "Iteration 6452, loss = 2.87759872\n",
            "Iteration 6453, loss = 2.87666809\n",
            "Iteration 6454, loss = 2.87573805\n",
            "Iteration 6455, loss = 2.87480859\n",
            "Iteration 6456, loss = 2.87387972\n",
            "Iteration 6457, loss = 2.87295142\n",
            "Iteration 6458, loss = 2.87202371\n",
            "Iteration 6459, loss = 2.87109658\n",
            "Iteration 6460, loss = 2.87017003\n",
            "Iteration 6461, loss = 2.86924406\n",
            "Iteration 6462, loss = 2.86831867\n",
            "Iteration 6463, loss = 2.86739387\n",
            "Iteration 6464, loss = 2.86646964\n",
            "Iteration 6465, loss = 2.86554599\n",
            "Iteration 6466, loss = 2.86462292\n",
            "Iteration 6467, loss = 2.86370043\n",
            "Iteration 6468, loss = 2.86277852\n",
            "Iteration 6469, loss = 2.86185719\n",
            "Iteration 6470, loss = 2.86093643\n",
            "Iteration 6471, loss = 2.86001626\n",
            "Iteration 6472, loss = 2.85909665\n",
            "Iteration 6473, loss = 2.85817763\n",
            "Iteration 6474, loss = 2.85725918\n",
            "Iteration 6475, loss = 2.85634131\n",
            "Iteration 6476, loss = 2.85542401\n",
            "Iteration 6477, loss = 2.85450729\n",
            "Iteration 6478, loss = 2.85359114\n",
            "Iteration 6479, loss = 2.85267557\n",
            "Iteration 6480, loss = 2.85176058\n",
            "Iteration 6481, loss = 2.85084615\n",
            "Iteration 6482, loss = 2.84993230\n",
            "Iteration 6483, loss = 2.84901902\n",
            "Iteration 6484, loss = 2.84810632\n",
            "Iteration 6485, loss = 2.84719419\n",
            "Iteration 6486, loss = 2.84628263\n",
            "Iteration 6487, loss = 2.84537164\n",
            "Iteration 6488, loss = 2.84446122\n",
            "Iteration 6489, loss = 2.84355137\n",
            "Iteration 6490, loss = 2.84264210\n",
            "Iteration 6491, loss = 2.84173339\n",
            "Iteration 6492, loss = 2.84082526\n",
            "Iteration 6493, loss = 2.83991769\n",
            "Iteration 6494, loss = 2.83901070\n",
            "Iteration 6495, loss = 2.83810427\n",
            "Iteration 6496, loss = 2.83719841\n",
            "Iteration 6497, loss = 2.83629312\n",
            "Iteration 6498, loss = 2.83538840\n",
            "Iteration 6499, loss = 2.83448424\n",
            "Iteration 6500, loss = 2.83358065\n",
            "Iteration 6501, loss = 2.83267763\n",
            "Iteration 6502, loss = 2.83177517\n",
            "Iteration 6503, loss = 2.83087328\n",
            "Iteration 6504, loss = 2.82997196\n",
            "Iteration 6505, loss = 2.82907120\n",
            "Iteration 6506, loss = 2.82817101\n",
            "Iteration 6507, loss = 2.82727138\n",
            "Iteration 6508, loss = 2.82637232\n",
            "Iteration 6509, loss = 2.82547381\n",
            "Iteration 6510, loss = 2.82457588\n",
            "Iteration 6511, loss = 2.82367850\n",
            "Iteration 6512, loss = 2.82278169\n",
            "Iteration 6513, loss = 2.82188545\n",
            "Iteration 6514, loss = 2.82098976\n",
            "Iteration 6515, loss = 2.82009464\n",
            "Iteration 6516, loss = 2.81920007\n",
            "Iteration 6517, loss = 2.81830607\n",
            "Iteration 6518, loss = 2.81741263\n",
            "Iteration 6519, loss = 2.81651975\n",
            "Iteration 6520, loss = 2.81562743\n",
            "Iteration 6521, loss = 2.81473567\n",
            "Iteration 6522, loss = 2.81384447\n",
            "Iteration 6523, loss = 2.81295383\n",
            "Iteration 6524, loss = 2.81206375\n",
            "Iteration 6525, loss = 2.81117422\n",
            "Iteration 6526, loss = 2.81028526\n",
            "Iteration 6527, loss = 2.80939685\n",
            "Iteration 6528, loss = 2.80850900\n",
            "Iteration 6529, loss = 2.80762170\n",
            "Iteration 6530, loss = 2.80673497\n",
            "Iteration 6531, loss = 2.80584878\n",
            "Iteration 6532, loss = 2.80496316\n",
            "Iteration 6533, loss = 2.80407809\n",
            "Iteration 6534, loss = 2.80319358\n",
            "Iteration 6535, loss = 2.80230962\n",
            "Iteration 6536, loss = 2.80142621\n",
            "Iteration 6537, loss = 2.80054336\n",
            "Iteration 6538, loss = 2.79966107\n",
            "Iteration 6539, loss = 2.79877932\n",
            "Iteration 6540, loss = 2.79789813\n",
            "Iteration 6541, loss = 2.79701750\n",
            "Iteration 6542, loss = 2.79613741\n",
            "Iteration 6543, loss = 2.79525788\n",
            "Iteration 6544, loss = 2.79437890\n",
            "Iteration 6545, loss = 2.79350048\n",
            "Iteration 6546, loss = 2.79262260\n",
            "Iteration 6547, loss = 2.79174527\n",
            "Iteration 6548, loss = 2.79086850\n",
            "Iteration 6549, loss = 2.78999227\n",
            "Iteration 6550, loss = 2.78911660\n",
            "Iteration 6551, loss = 2.78824147\n",
            "Iteration 6552, loss = 2.78736689\n",
            "Iteration 6553, loss = 2.78649287\n",
            "Iteration 6554, loss = 2.78561939\n",
            "Iteration 6555, loss = 2.78474646\n",
            "Iteration 6556, loss = 2.78387407\n",
            "Iteration 6557, loss = 2.78300224\n",
            "Iteration 6558, loss = 2.78213095\n",
            "Iteration 6559, loss = 2.78126021\n",
            "Iteration 6560, loss = 2.78039001\n",
            "Iteration 6561, loss = 2.77952036\n",
            "Iteration 6562, loss = 2.77865126\n",
            "Iteration 6563, loss = 2.77778270\n",
            "Iteration 6564, loss = 2.77691469\n",
            "Iteration 6565, loss = 2.77604722\n",
            "Iteration 6566, loss = 2.77518030\n",
            "Iteration 6567, loss = 2.77431392\n",
            "Iteration 6568, loss = 2.77344808\n",
            "Iteration 6569, loss = 2.77258279\n",
            "Iteration 6570, loss = 2.77171804\n",
            "Iteration 6571, loss = 2.77085384\n",
            "Iteration 6572, loss = 2.76999017\n",
            "Iteration 6573, loss = 2.76912705\n",
            "Iteration 6574, loss = 2.76826447\n",
            "Iteration 6575, loss = 2.76740243\n",
            "Iteration 6576, loss = 2.76654094\n",
            "Iteration 6577, loss = 2.76567998\n",
            "Iteration 6578, loss = 2.76481957\n",
            "Iteration 6579, loss = 2.76395969\n",
            "Iteration 6580, loss = 2.76310036\n",
            "Iteration 6581, loss = 2.76224156\n",
            "Iteration 6582, loss = 2.76138330\n",
            "Iteration 6583, loss = 2.76052558\n",
            "Iteration 6584, loss = 2.75966840\n",
            "Iteration 6585, loss = 2.75881176\n",
            "Iteration 6586, loss = 2.75795566\n",
            "Iteration 6587, loss = 2.75710009\n",
            "Iteration 6588, loss = 2.75624506\n",
            "Iteration 6589, loss = 2.75539057\n",
            "Iteration 6590, loss = 2.75453661\n",
            "Iteration 6591, loss = 2.75368320\n",
            "Iteration 6592, loss = 2.75283031\n",
            "Iteration 6593, loss = 2.75197796\n",
            "Iteration 6594, loss = 2.75112615\n",
            "Iteration 6595, loss = 2.75027487\n",
            "Iteration 6596, loss = 2.74942413\n",
            "Iteration 6597, loss = 2.74857392\n",
            "Iteration 6598, loss = 2.74772424\n",
            "Iteration 6599, loss = 2.74687510\n",
            "Iteration 6600, loss = 2.74602649\n",
            "Iteration 6601, loss = 2.74517842\n",
            "Iteration 6602, loss = 2.74433087\n",
            "Iteration 6603, loss = 2.74348386\n",
            "Iteration 6604, loss = 2.74263738\n",
            "Iteration 6605, loss = 2.74179143\n",
            "Iteration 6606, loss = 2.74094602\n",
            "Iteration 6607, loss = 2.74010113\n",
            "Iteration 6608, loss = 2.73925678\n",
            "Iteration 6609, loss = 2.73841295\n",
            "Iteration 6610, loss = 2.73756966\n",
            "Iteration 6611, loss = 2.73672689\n",
            "Iteration 6612, loss = 2.73588466\n",
            "Iteration 6613, loss = 2.73504295\n",
            "Iteration 6614, loss = 2.73420177\n",
            "Iteration 6615, loss = 2.73336112\n",
            "Iteration 6616, loss = 2.73252100\n",
            "Iteration 6617, loss = 2.73168141\n",
            "Iteration 6618, loss = 2.73084234\n",
            "Iteration 6619, loss = 2.73000380\n",
            "Iteration 6620, loss = 2.72916579\n",
            "Iteration 6621, loss = 2.72832830\n",
            "Iteration 6622, loss = 2.72749134\n",
            "Iteration 6623, loss = 2.72665490\n",
            "Iteration 6624, loss = 2.72581899\n",
            "Iteration 6625, loss = 2.72498361\n",
            "Iteration 6626, loss = 2.72414875\n",
            "Iteration 6627, loss = 2.72331442\n",
            "Iteration 6628, loss = 2.72248060\n",
            "Iteration 6629, loss = 2.72164732\n",
            "Iteration 6630, loss = 2.72081455\n",
            "Iteration 6631, loss = 2.71998231\n",
            "Iteration 6632, loss = 2.71915060\n",
            "Iteration 6633, loss = 2.71831940\n",
            "Iteration 6634, loss = 2.71748873\n",
            "Iteration 6635, loss = 2.71665858\n",
            "Iteration 6636, loss = 2.71582895\n",
            "Iteration 6637, loss = 2.71499984\n",
            "Iteration 6638, loss = 2.71417125\n",
            "Iteration 6639, loss = 2.71334318\n",
            "Iteration 6640, loss = 2.71251564\n",
            "Iteration 6641, loss = 2.71168861\n",
            "Iteration 6642, loss = 2.71086211\n",
            "Iteration 6643, loss = 2.71003612\n",
            "Iteration 6644, loss = 2.70921065\n",
            "Iteration 6645, loss = 2.70838570\n",
            "Iteration 6646, loss = 2.70756127\n",
            "Iteration 6647, loss = 2.70673736\n",
            "Iteration 6648, loss = 2.70591396\n",
            "Iteration 6649, loss = 2.70509108\n",
            "Iteration 6650, loss = 2.70426872\n",
            "Iteration 6651, loss = 2.70344688\n",
            "Iteration 6652, loss = 2.70262555\n",
            "Iteration 6653, loss = 2.70180474\n",
            "Iteration 6654, loss = 2.70098444\n",
            "Iteration 6655, loss = 2.70016466\n",
            "Iteration 6656, loss = 2.69934540\n",
            "Iteration 6657, loss = 2.69852665\n",
            "Iteration 6658, loss = 2.69770841\n",
            "Iteration 6659, loss = 2.69689069\n",
            "Iteration 6660, loss = 2.69607348\n",
            "Iteration 6661, loss = 2.69525679\n",
            "Iteration 6662, loss = 2.69444061\n",
            "Iteration 6663, loss = 2.69362494\n",
            "Iteration 6664, loss = 2.69280979\n",
            "Iteration 6665, loss = 2.69199515\n",
            "Iteration 6666, loss = 2.69118102\n",
            "Iteration 6667, loss = 2.69036740\n",
            "Iteration 6668, loss = 2.68955429\n",
            "Iteration 6669, loss = 2.68874170\n",
            "Iteration 6670, loss = 2.68792961\n",
            "Iteration 6671, loss = 2.68711804\n",
            "Iteration 6672, loss = 2.68630697\n",
            "Iteration 6673, loss = 2.68549642\n",
            "Iteration 6674, loss = 2.68468637\n",
            "Iteration 6675, loss = 2.68387684\n",
            "Iteration 6676, loss = 2.68306781\n",
            "Iteration 6677, loss = 2.68225930\n",
            "Iteration 6678, loss = 2.68145129\n",
            "Iteration 6679, loss = 2.68064379\n",
            "Iteration 6680, loss = 2.67983679\n",
            "Iteration 6681, loss = 2.67903031\n",
            "Iteration 6682, loss = 2.67822433\n",
            "Iteration 6683, loss = 2.67741885\n",
            "Iteration 6684, loss = 2.67661389\n",
            "Iteration 6685, loss = 2.67580943\n",
            "Iteration 6686, loss = 2.67500548\n",
            "Iteration 6687, loss = 2.67420203\n",
            "Iteration 6688, loss = 2.67339908\n",
            "Iteration 6689, loss = 2.67259665\n",
            "Iteration 6690, loss = 2.67179471\n",
            "Iteration 6691, loss = 2.67099328\n",
            "Iteration 6692, loss = 2.67019236\n",
            "Iteration 6693, loss = 2.66939194\n",
            "Iteration 6694, loss = 2.66859202\n",
            "Iteration 6695, loss = 2.66779260\n",
            "Iteration 6696, loss = 2.66699369\n",
            "Iteration 6697, loss = 2.66619528\n",
            "Iteration 6698, loss = 2.66539738\n",
            "Iteration 6699, loss = 2.66459997\n",
            "Iteration 6700, loss = 2.66380307\n",
            "Iteration 6701, loss = 2.66300666\n",
            "Iteration 6702, loss = 2.66221076\n",
            "Iteration 6703, loss = 2.66141536\n",
            "Iteration 6704, loss = 2.66062046\n",
            "Iteration 6705, loss = 2.65982606\n",
            "Iteration 6706, loss = 2.65903216\n",
            "Iteration 6707, loss = 2.65823876\n",
            "Iteration 6708, loss = 2.65744585\n",
            "Iteration 6709, loss = 2.65665345\n",
            "Iteration 6710, loss = 2.65586154\n",
            "Iteration 6711, loss = 2.65507014\n",
            "Iteration 6712, loss = 2.65427923\n",
            "Iteration 6713, loss = 2.65348882\n",
            "Iteration 6714, loss = 2.65269890\n",
            "Iteration 6715, loss = 2.65190949\n",
            "Iteration 6716, loss = 2.65112056\n",
            "Iteration 6717, loss = 2.65033214\n",
            "Iteration 6718, loss = 2.64954421\n",
            "Iteration 6719, loss = 2.64875678\n",
            "Iteration 6720, loss = 2.64796984\n",
            "Iteration 6721, loss = 2.64718340\n",
            "Iteration 6722, loss = 2.64639745\n",
            "Iteration 6723, loss = 2.64561200\n",
            "Iteration 6724, loss = 2.64482704\n",
            "Iteration 6725, loss = 2.64404258\n",
            "Iteration 6726, loss = 2.64325861\n",
            "Iteration 6727, loss = 2.64247513\n",
            "Iteration 6728, loss = 2.64169215\n",
            "Iteration 6729, loss = 2.64090966\n",
            "Iteration 6730, loss = 2.64012766\n",
            "Iteration 6731, loss = 2.63934615\n",
            "Iteration 6732, loss = 2.63856514\n",
            "Iteration 6733, loss = 2.63778462\n",
            "Iteration 6734, loss = 2.63700458\n",
            "Iteration 6735, loss = 2.63622504\n",
            "Iteration 6736, loss = 2.63544599\n",
            "Iteration 6737, loss = 2.63466743\n",
            "Iteration 6738, loss = 2.63388936\n",
            "Iteration 6739, loss = 2.63311178\n",
            "Iteration 6740, loss = 2.63233469\n",
            "Iteration 6741, loss = 2.63155809\n",
            "Iteration 6742, loss = 2.63078198\n",
            "Iteration 6743, loss = 2.63000636\n",
            "Iteration 6744, loss = 2.62923122\n",
            "Iteration 6745, loss = 2.62845657\n",
            "Iteration 6746, loss = 2.62768241\n",
            "Iteration 6747, loss = 2.62690874\n",
            "Iteration 6748, loss = 2.62613556\n",
            "Iteration 6749, loss = 2.62536286\n",
            "Iteration 6750, loss = 2.62459065\n",
            "Iteration 6751, loss = 2.62381892\n",
            "Iteration 6752, loss = 2.62304768\n",
            "Iteration 6753, loss = 2.62227693\n",
            "Iteration 6754, loss = 2.62150666\n",
            "Iteration 6755, loss = 2.62073687\n",
            "Iteration 6756, loss = 2.61996758\n",
            "Iteration 6757, loss = 2.61919876\n",
            "Iteration 6758, loss = 2.61843043\n",
            "Iteration 6759, loss = 2.61766258\n",
            "Iteration 6760, loss = 2.61689522\n",
            "Iteration 6761, loss = 2.61612834\n",
            "Iteration 6762, loss = 2.61536194\n",
            "Iteration 6763, loss = 2.61459603\n",
            "Iteration 6764, loss = 2.61383060\n",
            "Iteration 6765, loss = 2.61306565\n",
            "Iteration 6766, loss = 2.61230118\n",
            "Iteration 6767, loss = 2.61153719\n",
            "Iteration 6768, loss = 2.61077369\n",
            "Iteration 6769, loss = 2.61001066\n",
            "Iteration 6770, loss = 2.60924812\n",
            "Iteration 6771, loss = 2.60848606\n",
            "Iteration 6772, loss = 2.60772447\n",
            "Iteration 6773, loss = 2.60696337\n",
            "Iteration 6774, loss = 2.60620275\n",
            "Iteration 6775, loss = 2.60544260\n",
            "Iteration 6776, loss = 2.60468294\n",
            "Iteration 6777, loss = 2.60392375\n",
            "Iteration 6778, loss = 2.60316504\n",
            "Iteration 6779, loss = 2.60240681\n",
            "Iteration 6780, loss = 2.60164906\n",
            "Iteration 6781, loss = 2.60089178\n",
            "Iteration 6782, loss = 2.60013498\n",
            "Iteration 6783, loss = 2.59937866\n",
            "Iteration 6784, loss = 2.59862281\n",
            "Iteration 6785, loss = 2.59786745\n",
            "Iteration 6786, loss = 2.59711255\n",
            "Iteration 6787, loss = 2.59635814\n",
            "Iteration 6788, loss = 2.59560419\n",
            "Iteration 6789, loss = 2.59485073\n",
            "Iteration 6790, loss = 2.59409773\n",
            "Iteration 6791, loss = 2.59334522\n",
            "Iteration 6792, loss = 2.59259317\n",
            "Iteration 6793, loss = 2.59184160\n",
            "Iteration 6794, loss = 2.59109051\n",
            "Iteration 6795, loss = 2.59033989\n",
            "Iteration 6796, loss = 2.58958974\n",
            "Iteration 6797, loss = 2.58884006\n",
            "Iteration 6798, loss = 2.58809086\n",
            "Iteration 6799, loss = 2.58734213\n",
            "Iteration 6800, loss = 2.58659387\n",
            "Iteration 6801, loss = 2.58584608\n",
            "Iteration 6802, loss = 2.58509876\n",
            "Iteration 6803, loss = 2.58435192\n",
            "Iteration 6804, loss = 2.58360554\n",
            "Iteration 6805, loss = 2.58285964\n",
            "Iteration 6806, loss = 2.58211421\n",
            "Iteration 6807, loss = 2.58136924\n",
            "Iteration 6808, loss = 2.58062475\n",
            "Iteration 6809, loss = 2.57988072\n",
            "Iteration 6810, loss = 2.57913717\n",
            "Iteration 6811, loss = 2.57839408\n",
            "Iteration 6812, loss = 2.57765146\n",
            "Iteration 6813, loss = 2.57690931\n",
            "Iteration 6814, loss = 2.57616763\n",
            "Iteration 6815, loss = 2.57542642\n",
            "Iteration 6816, loss = 2.57468567\n",
            "Iteration 6817, loss = 2.57394539\n",
            "Iteration 6818, loss = 2.57320558\n",
            "Iteration 6819, loss = 2.57246623\n",
            "Iteration 6820, loss = 2.57172735\n",
            "Iteration 6821, loss = 2.57098893\n",
            "Iteration 6822, loss = 2.57025099\n",
            "Iteration 6823, loss = 2.56951350\n",
            "Iteration 6824, loss = 2.56877648\n",
            "Iteration 6825, loss = 2.56803993\n",
            "Iteration 6826, loss = 2.56730384\n",
            "Iteration 6827, loss = 2.56656822\n",
            "Iteration 6828, loss = 2.56583306\n",
            "Iteration 6829, loss = 2.56509836\n",
            "Iteration 6830, loss = 2.56436413\n",
            "Iteration 6831, loss = 2.56363036\n",
            "Iteration 6832, loss = 2.56289705\n",
            "Iteration 6833, loss = 2.56216421\n",
            "Iteration 6834, loss = 2.56143183\n",
            "Iteration 6835, loss = 2.56069991\n",
            "Iteration 6836, loss = 2.55996845\n",
            "Iteration 6837, loss = 2.55923745\n",
            "Iteration 6838, loss = 2.55850692\n",
            "Iteration 6839, loss = 2.55777684\n",
            "Iteration 6840, loss = 2.55704723\n",
            "Iteration 6841, loss = 2.55631807\n",
            "Iteration 6842, loss = 2.55558938\n",
            "Iteration 6843, loss = 2.55486115\n",
            "Iteration 6844, loss = 2.55413337\n",
            "Iteration 6845, loss = 2.55340606\n",
            "Iteration 6846, loss = 2.55267920\n",
            "Iteration 6847, loss = 2.55195281\n",
            "Iteration 6848, loss = 2.55122687\n",
            "Iteration 6849, loss = 2.55050139\n",
            "Iteration 6850, loss = 2.54977636\n",
            "Iteration 6851, loss = 2.54905180\n",
            "Iteration 6852, loss = 2.54832769\n",
            "Iteration 6853, loss = 2.54760404\n",
            "Iteration 6854, loss = 2.54688085\n",
            "Iteration 6855, loss = 2.54615811\n",
            "Iteration 6856, loss = 2.54543583\n",
            "Iteration 6857, loss = 2.54471400\n",
            "Iteration 6858, loss = 2.54399263\n",
            "Iteration 6859, loss = 2.54327172\n",
            "Iteration 6860, loss = 2.54255126\n",
            "Iteration 6861, loss = 2.54183126\n",
            "Iteration 6862, loss = 2.54111171\n",
            "Iteration 6863, loss = 2.54039261\n",
            "Iteration 6864, loss = 2.53967397\n",
            "Iteration 6865, loss = 2.53895578\n",
            "Iteration 6866, loss = 2.53823805\n",
            "Iteration 6867, loss = 2.53752077\n",
            "Iteration 6868, loss = 2.53680394\n",
            "Iteration 6869, loss = 2.53608756\n",
            "Iteration 6870, loss = 2.53537164\n",
            "Iteration 6871, loss = 2.53465617\n",
            "Iteration 6872, loss = 2.53394115\n",
            "Iteration 6873, loss = 2.53322658\n",
            "Iteration 6874, loss = 2.53251247\n",
            "Iteration 6875, loss = 2.53179880\n",
            "Iteration 6876, loss = 2.53108559\n",
            "Iteration 6877, loss = 2.53037282\n",
            "Iteration 6878, loss = 2.52966051\n",
            "Iteration 6879, loss = 2.52894865\n",
            "Iteration 6880, loss = 2.52823723\n",
            "Iteration 6881, loss = 2.52752627\n",
            "Iteration 6882, loss = 2.52681575\n",
            "Iteration 6883, loss = 2.52610568\n",
            "Iteration 6884, loss = 2.52539607\n",
            "Iteration 6885, loss = 2.52468690\n",
            "Iteration 6886, loss = 2.52397817\n",
            "Iteration 6887, loss = 2.52326990\n",
            "Iteration 6888, loss = 2.52256207\n",
            "Iteration 6889, loss = 2.52185469\n",
            "Iteration 6890, loss = 2.52114776\n",
            "Iteration 6891, loss = 2.52044127\n",
            "Iteration 6892, loss = 2.51973523\n",
            "Iteration 6893, loss = 2.51902964\n",
            "Iteration 6894, loss = 2.51832449\n",
            "Iteration 6895, loss = 2.51761979\n",
            "Iteration 6896, loss = 2.51691553\n",
            "Iteration 6897, loss = 2.51621172\n",
            "Iteration 6898, loss = 2.51550836\n",
            "Iteration 6899, loss = 2.51480543\n",
            "Iteration 6900, loss = 2.51410296\n",
            "Iteration 6901, loss = 2.51340092\n",
            "Iteration 6902, loss = 2.51269933\n",
            "Iteration 6903, loss = 2.51199818\n",
            "Iteration 6904, loss = 2.51129748\n",
            "Iteration 6905, loss = 2.51059722\n",
            "Iteration 6906, loss = 2.50989740\n",
            "Iteration 6907, loss = 2.50919803\n",
            "Iteration 6908, loss = 2.50849909\n",
            "Iteration 6909, loss = 2.50780060\n",
            "Iteration 6910, loss = 2.50710255\n",
            "Iteration 6911, loss = 2.50640494\n",
            "Iteration 6912, loss = 2.50570777\n",
            "Iteration 6913, loss = 2.50501105\n",
            "Iteration 6914, loss = 2.50431476\n",
            "Iteration 6915, loss = 2.50361891\n",
            "Iteration 6916, loss = 2.50292351\n",
            "Iteration 6917, loss = 2.50222854\n",
            "Iteration 6918, loss = 2.50153401\n",
            "Iteration 6919, loss = 2.50083992\n",
            "Iteration 6920, loss = 2.50014628\n",
            "Iteration 6921, loss = 2.49945307\n",
            "Iteration 6922, loss = 2.49876029\n",
            "Iteration 6923, loss = 2.49806796\n",
            "Iteration 6924, loss = 2.49737606\n",
            "Iteration 6925, loss = 2.49668460\n",
            "Iteration 6926, loss = 2.49599358\n",
            "Iteration 6927, loss = 2.49530300\n",
            "Iteration 6928, loss = 2.49461285\n",
            "Iteration 6929, loss = 2.49392314\n",
            "Iteration 6930, loss = 2.49323387\n",
            "Iteration 6931, loss = 2.49254503\n",
            "Iteration 6932, loss = 2.49185663\n",
            "Iteration 6933, loss = 2.49116866\n",
            "Iteration 6934, loss = 2.49048113\n",
            "Iteration 6935, loss = 2.48979403\n",
            "Iteration 6936, loss = 2.48910737\n",
            "Iteration 6937, loss = 2.48842114\n",
            "Iteration 6938, loss = 2.48773535\n",
            "Iteration 6939, loss = 2.48704999\n",
            "Iteration 6940, loss = 2.48636506\n",
            "Iteration 6941, loss = 2.48568057\n",
            "Iteration 6942, loss = 2.48499651\n",
            "Iteration 6943, loss = 2.48431288\n",
            "Iteration 6944, loss = 2.48362969\n",
            "Iteration 6945, loss = 2.48294693\n",
            "Iteration 6946, loss = 2.48226460\n",
            "Iteration 6947, loss = 2.48158270\n",
            "Iteration 6948, loss = 2.48090124\n",
            "Iteration 6949, loss = 2.48022020\n",
            "Iteration 6950, loss = 2.47953960\n",
            "Iteration 6951, loss = 2.47885942\n",
            "Iteration 6952, loss = 2.47817968\n",
            "Iteration 6953, loss = 2.47750037\n",
            "Iteration 6954, loss = 2.47682149\n",
            "Iteration 6955, loss = 2.47614304\n",
            "Iteration 6956, loss = 2.47546501\n",
            "Iteration 6957, loss = 2.47478742\n",
            "Iteration 6958, loss = 2.47411026\n",
            "Iteration 6959, loss = 2.47343352\n",
            "Iteration 6960, loss = 2.47275721\n",
            "Iteration 6961, loss = 2.47208133\n",
            "Iteration 6962, loss = 2.47140588\n",
            "Iteration 6963, loss = 2.47073086\n",
            "Iteration 6964, loss = 2.47005626\n",
            "Iteration 6965, loss = 2.46938209\n",
            "Iteration 6966, loss = 2.46870835\n",
            "Iteration 6967, loss = 2.46803504\n",
            "Iteration 6968, loss = 2.46736215\n",
            "Iteration 6969, loss = 2.46668969\n",
            "Iteration 6970, loss = 2.46601765\n",
            "Iteration 6971, loss = 2.46534604\n",
            "Iteration 6972, loss = 2.46467485\n",
            "Iteration 6973, loss = 2.46400409\n",
            "Iteration 6974, loss = 2.46333376\n",
            "Iteration 6975, loss = 2.46266385\n",
            "Iteration 6976, loss = 2.46199436\n",
            "Iteration 6977, loss = 2.46132530\n",
            "Iteration 6978, loss = 2.46065666\n",
            "Iteration 6979, loss = 2.45998844\n",
            "Iteration 6980, loss = 2.45932065\n",
            "Iteration 6981, loss = 2.45865328\n",
            "Iteration 6982, loss = 2.45798634\n",
            "Iteration 6983, loss = 2.45731981\n",
            "Iteration 6984, loss = 2.45665371\n",
            "Iteration 6985, loss = 2.45598804\n",
            "Iteration 6986, loss = 2.45532278\n",
            "Iteration 6987, loss = 2.45465794\n",
            "Iteration 6988, loss = 2.45399353\n",
            "Iteration 6989, loss = 2.45332954\n",
            "Iteration 6990, loss = 2.45266596\n",
            "Iteration 6991, loss = 2.45200281\n",
            "Iteration 6992, loss = 2.45134008\n",
            "Iteration 6993, loss = 2.45067777\n",
            "Iteration 6994, loss = 2.45001588\n",
            "Iteration 6995, loss = 2.44935441\n",
            "Iteration 6996, loss = 2.44869335\n",
            "Iteration 6997, loss = 2.44803272\n",
            "Iteration 6998, loss = 2.44737251\n",
            "Iteration 6999, loss = 2.44671271\n",
            "Iteration 7000, loss = 2.44605333\n",
            "Iteration 7001, loss = 2.44539437\n",
            "Iteration 7002, loss = 2.44473583\n",
            "Iteration 7003, loss = 2.44407771\n",
            "Iteration 7004, loss = 2.44342000\n",
            "Iteration 7005, loss = 2.44276271\n",
            "Iteration 7006, loss = 2.44210584\n",
            "Iteration 7007, loss = 2.44144938\n",
            "Iteration 7008, loss = 2.44079334\n",
            "Iteration 7009, loss = 2.44013771\n",
            "Iteration 7010, loss = 2.43948251\n",
            "Iteration 7011, loss = 2.43882771\n",
            "Iteration 7012, loss = 2.43817333\n",
            "Iteration 7013, loss = 2.43751937\n",
            "Iteration 7014, loss = 2.43686582\n",
            "Iteration 7015, loss = 2.43621269\n",
            "Iteration 7016, loss = 2.43555997\n",
            "Iteration 7017, loss = 2.43490767\n",
            "Iteration 7018, loss = 2.43425577\n",
            "Iteration 7019, loss = 2.43360430\n",
            "Iteration 7020, loss = 2.43295323\n",
            "Iteration 7021, loss = 2.43230258\n",
            "Iteration 7022, loss = 2.43165234\n",
            "Iteration 7023, loss = 2.43100252\n",
            "Iteration 7024, loss = 2.43035310\n",
            "Iteration 7025, loss = 2.42970410\n",
            "Iteration 7026, loss = 2.42905551\n",
            "Iteration 7027, loss = 2.42840733\n",
            "Iteration 7028, loss = 2.42775956\n",
            "Iteration 7029, loss = 2.42711221\n",
            "Iteration 7030, loss = 2.42646526\n",
            "Iteration 7031, loss = 2.42581873\n",
            "Iteration 7032, loss = 2.42517260\n",
            "Iteration 7033, loss = 2.42452689\n",
            "Iteration 7034, loss = 2.42388159\n",
            "Iteration 7035, loss = 2.42323669\n",
            "Iteration 7036, loss = 2.42259221\n",
            "Iteration 7037, loss = 2.42194813\n",
            "Iteration 7038, loss = 2.42130446\n",
            "Iteration 7039, loss = 2.42066120\n",
            "Iteration 7040, loss = 2.42001835\n",
            "Iteration 7041, loss = 2.41937591\n",
            "Iteration 7042, loss = 2.41873388\n",
            "Iteration 7043, loss = 2.41809225\n",
            "Iteration 7044, loss = 2.41745103\n",
            "Iteration 7045, loss = 2.41681022\n",
            "Iteration 7046, loss = 2.41616981\n",
            "Iteration 7047, loss = 2.41552981\n",
            "Iteration 7048, loss = 2.41489022\n",
            "Iteration 7049, loss = 2.41425104\n",
            "Iteration 7050, loss = 2.41361226\n",
            "Iteration 7051, loss = 2.41297388\n",
            "Iteration 7052, loss = 2.41233591\n",
            "Iteration 7053, loss = 2.41169835\n",
            "Iteration 7054, loss = 2.41106119\n",
            "Iteration 7055, loss = 2.41042443\n",
            "Iteration 7056, loss = 2.40978808\n",
            "Iteration 7057, loss = 2.40915214\n",
            "Iteration 7058, loss = 2.40851660\n",
            "Iteration 7059, loss = 2.40788146\n",
            "Iteration 7060, loss = 2.40724673\n",
            "Iteration 7061, loss = 2.40661239\n",
            "Iteration 7062, loss = 2.40597847\n",
            "Iteration 7063, loss = 2.40534494\n",
            "Iteration 7064, loss = 2.40471182\n",
            "Iteration 7065, loss = 2.40407910\n",
            "Iteration 7066, loss = 2.40344678\n",
            "Iteration 7067, loss = 2.40281486\n",
            "Iteration 7068, loss = 2.40218335\n",
            "Iteration 7069, loss = 2.40155223\n",
            "Iteration 7070, loss = 2.40092152\n",
            "Iteration 7071, loss = 2.40029121\n",
            "Iteration 7072, loss = 2.39966130\n",
            "Iteration 7073, loss = 2.39903179\n",
            "Iteration 7074, loss = 2.39840268\n",
            "Iteration 7075, loss = 2.39777397\n",
            "Iteration 7076, loss = 2.39714566\n",
            "Iteration 7077, loss = 2.39651774\n",
            "Iteration 7078, loss = 2.39589023\n",
            "Iteration 7079, loss = 2.39526312\n",
            "Iteration 7080, loss = 2.39463640\n",
            "Iteration 7081, loss = 2.39401009\n",
            "Iteration 7082, loss = 2.39338417\n",
            "Iteration 7083, loss = 2.39275865\n",
            "Iteration 7084, loss = 2.39213352\n",
            "Iteration 7085, loss = 2.39150880\n",
            "Iteration 7086, loss = 2.39088447\n",
            "Iteration 7087, loss = 2.39026054\n",
            "Iteration 7088, loss = 2.38963701\n",
            "Iteration 7089, loss = 2.38901387\n",
            "Iteration 7090, loss = 2.38839113\n",
            "Iteration 7091, loss = 2.38776878\n",
            "Iteration 7092, loss = 2.38714683\n",
            "Iteration 7093, loss = 2.38652528\n",
            "Iteration 7094, loss = 2.38590412\n",
            "Iteration 7095, loss = 2.38528336\n",
            "Iteration 7096, loss = 2.38466299\n",
            "Iteration 7097, loss = 2.38404302\n",
            "Iteration 7098, loss = 2.38342344\n",
            "Iteration 7099, loss = 2.38280425\n",
            "Iteration 7100, loss = 2.38218546\n",
            "Iteration 7101, loss = 2.38156707\n",
            "Iteration 7102, loss = 2.38094906\n",
            "Iteration 7103, loss = 2.38033145\n",
            "Iteration 7104, loss = 2.37971423\n",
            "Iteration 7105, loss = 2.37909741\n",
            "Iteration 7106, loss = 2.37848098\n",
            "Iteration 7107, loss = 2.37786494\n",
            "Iteration 7108, loss = 2.37724929\n",
            "Iteration 7109, loss = 2.37663403\n",
            "Iteration 7110, loss = 2.37601917\n",
            "Iteration 7111, loss = 2.37540469\n",
            "Iteration 7112, loss = 2.37479061\n",
            "Iteration 7113, loss = 2.37417692\n",
            "Iteration 7114, loss = 2.37356362\n",
            "Iteration 7115, loss = 2.37295071\n",
            "Iteration 7116, loss = 2.37233819\n",
            "Iteration 7117, loss = 2.37172606\n",
            "Iteration 7118, loss = 2.37111432\n",
            "Iteration 7119, loss = 2.37050297\n",
            "Iteration 7120, loss = 2.36989201\n",
            "Iteration 7121, loss = 2.36928144\n",
            "Iteration 7122, loss = 2.36867125\n",
            "Iteration 7123, loss = 2.36806146\n",
            "Iteration 7124, loss = 2.36745205\n",
            "Iteration 7125, loss = 2.36684303\n",
            "Iteration 7126, loss = 2.36623440\n",
            "Iteration 7127, loss = 2.36562616\n",
            "Iteration 7128, loss = 2.36501830\n",
            "Iteration 7129, loss = 2.36441083\n",
            "Iteration 7130, loss = 2.36380375\n",
            "Iteration 7131, loss = 2.36319706\n",
            "Iteration 7132, loss = 2.36259075\n",
            "Iteration 7133, loss = 2.36198483\n",
            "Iteration 7134, loss = 2.36137929\n",
            "Iteration 7135, loss = 2.36077414\n",
            "Iteration 7136, loss = 2.36016937\n",
            "Iteration 7137, loss = 2.35956500\n",
            "Iteration 7138, loss = 2.35896100\n",
            "Iteration 7139, loss = 2.35835739\n",
            "Iteration 7140, loss = 2.35775417\n",
            "Iteration 7141, loss = 2.35715133\n",
            "Iteration 7142, loss = 2.35654887\n",
            "Iteration 7143, loss = 2.35594680\n",
            "Iteration 7144, loss = 2.35534511\n",
            "Iteration 7145, loss = 2.35474380\n",
            "Iteration 7146, loss = 2.35414288\n",
            "Iteration 7147, loss = 2.35354234\n",
            "Iteration 7148, loss = 2.35294219\n",
            "Iteration 7149, loss = 2.35234241\n",
            "Iteration 7150, loss = 2.35174302\n",
            "Iteration 7151, loss = 2.35114401\n",
            "Iteration 7152, loss = 2.35054538\n",
            "Iteration 7153, loss = 2.34994714\n",
            "Iteration 7154, loss = 2.34934927\n",
            "Iteration 7155, loss = 2.34875179\n",
            "Iteration 7156, loss = 2.34815469\n",
            "Iteration 7157, loss = 2.34755797\n",
            "Iteration 7158, loss = 2.34696163\n",
            "Iteration 7159, loss = 2.34636567\n",
            "Iteration 7160, loss = 2.34577009\n",
            "Iteration 7161, loss = 2.34517488\n",
            "Iteration 7162, loss = 2.34458006\n",
            "Iteration 7163, loss = 2.34398562\n",
            "Iteration 7164, loss = 2.34339156\n",
            "Iteration 7165, loss = 2.34279788\n",
            "Iteration 7166, loss = 2.34220457\n",
            "Iteration 7167, loss = 2.34161164\n",
            "Iteration 7168, loss = 2.34101910\n",
            "Iteration 7169, loss = 2.34042693\n",
            "Iteration 7170, loss = 2.33983513\n",
            "Iteration 7171, loss = 2.33924372\n",
            "Iteration 7172, loss = 2.33865268\n",
            "Iteration 7173, loss = 2.33806202\n",
            "Iteration 7174, loss = 2.33747174\n",
            "Iteration 7175, loss = 2.33688183\n",
            "Iteration 7176, loss = 2.33629230\n",
            "Iteration 7177, loss = 2.33570315\n",
            "Iteration 7178, loss = 2.33511437\n",
            "Iteration 7179, loss = 2.33452597\n",
            "Iteration 7180, loss = 2.33393794\n",
            "Iteration 7181, loss = 2.33335029\n",
            "Iteration 7182, loss = 2.33276301\n",
            "Iteration 7183, loss = 2.33217611\n",
            "Iteration 7184, loss = 2.33158958\n",
            "Iteration 7185, loss = 2.33100343\n",
            "Iteration 7186, loss = 2.33041765\n",
            "Iteration 7187, loss = 2.32983225\n",
            "Iteration 7188, loss = 2.32924722\n",
            "Iteration 7189, loss = 2.32866256\n",
            "Iteration 7190, loss = 2.32807828\n",
            "Iteration 7191, loss = 2.32749437\n",
            "Iteration 7192, loss = 2.32691083\n",
            "Iteration 7193, loss = 2.32632766\n",
            "Iteration 7194, loss = 2.32574487\n",
            "Iteration 7195, loss = 2.32516245\n",
            "Iteration 7196, loss = 2.32458040\n",
            "Iteration 7197, loss = 2.32399872\n",
            "Iteration 7198, loss = 2.32341742\n",
            "Iteration 7199, loss = 2.32283649\n",
            "Iteration 7200, loss = 2.32225592\n",
            "Iteration 7201, loss = 2.32167573\n",
            "Iteration 7202, loss = 2.32109591\n",
            "Iteration 7203, loss = 2.32051646\n",
            "Iteration 7204, loss = 2.31993738\n",
            "Iteration 7205, loss = 2.31935867\n",
            "Iteration 7206, loss = 2.31878033\n",
            "Iteration 7207, loss = 2.31820235\n",
            "Iteration 7208, loss = 2.31762475\n",
            "Iteration 7209, loss = 2.31704752\n",
            "Iteration 7210, loss = 2.31647066\n",
            "Iteration 7211, loss = 2.31589416\n",
            "Iteration 7212, loss = 2.31531803\n",
            "Iteration 7213, loss = 2.31474227\n",
            "Iteration 7214, loss = 2.31416688\n",
            "Iteration 7215, loss = 2.31359186\n",
            "Iteration 7216, loss = 2.31301720\n",
            "Iteration 7217, loss = 2.31244292\n",
            "Iteration 7218, loss = 2.31186900\n",
            "Iteration 7219, loss = 2.31129544\n",
            "Iteration 7220, loss = 2.31072225\n",
            "Iteration 7221, loss = 2.31014943\n",
            "Iteration 7222, loss = 2.30957698\n",
            "Iteration 7223, loss = 2.30900489\n",
            "Iteration 7224, loss = 2.30843317\n",
            "Iteration 7225, loss = 2.30786181\n",
            "Iteration 7226, loss = 2.30729082\n",
            "Iteration 7227, loss = 2.30672019\n",
            "Iteration 7228, loss = 2.30614993\n",
            "Iteration 7229, loss = 2.30558003\n",
            "Iteration 7230, loss = 2.30501050\n",
            "Iteration 7231, loss = 2.30444133\n",
            "Iteration 7232, loss = 2.30387253\n",
            "Iteration 7233, loss = 2.30330409\n",
            "Iteration 7234, loss = 2.30273601\n",
            "Iteration 7235, loss = 2.30216830\n",
            "Iteration 7236, loss = 2.30160095\n",
            "Iteration 7237, loss = 2.30103396\n",
            "Iteration 7238, loss = 2.30046733\n",
            "Iteration 7239, loss = 2.29990107\n",
            "Iteration 7240, loss = 2.29933517\n",
            "Iteration 7241, loss = 2.29876964\n",
            "Iteration 7242, loss = 2.29820446\n",
            "Iteration 7243, loss = 2.29763965\n",
            "Iteration 7244, loss = 2.29707519\n",
            "Iteration 7245, loss = 2.29651110\n",
            "Iteration 7246, loss = 2.29594737\n",
            "Iteration 7247, loss = 2.29538400\n",
            "Iteration 7248, loss = 2.29482100\n",
            "Iteration 7249, loss = 2.29425835\n",
            "Iteration 7250, loss = 2.29369606\n",
            "Iteration 7251, loss = 2.29313413\n",
            "Iteration 7252, loss = 2.29257256\n",
            "Iteration 7253, loss = 2.29201135\n",
            "Iteration 7254, loss = 2.29145050\n",
            "Iteration 7255, loss = 2.29089001\n",
            "Iteration 7256, loss = 2.29032988\n",
            "Iteration 7257, loss = 2.28977011\n",
            "Iteration 7258, loss = 2.28921070\n",
            "Iteration 7259, loss = 2.28865164\n",
            "Iteration 7260, loss = 2.28809294\n",
            "Iteration 7261, loss = 2.28753460\n",
            "Iteration 7262, loss = 2.28697662\n",
            "Iteration 7263, loss = 2.28641899\n",
            "Iteration 7264, loss = 2.28586173\n",
            "Iteration 7265, loss = 2.28530481\n",
            "Iteration 7266, loss = 2.28474826\n",
            "Iteration 7267, loss = 2.28419206\n",
            "Iteration 7268, loss = 2.28363622\n",
            "Iteration 7269, loss = 2.28308074\n",
            "Iteration 7270, loss = 2.28252561\n",
            "Iteration 7271, loss = 2.28197083\n",
            "Iteration 7272, loss = 2.28141641\n",
            "Iteration 7273, loss = 2.28086235\n",
            "Iteration 7274, loss = 2.28030864\n",
            "Iteration 7275, loss = 2.27975529\n",
            "Iteration 7276, loss = 2.27920229\n",
            "Iteration 7277, loss = 2.27864965\n",
            "Iteration 7278, loss = 2.27809736\n",
            "Iteration 7279, loss = 2.27754542\n",
            "Iteration 7280, loss = 2.27699384\n",
            "Iteration 7281, loss = 2.27644261\n",
            "Iteration 7282, loss = 2.27589174\n",
            "Iteration 7283, loss = 2.27534122\n",
            "Iteration 7284, loss = 2.27479105\n",
            "Iteration 7285, loss = 2.27424123\n",
            "Iteration 7286, loss = 2.27369177\n",
            "Iteration 7287, loss = 2.27314266\n",
            "Iteration 7288, loss = 2.27259390\n",
            "Iteration 7289, loss = 2.27204549\n",
            "Iteration 7290, loss = 2.27149743\n",
            "Iteration 7291, loss = 2.27094973\n",
            "Iteration 7292, loss = 2.27040238\n",
            "Iteration 7293, loss = 2.26985538\n",
            "Iteration 7294, loss = 2.26930873\n",
            "Iteration 7295, loss = 2.26876242\n",
            "Iteration 7296, loss = 2.26821648\n",
            "Iteration 7297, loss = 2.26767088\n",
            "Iteration 7298, loss = 2.26712563\n",
            "Iteration 7299, loss = 2.26658073\n",
            "Iteration 7300, loss = 2.26603618\n",
            "Iteration 7301, loss = 2.26549198\n",
            "Iteration 7302, loss = 2.26494812\n",
            "Iteration 7303, loss = 2.26440462\n",
            "Iteration 7304, loss = 2.26386147\n",
            "Iteration 7305, loss = 2.26331866\n",
            "Iteration 7306, loss = 2.26277621\n",
            "Iteration 7307, loss = 2.26223410\n",
            "Iteration 7308, loss = 2.26169234\n",
            "Iteration 7309, loss = 2.26115092\n",
            "Iteration 7310, loss = 2.26060986\n",
            "Iteration 7311, loss = 2.26006914\n",
            "Iteration 7312, loss = 2.25952877\n",
            "Iteration 7313, loss = 2.25898874\n",
            "Iteration 7314, loss = 2.25844907\n",
            "Iteration 7315, loss = 2.25790974\n",
            "Iteration 7316, loss = 2.25737075\n",
            "Iteration 7317, loss = 2.25683211\n",
            "Iteration 7318, loss = 2.25629382\n",
            "Iteration 7319, loss = 2.25575587\n",
            "Iteration 7320, loss = 2.25521827\n",
            "Iteration 7321, loss = 2.25468102\n",
            "Iteration 7322, loss = 2.25414410\n",
            "Iteration 7323, loss = 2.25360754\n",
            "Iteration 7324, loss = 2.25307132\n",
            "Iteration 7325, loss = 2.25253544\n",
            "Iteration 7326, loss = 2.25199991\n",
            "Iteration 7327, loss = 2.25146472\n",
            "Iteration 7328, loss = 2.25092987\n",
            "Iteration 7329, loss = 2.25039537\n",
            "Iteration 7330, loss = 2.24986121\n",
            "Iteration 7331, loss = 2.24932740\n",
            "Iteration 7332, loss = 2.24879392\n",
            "Iteration 7333, loss = 2.24826079\n",
            "Iteration 7334, loss = 2.24772801\n",
            "Iteration 7335, loss = 2.24719556\n",
            "Iteration 7336, loss = 2.24666346\n",
            "Iteration 7337, loss = 2.24613170\n",
            "Iteration 7338, loss = 2.24560028\n",
            "Iteration 7339, loss = 2.24506921\n",
            "Iteration 7340, loss = 2.24453847\n",
            "Iteration 7341, loss = 2.24400808\n",
            "Iteration 7342, loss = 2.24347802\n",
            "Iteration 7343, loss = 2.24294831\n",
            "Iteration 7344, loss = 2.24241894\n",
            "Iteration 7345, loss = 2.24188991\n",
            "Iteration 7346, loss = 2.24136122\n",
            "Iteration 7347, loss = 2.24083287\n",
            "Iteration 7348, loss = 2.24030485\n",
            "Iteration 7349, loss = 2.23977718\n",
            "Iteration 7350, loss = 2.23924985\n",
            "Iteration 7351, loss = 2.23872286\n",
            "Iteration 7352, loss = 2.23819620\n",
            "Iteration 7353, loss = 2.23766988\n",
            "Iteration 7354, loss = 2.23714391\n",
            "Iteration 7355, loss = 2.23661827\n",
            "Iteration 7356, loss = 2.23609297\n",
            "Iteration 7357, loss = 2.23556800\n",
            "Iteration 7358, loss = 2.23504338\n",
            "Iteration 7359, loss = 2.23451909\n",
            "Iteration 7360, loss = 2.23399514\n",
            "Iteration 7361, loss = 2.23347153\n",
            "Iteration 7362, loss = 2.23294825\n",
            "Iteration 7363, loss = 2.23242531\n",
            "Iteration 7364, loss = 2.23190271\n",
            "Iteration 7365, loss = 2.23138044\n",
            "Iteration 7366, loss = 2.23085851\n",
            "Iteration 7367, loss = 2.23033692\n",
            "Iteration 7368, loss = 2.22981566\n",
            "Iteration 7369, loss = 2.22929473\n",
            "Iteration 7370, loss = 2.22877415\n",
            "Iteration 7371, loss = 2.22825389\n",
            "Iteration 7372, loss = 2.22773398\n",
            "Iteration 7373, loss = 2.22721439\n",
            "Iteration 7374, loss = 2.22669514\n",
            "Iteration 7375, loss = 2.22617623\n",
            "Iteration 7376, loss = 2.22565765\n",
            "Iteration 7377, loss = 2.22513940\n",
            "Iteration 7378, loss = 2.22462149\n",
            "Iteration 7379, loss = 2.22410391\n",
            "Iteration 7380, loss = 2.22358666\n",
            "Iteration 7381, loss = 2.22306975\n",
            "Iteration 7382, loss = 2.22255317\n",
            "Iteration 7383, loss = 2.22203693\n",
            "Iteration 7384, loss = 2.22152101\n",
            "Iteration 7385, loss = 2.22100543\n",
            "Iteration 7386, loss = 2.22049018\n",
            "Iteration 7387, loss = 2.21997526\n",
            "Iteration 7388, loss = 2.21946068\n",
            "Iteration 7389, loss = 2.21894642\n",
            "Iteration 7390, loss = 2.21843250\n",
            "Iteration 7391, loss = 2.21791891\n",
            "Iteration 7392, loss = 2.21740564\n",
            "Iteration 7393, loss = 2.21689271\n",
            "Iteration 7394, loss = 2.21638011\n",
            "Iteration 7395, loss = 2.21586784\n",
            "Iteration 7396, loss = 2.21535591\n",
            "Iteration 7397, loss = 2.21484430\n",
            "Iteration 7398, loss = 2.21433302\n",
            "Iteration 7399, loss = 2.21382207\n",
            "Iteration 7400, loss = 2.21331145\n",
            "Iteration 7401, loss = 2.21280116\n",
            "Iteration 7402, loss = 2.21229119\n",
            "Iteration 7403, loss = 2.21178156\n",
            "Iteration 7404, loss = 2.21127225\n",
            "Iteration 7405, loss = 2.21076328\n",
            "Iteration 7406, loss = 2.21025463\n",
            "Iteration 7407, loss = 2.20974631\n",
            "Iteration 7408, loss = 2.20923832\n",
            "Iteration 7409, loss = 2.20873065\n",
            "Iteration 7410, loss = 2.20822332\n",
            "Iteration 7411, loss = 2.20771631\n",
            "Iteration 7412, loss = 2.20720962\n",
            "Iteration 7413, loss = 2.20670327\n",
            "Iteration 7414, loss = 2.20619724\n",
            "Iteration 7415, loss = 2.20569153\n",
            "Iteration 7416, loss = 2.20518616\n",
            "Iteration 7417, loss = 2.20468111\n",
            "Iteration 7418, loss = 2.20417638\n",
            "Iteration 7419, loss = 2.20367198\n",
            "Iteration 7420, loss = 2.20316791\n",
            "Iteration 7421, loss = 2.20266416\n",
            "Iteration 7422, loss = 2.20216074\n",
            "Iteration 7423, loss = 2.20165764\n",
            "Iteration 7424, loss = 2.20115487\n",
            "Iteration 7425, loss = 2.20065242\n",
            "Iteration 7426, loss = 2.20015029\n",
            "Iteration 7427, loss = 2.19964849\n",
            "Iteration 7428, loss = 2.19914702\n",
            "Iteration 7429, loss = 2.19864586\n",
            "Iteration 7430, loss = 2.19814503\n",
            "Iteration 7431, loss = 2.19764453\n",
            "Iteration 7432, loss = 2.19714435\n",
            "Iteration 7433, loss = 2.19664449\n",
            "Iteration 7434, loss = 2.19614495\n",
            "Iteration 7435, loss = 2.19564573\n",
            "Iteration 7436, loss = 2.19514684\n",
            "Iteration 7437, loss = 2.19464827\n",
            "Iteration 7438, loss = 2.19415003\n",
            "Iteration 7439, loss = 2.19365210\n",
            "Iteration 7440, loss = 2.19315450\n",
            "Iteration 7441, loss = 2.19265721\n",
            "Iteration 7442, loss = 2.19216025\n",
            "Iteration 7443, loss = 2.19166361\n",
            "Iteration 7444, loss = 2.19116729\n",
            "Iteration 7445, loss = 2.19067129\n",
            "Iteration 7446, loss = 2.19017562\n",
            "Iteration 7447, loss = 2.18968026\n",
            "Iteration 7448, loss = 2.18918522\n",
            "Iteration 7449, loss = 2.18869050\n",
            "Iteration 7450, loss = 2.18819610\n",
            "Iteration 7451, loss = 2.18770202\n",
            "Iteration 7452, loss = 2.18720827\n",
            "Iteration 7453, loss = 2.18671483\n",
            "Iteration 7454, loss = 2.18622170\n",
            "Iteration 7455, loss = 2.18572890\n",
            "Iteration 7456, loss = 2.18523642\n",
            "Iteration 7457, loss = 2.18474425\n",
            "Iteration 7458, loss = 2.18425241\n",
            "Iteration 7459, loss = 2.18376088\n",
            "Iteration 7460, loss = 2.18326967\n",
            "Iteration 7461, loss = 2.18277877\n",
            "Iteration 7462, loss = 2.18228820\n",
            "Iteration 7463, loss = 2.18179794\n",
            "Iteration 7464, loss = 2.18130800\n",
            "Iteration 7465, loss = 2.18081837\n",
            "Iteration 7466, loss = 2.18032906\n",
            "Iteration 7467, loss = 2.17984007\n",
            "Iteration 7468, loss = 2.17935140\n",
            "Iteration 7469, loss = 2.17886304\n",
            "Iteration 7470, loss = 2.17837499\n",
            "Iteration 7471, loss = 2.17788727\n",
            "Iteration 7472, loss = 2.17739985\n",
            "Iteration 7473, loss = 2.17691276\n",
            "Iteration 7474, loss = 2.17642598\n",
            "Iteration 7475, loss = 2.17593951\n",
            "Iteration 7476, loss = 2.17545336\n",
            "Iteration 7477, loss = 2.17496752\n",
            "Iteration 7478, loss = 2.17448200\n",
            "Iteration 7479, loss = 2.17399679\n",
            "Iteration 7480, loss = 2.17351190\n",
            "Iteration 7481, loss = 2.17302732\n",
            "Iteration 7482, loss = 2.17254305\n",
            "Iteration 7483, loss = 2.17205910\n",
            "Iteration 7484, loss = 2.17157546\n",
            "Iteration 7485, loss = 2.17109213\n",
            "Iteration 7486, loss = 2.17060912\n",
            "Iteration 7487, loss = 2.17012642\n",
            "Iteration 7488, loss = 2.16964403\n",
            "Iteration 7489, loss = 2.16916196\n",
            "Iteration 7490, loss = 2.16868019\n",
            "Iteration 7491, loss = 2.16819874\n",
            "Iteration 7492, loss = 2.16771760\n",
            "Iteration 7493, loss = 2.16723677\n",
            "Iteration 7494, loss = 2.16675626\n",
            "Iteration 7495, loss = 2.16627605\n",
            "Iteration 7496, loss = 2.16579616\n",
            "Iteration 7497, loss = 2.16531658\n",
            "Iteration 7498, loss = 2.16483730\n",
            "Iteration 7499, loss = 2.16435834\n",
            "Iteration 7500, loss = 2.16387969\n",
            "Iteration 7501, loss = 2.16340135\n",
            "Iteration 7502, loss = 2.16292332\n",
            "Iteration 7503, loss = 2.16244560\n",
            "Iteration 7504, loss = 2.16196818\n",
            "Iteration 7505, loss = 2.16149108\n",
            "Iteration 7506, loss = 2.16101429\n",
            "Iteration 7507, loss = 2.16053780\n",
            "Iteration 7508, loss = 2.16006163\n",
            "Iteration 7509, loss = 2.15958576\n",
            "Iteration 7510, loss = 2.15911021\n",
            "Iteration 7511, loss = 2.15863496\n",
            "Iteration 7512, loss = 2.15816001\n",
            "Iteration 7513, loss = 2.15768538\n",
            "Iteration 7514, loss = 2.15721106\n",
            "Iteration 7515, loss = 2.15673704\n",
            "Iteration 7516, loss = 2.15626333\n",
            "Iteration 7517, loss = 2.15578992\n",
            "Iteration 7518, loss = 2.15531683\n",
            "Iteration 7519, loss = 2.15484404\n",
            "Iteration 7520, loss = 2.15437156\n",
            "Iteration 7521, loss = 2.15389938\n",
            "Iteration 7522, loss = 2.15342751\n",
            "Iteration 7523, loss = 2.15295595\n",
            "Iteration 7524, loss = 2.15248469\n",
            "Iteration 7525, loss = 2.15201374\n",
            "Iteration 7526, loss = 2.15154309\n",
            "Iteration 7527, loss = 2.15107275\n",
            "Iteration 7528, loss = 2.15060271\n",
            "Iteration 7529, loss = 2.15013298\n",
            "Iteration 7530, loss = 2.14966356\n",
            "Iteration 7531, loss = 2.14919443\n",
            "Iteration 7532, loss = 2.14872562\n",
            "Iteration 7533, loss = 2.14825711\n",
            "Iteration 7534, loss = 2.14778890\n",
            "Iteration 7535, loss = 2.14732099\n",
            "Iteration 7536, loss = 2.14685339\n",
            "Iteration 7537, loss = 2.14638610\n",
            "Iteration 7538, loss = 2.14591910\n",
            "Iteration 7539, loss = 2.14545241\n",
            "Iteration 7540, loss = 2.14498603\n",
            "Iteration 7541, loss = 2.14451994\n",
            "Iteration 7542, loss = 2.14405416\n",
            "Iteration 7543, loss = 2.14358868\n",
            "Iteration 7544, loss = 2.14312351\n",
            "Iteration 7545, loss = 2.14265863\n",
            "Iteration 7546, loss = 2.14219406\n",
            "Iteration 7547, loss = 2.14172979\n",
            "Iteration 7548, loss = 2.14126582\n",
            "Iteration 7549, loss = 2.14080215\n",
            "Iteration 7550, loss = 2.14033879\n",
            "Iteration 7551, loss = 2.13987572\n",
            "Iteration 7552, loss = 2.13941296\n",
            "Iteration 7553, loss = 2.13895049\n",
            "Iteration 7554, loss = 2.13848833\n",
            "Iteration 7555, loss = 2.13802647\n",
            "Iteration 7556, loss = 2.13756490\n",
            "Iteration 7557, loss = 2.13710364\n",
            "Iteration 7558, loss = 2.13664268\n",
            "Iteration 7559, loss = 2.13618202\n",
            "Iteration 7560, loss = 2.13572165\n",
            "Iteration 7561, loss = 2.13526159\n",
            "Iteration 7562, loss = 2.13480182\n",
            "Iteration 7563, loss = 2.13434236\n",
            "Iteration 7564, loss = 2.13388319\n",
            "Iteration 7565, loss = 2.13342432\n",
            "Iteration 7566, loss = 2.13296575\n",
            "Iteration 7567, loss = 2.13250748\n",
            "Iteration 7568, loss = 2.13204950\n",
            "Iteration 7569, loss = 2.13159183\n",
            "Iteration 7570, loss = 2.13113445\n",
            "Iteration 7571, loss = 2.13067737\n",
            "Iteration 7572, loss = 2.13022059\n",
            "Iteration 7573, loss = 2.12976410\n",
            "Iteration 7574, loss = 2.12930791\n",
            "Iteration 7575, loss = 2.12885202\n",
            "Iteration 7576, loss = 2.12839642\n",
            "Iteration 7577, loss = 2.12794112\n",
            "Iteration 7578, loss = 2.12748612\n",
            "Iteration 7579, loss = 2.12703141\n",
            "Iteration 7580, loss = 2.12657700\n",
            "Iteration 7581, loss = 2.12612288\n",
            "Iteration 7582, loss = 2.12566906\n",
            "Iteration 7583, loss = 2.12521554\n",
            "Iteration 7584, loss = 2.12476231\n",
            "Iteration 7585, loss = 2.12430938\n",
            "Iteration 7586, loss = 2.12385674\n",
            "Iteration 7587, loss = 2.12340439\n",
            "Iteration 7588, loss = 2.12295234\n",
            "Iteration 7589, loss = 2.12250059\n",
            "Iteration 7590, loss = 2.12204913\n",
            "Iteration 7591, loss = 2.12159796\n",
            "Iteration 7592, loss = 2.12114708\n",
            "Iteration 7593, loss = 2.12069650\n",
            "Iteration 7594, loss = 2.12024622\n",
            "Iteration 7595, loss = 2.11979622\n",
            "Iteration 7596, loss = 2.11934652\n",
            "Iteration 7597, loss = 2.11889712\n",
            "Iteration 7598, loss = 2.11844800\n",
            "Iteration 7599, loss = 2.11799918\n",
            "Iteration 7600, loss = 2.11755065\n",
            "Iteration 7601, loss = 2.11710241\n",
            "Iteration 7602, loss = 2.11665447\n",
            "Iteration 7603, loss = 2.11620682\n",
            "Iteration 7604, loss = 2.11575945\n",
            "Iteration 7605, loss = 2.11531238\n",
            "Iteration 7606, loss = 2.11486561\n",
            "Iteration 7607, loss = 2.11441912\n",
            "Iteration 7608, loss = 2.11397292\n",
            "Iteration 7609, loss = 2.11352702\n",
            "Iteration 7610, loss = 2.11308140\n",
            "Iteration 7611, loss = 2.11263608\n",
            "Iteration 7612, loss = 2.11219104\n",
            "Iteration 7613, loss = 2.11174630\n",
            "Iteration 7614, loss = 2.11130185\n",
            "Iteration 7615, loss = 2.11085768\n",
            "Iteration 7616, loss = 2.11041381\n",
            "Iteration 7617, loss = 2.10997023\n",
            "Iteration 7618, loss = 2.10952693\n",
            "Iteration 7619, loss = 2.10908392\n",
            "Iteration 7620, loss = 2.10864121\n",
            "Iteration 7621, loss = 2.10819878\n",
            "Iteration 7622, loss = 2.10775664\n",
            "Iteration 7623, loss = 2.10731479\n",
            "Iteration 7624, loss = 2.10687322\n",
            "Iteration 7625, loss = 2.10643195\n",
            "Iteration 7626, loss = 2.10599096\n",
            "Iteration 7627, loss = 2.10555026\n",
            "Iteration 7628, loss = 2.10510985\n",
            "Iteration 7629, loss = 2.10466973\n",
            "Iteration 7630, loss = 2.10422989\n",
            "Iteration 7631, loss = 2.10379034\n",
            "Iteration 7632, loss = 2.10335108\n",
            "Iteration 7633, loss = 2.10291210\n",
            "Iteration 7634, loss = 2.10247341\n",
            "Iteration 7635, loss = 2.10203501\n",
            "Iteration 7636, loss = 2.10159689\n",
            "Iteration 7637, loss = 2.10115906\n",
            "Iteration 7638, loss = 2.10072151\n",
            "Iteration 7639, loss = 2.10028425\n",
            "Iteration 7640, loss = 2.09984728\n",
            "Iteration 7641, loss = 2.09941059\n",
            "Iteration 7642, loss = 2.09897419\n",
            "Iteration 7643, loss = 2.09853807\n",
            "Iteration 7644, loss = 2.09810224\n",
            "Iteration 7645, loss = 2.09766669\n",
            "Iteration 7646, loss = 2.09723142\n",
            "Iteration 7647, loss = 2.09679644\n",
            "Iteration 7648, loss = 2.09636175\n",
            "Iteration 7649, loss = 2.09592734\n",
            "Iteration 7650, loss = 2.09549321\n",
            "Iteration 7651, loss = 2.09505936\n",
            "Iteration 7652, loss = 2.09462580\n",
            "Iteration 7653, loss = 2.09419253\n",
            "Iteration 7654, loss = 2.09375953\n",
            "Iteration 7655, loss = 2.09332682\n",
            "Iteration 7656, loss = 2.09289439\n",
            "Iteration 7657, loss = 2.09246225\n",
            "Iteration 7658, loss = 2.09203038\n",
            "Iteration 7659, loss = 2.09159880\n",
            "Iteration 7660, loss = 2.09116750\n",
            "Iteration 7661, loss = 2.09073649\n",
            "Iteration 7662, loss = 2.09030575\n",
            "Iteration 7663, loss = 2.08987530\n",
            "Iteration 7664, loss = 2.08944513\n",
            "Iteration 7665, loss = 2.08901524\n",
            "Iteration 7666, loss = 2.08858563\n",
            "Iteration 7667, loss = 2.08815630\n",
            "Iteration 7668, loss = 2.08772725\n",
            "Iteration 7669, loss = 2.08729848\n",
            "Iteration 7670, loss = 2.08687000\n",
            "Iteration 7671, loss = 2.08644179\n",
            "Iteration 7672, loss = 2.08601387\n",
            "Iteration 7673, loss = 2.08558622\n",
            "Iteration 7674, loss = 2.08515885\n",
            "Iteration 7675, loss = 2.08473177\n",
            "Iteration 7676, loss = 2.08430496\n",
            "Iteration 7677, loss = 2.08387843\n",
            "Iteration 7678, loss = 2.08345218\n",
            "Iteration 7679, loss = 2.08302621\n",
            "Iteration 7680, loss = 2.08260052\n",
            "Iteration 7681, loss = 2.08217511\n",
            "Iteration 7682, loss = 2.08174998\n",
            "Iteration 7683, loss = 2.08132512\n",
            "Iteration 7684, loss = 2.08090055\n",
            "Iteration 7685, loss = 2.08047625\n",
            "Iteration 7686, loss = 2.08005223\n",
            "Iteration 7687, loss = 2.07962848\n",
            "Iteration 7688, loss = 2.07920502\n",
            "Iteration 7689, loss = 2.07878183\n",
            "Iteration 7690, loss = 2.07835891\n",
            "Iteration 7691, loss = 2.07793628\n",
            "Iteration 7692, loss = 2.07751392\n",
            "Iteration 7693, loss = 2.07709184\n",
            "Iteration 7694, loss = 2.07667004\n",
            "Iteration 7695, loss = 2.07624851\n",
            "Iteration 7696, loss = 2.07582726\n",
            "Iteration 7697, loss = 2.07540628\n",
            "Iteration 7698, loss = 2.07498558\n",
            "Iteration 7699, loss = 2.07456515\n",
            "Iteration 7700, loss = 2.07414500\n",
            "Iteration 7701, loss = 2.07372513\n",
            "Iteration 7702, loss = 2.07330553\n",
            "Iteration 7703, loss = 2.07288621\n",
            "Iteration 7704, loss = 2.07246716\n",
            "Iteration 7705, loss = 2.07204838\n",
            "Iteration 7706, loss = 2.07162989\n",
            "Iteration 7707, loss = 2.07121166\n",
            "Iteration 7708, loss = 2.07079371\n",
            "Iteration 7709, loss = 2.07037603\n",
            "Iteration 7710, loss = 2.06995863\n",
            "Iteration 7711, loss = 2.06954150\n",
            "Iteration 7712, loss = 2.06912464\n",
            "Iteration 7713, loss = 2.06870806\n",
            "Iteration 7714, loss = 2.06829175\n",
            "Iteration 7715, loss = 2.06787571\n",
            "Iteration 7716, loss = 2.06745995\n",
            "Iteration 7717, loss = 2.06704446\n",
            "Iteration 7718, loss = 2.06662924\n",
            "Iteration 7719, loss = 2.06621430\n",
            "Iteration 7720, loss = 2.06579962\n",
            "Iteration 7721, loss = 2.06538522\n",
            "Iteration 7722, loss = 2.06497109\n",
            "Iteration 7723, loss = 2.06455723\n",
            "Iteration 7724, loss = 2.06414365\n",
            "Iteration 7725, loss = 2.06373033\n",
            "Iteration 7726, loss = 2.06331729\n",
            "Iteration 7727, loss = 2.06290452\n",
            "Iteration 7728, loss = 2.06249202\n",
            "Iteration 7729, loss = 2.06207978\n",
            "Iteration 7730, loss = 2.06166782\n",
            "Iteration 7731, loss = 2.06125613\n",
            "Iteration 7732, loss = 2.06084472\n",
            "Iteration 7733, loss = 2.06043357\n",
            "Iteration 7734, loss = 2.06002269\n",
            "Iteration 7735, loss = 2.05961208\n",
            "Iteration 7736, loss = 2.05920174\n",
            "Iteration 7737, loss = 2.05879167\n",
            "Iteration 7738, loss = 2.05838187\n",
            "Iteration 7739, loss = 2.05797233\n",
            "Iteration 7740, loss = 2.05756307\n",
            "Iteration 7741, loss = 2.05715408\n",
            "Iteration 7742, loss = 2.05674535\n",
            "Iteration 7743, loss = 2.05633690\n",
            "Iteration 7744, loss = 2.05592871\n",
            "Iteration 7745, loss = 2.05552079\n",
            "Iteration 7746, loss = 2.05511313\n",
            "Iteration 7747, loss = 2.05470575\n",
            "Iteration 7748, loss = 2.05429863\n",
            "Iteration 7749, loss = 2.05389178\n",
            "Iteration 7750, loss = 2.05348520\n",
            "Iteration 7751, loss = 2.05307889\n",
            "Iteration 7752, loss = 2.05267284\n",
            "Iteration 7753, loss = 2.05226706\n",
            "Iteration 7754, loss = 2.05186155\n",
            "Iteration 7755, loss = 2.05145630\n",
            "Iteration 7756, loss = 2.05105132\n",
            "Iteration 7757, loss = 2.05064661\n",
            "Iteration 7758, loss = 2.05024216\n",
            "Iteration 7759, loss = 2.04983798\n",
            "Iteration 7760, loss = 2.04943406\n",
            "Iteration 7761, loss = 2.04903041\n",
            "Iteration 7762, loss = 2.04862702\n",
            "Iteration 7763, loss = 2.04822390\n",
            "Iteration 7764, loss = 2.04782105\n",
            "Iteration 7765, loss = 2.04741846\n",
            "Iteration 7766, loss = 2.04701613\n",
            "Iteration 7767, loss = 2.04661407\n",
            "Iteration 7768, loss = 2.04621228\n",
            "Iteration 7769, loss = 2.04581075\n",
            "Iteration 7770, loss = 2.04540948\n",
            "Iteration 7771, loss = 2.04500848\n",
            "Iteration 7772, loss = 2.04460774\n",
            "Iteration 7773, loss = 2.04420726\n",
            "Iteration 7774, loss = 2.04380705\n",
            "Iteration 7775, loss = 2.04340710\n",
            "Iteration 7776, loss = 2.04300742\n",
            "Iteration 7777, loss = 2.04260800\n",
            "Iteration 7778, loss = 2.04220884\n",
            "Iteration 7779, loss = 2.04180994\n",
            "Iteration 7780, loss = 2.04141131\n",
            "Iteration 7781, loss = 2.04101294\n",
            "Iteration 7782, loss = 2.04061483\n",
            "Iteration 7783, loss = 2.04021698\n",
            "Iteration 7784, loss = 2.03981940\n",
            "Iteration 7785, loss = 2.03942207\n",
            "Iteration 7786, loss = 2.03902501\n",
            "Iteration 7787, loss = 2.03862821\n",
            "Iteration 7788, loss = 2.03823168\n",
            "Iteration 7789, loss = 2.03783540\n",
            "Iteration 7790, loss = 2.03743938\n",
            "Iteration 7791, loss = 2.03704363\n",
            "Iteration 7792, loss = 2.03664813\n",
            "Iteration 7793, loss = 2.03625290\n",
            "Iteration 7794, loss = 2.03585793\n",
            "Iteration 7795, loss = 2.03546322\n",
            "Iteration 7796, loss = 2.03506876\n",
            "Iteration 7797, loss = 2.03467457\n",
            "Iteration 7798, loss = 2.03428064\n",
            "Iteration 7799, loss = 2.03388697\n",
            "Iteration 7800, loss = 2.03349355\n",
            "Iteration 7801, loss = 2.03310040\n",
            "Iteration 7802, loss = 2.03270750\n",
            "Iteration 7803, loss = 2.03231487\n",
            "Iteration 7804, loss = 2.03192249\n",
            "Iteration 7805, loss = 2.03153037\n",
            "Iteration 7806, loss = 2.03113851\n",
            "Iteration 7807, loss = 2.03074691\n",
            "Iteration 7808, loss = 2.03035557\n",
            "Iteration 7809, loss = 2.02996448\n",
            "Iteration 7810, loss = 2.02957366\n",
            "Iteration 7811, loss = 2.02918309\n",
            "Iteration 7812, loss = 2.02879278\n",
            "Iteration 7813, loss = 2.02840272\n",
            "Iteration 7814, loss = 2.02801293\n",
            "Iteration 7815, loss = 2.02762339\n",
            "Iteration 7816, loss = 2.02723411\n",
            "Iteration 7817, loss = 2.02684508\n",
            "Iteration 7818, loss = 2.02645631\n",
            "Iteration 7819, loss = 2.02606780\n",
            "Iteration 7820, loss = 2.02567954\n",
            "Iteration 7821, loss = 2.02529155\n",
            "Iteration 7822, loss = 2.02490380\n",
            "Iteration 7823, loss = 2.02451632\n",
            "Iteration 7824, loss = 2.02412908\n",
            "Iteration 7825, loss = 2.02374211\n",
            "Iteration 7826, loss = 2.02335539\n",
            "Iteration 7827, loss = 2.02296892\n",
            "Iteration 7828, loss = 2.02258271\n",
            "Iteration 7829, loss = 2.02219676\n",
            "Iteration 7830, loss = 2.02181106\n",
            "Iteration 7831, loss = 2.02142562\n",
            "Iteration 7832, loss = 2.02104043\n",
            "Iteration 7833, loss = 2.02065549\n",
            "Iteration 7834, loss = 2.02027081\n",
            "Iteration 7835, loss = 2.01988638\n",
            "Iteration 7836, loss = 2.01950221\n",
            "Iteration 7837, loss = 2.01911829\n",
            "Iteration 7838, loss = 2.01873462\n",
            "Iteration 7839, loss = 2.01835121\n",
            "Iteration 7840, loss = 2.01796805\n",
            "Iteration 7841, loss = 2.01758515\n",
            "Iteration 7842, loss = 2.01720249\n",
            "Iteration 7843, loss = 2.01682009\n",
            "Iteration 7844, loss = 2.01643795\n",
            "Iteration 7845, loss = 2.01605605\n",
            "Iteration 7846, loss = 2.01567441\n",
            "Iteration 7847, loss = 2.01529302\n",
            "Iteration 7848, loss = 2.01491188\n",
            "Iteration 7849, loss = 2.01453100\n",
            "Iteration 7850, loss = 2.01415036\n",
            "Iteration 7851, loss = 2.01376998\n",
            "Iteration 7852, loss = 2.01338985\n",
            "Iteration 7853, loss = 2.01300997\n",
            "Iteration 7854, loss = 2.01263035\n",
            "Iteration 7855, loss = 2.01225097\n",
            "Iteration 7856, loss = 2.01187184\n",
            "Iteration 7857, loss = 2.01149297\n",
            "Iteration 7858, loss = 2.01111435\n",
            "Iteration 7859, loss = 2.01073597\n",
            "Iteration 7860, loss = 2.01035785\n",
            "Iteration 7861, loss = 2.00997998\n",
            "Iteration 7862, loss = 2.00960235\n",
            "Iteration 7863, loss = 2.00922498\n",
            "Iteration 7864, loss = 2.00884786\n",
            "Iteration 7865, loss = 2.00847098\n",
            "Iteration 7866, loss = 2.00809436\n",
            "Iteration 7867, loss = 2.00771798\n",
            "Iteration 7868, loss = 2.00734186\n",
            "Iteration 7869, loss = 2.00696598\n",
            "Iteration 7870, loss = 2.00659035\n",
            "Iteration 7871, loss = 2.00621497\n",
            "Iteration 7872, loss = 2.00583984\n",
            "Iteration 7873, loss = 2.00546496\n",
            "Iteration 7874, loss = 2.00509033\n",
            "Iteration 7875, loss = 2.00471594\n",
            "Iteration 7876, loss = 2.00434180\n",
            "Iteration 7877, loss = 2.00396791\n",
            "Iteration 7878, loss = 2.00359427\n",
            "Iteration 7879, loss = 2.00322088\n",
            "Iteration 7880, loss = 2.00284773\n",
            "Iteration 7881, loss = 2.00247483\n",
            "Iteration 7882, loss = 2.00210218\n",
            "Iteration 7883, loss = 2.00172977\n",
            "Iteration 7884, loss = 2.00135761\n",
            "Iteration 7885, loss = 2.00098570\n",
            "Iteration 7886, loss = 2.00061403\n",
            "Iteration 7887, loss = 2.00024261\n",
            "Iteration 7888, loss = 1.99987144\n",
            "Iteration 7889, loss = 1.99950051\n",
            "Iteration 7890, loss = 1.99912983\n",
            "Iteration 7891, loss = 1.99875939\n",
            "Iteration 7892, loss = 1.99838920\n",
            "Iteration 7893, loss = 1.99801926\n",
            "Iteration 7894, loss = 1.99764956\n",
            "Iteration 7895, loss = 1.99728010\n",
            "Iteration 7896, loss = 1.99691089\n",
            "Iteration 7897, loss = 1.99654193\n",
            "Iteration 7898, loss = 1.99617321\n",
            "Iteration 7899, loss = 1.99580473\n",
            "Iteration 7900, loss = 1.99543650\n",
            "Iteration 7901, loss = 1.99506851\n",
            "Iteration 7902, loss = 1.99470077\n",
            "Iteration 7903, loss = 1.99433327\n",
            "Iteration 7904, loss = 1.99396602\n",
            "Iteration 7905, loss = 1.99359900\n",
            "Iteration 7906, loss = 1.99323224\n",
            "Iteration 7907, loss = 1.99286571\n",
            "Iteration 7908, loss = 1.99249943\n",
            "Iteration 7909, loss = 1.99213339\n",
            "Iteration 7910, loss = 1.99176760\n",
            "Iteration 7911, loss = 1.99140205\n",
            "Iteration 7912, loss = 1.99103674\n",
            "Iteration 7913, loss = 1.99067167\n",
            "Iteration 7914, loss = 1.99030684\n",
            "Iteration 7915, loss = 1.98994226\n",
            "Iteration 7916, loss = 1.98957792\n",
            "Iteration 7917, loss = 1.98921382\n",
            "Iteration 7918, loss = 1.98884997\n",
            "Iteration 7919, loss = 1.98848635\n",
            "Iteration 7920, loss = 1.98812298\n",
            "Iteration 7921, loss = 1.98775984\n",
            "Iteration 7922, loss = 1.98739695\n",
            "Iteration 7923, loss = 1.98703430\n",
            "Iteration 7924, loss = 1.98667189\n",
            "Iteration 7925, loss = 1.98630973\n",
            "Iteration 7926, loss = 1.98594780\n",
            "Iteration 7927, loss = 1.98558611\n",
            "Iteration 7928, loss = 1.98522466\n",
            "Iteration 7929, loss = 1.98486346\n",
            "Iteration 7930, loss = 1.98450249\n",
            "Iteration 7931, loss = 1.98414176\n",
            "Iteration 7932, loss = 1.98378128\n",
            "Iteration 7933, loss = 1.98342103\n",
            "Iteration 7934, loss = 1.98306102\n",
            "Iteration 7935, loss = 1.98270125\n",
            "Iteration 7936, loss = 1.98234172\n",
            "Iteration 7937, loss = 1.98198243\n",
            "Iteration 7938, loss = 1.98162338\n",
            "Iteration 7939, loss = 1.98126457\n",
            "Iteration 7940, loss = 1.98090599\n",
            "Iteration 7941, loss = 1.98054766\n",
            "Iteration 7942, loss = 1.98018956\n",
            "Iteration 7943, loss = 1.97983170\n",
            "Iteration 7944, loss = 1.97947408\n",
            "Iteration 7945, loss = 1.97911670\n",
            "Iteration 7946, loss = 1.97875955\n",
            "Iteration 7947, loss = 1.97840264\n",
            "Iteration 7948, loss = 1.97804597\n",
            "Iteration 7949, loss = 1.97768954\n",
            "Iteration 7950, loss = 1.97733334\n",
            "Iteration 7951, loss = 1.97697738\n",
            "Iteration 7952, loss = 1.97662166\n",
            "Iteration 7953, loss = 1.97626617\n",
            "Iteration 7954, loss = 1.97591093\n",
            "Iteration 7955, loss = 1.97555591\n",
            "Iteration 7956, loss = 1.97520114\n",
            "Iteration 7957, loss = 1.97484660\n",
            "Iteration 7958, loss = 1.97449229\n",
            "Iteration 7959, loss = 1.97413822\n",
            "Iteration 7960, loss = 1.97378439\n",
            "Iteration 7961, loss = 1.97343080\n",
            "Iteration 7962, loss = 1.97307743\n",
            "Iteration 7963, loss = 1.97272431\n",
            "Iteration 7964, loss = 1.97237142\n",
            "Iteration 7965, loss = 1.97201876\n",
            "Iteration 7966, loss = 1.97166634\n",
            "Iteration 7967, loss = 1.97131415\n",
            "Iteration 7968, loss = 1.97096220\n",
            "Iteration 7969, loss = 1.97061049\n",
            "Iteration 7970, loss = 1.97025900\n",
            "Iteration 7971, loss = 1.96990775\n",
            "Iteration 7972, loss = 1.96955674\n",
            "Iteration 7973, loss = 1.96920596\n",
            "Iteration 7974, loss = 1.96885541\n",
            "Iteration 7975, loss = 1.96850510\n",
            "Iteration 7976, loss = 1.96815502\n",
            "Iteration 7977, loss = 1.96780517\n",
            "Iteration 7978, loss = 1.96745556\n",
            "Iteration 7979, loss = 1.96710618\n",
            "Iteration 7980, loss = 1.96675703\n",
            "Iteration 7981, loss = 1.96640812\n",
            "Iteration 7982, loss = 1.96605944\n",
            "Iteration 7983, loss = 1.96571099\n",
            "Iteration 7984, loss = 1.96536277\n",
            "Iteration 7985, loss = 1.96501478\n",
            "Iteration 7986, loss = 1.96466703\n",
            "Iteration 7987, loss = 1.96431951\n",
            "Iteration 7988, loss = 1.96397222\n",
            "Iteration 7989, loss = 1.96362516\n",
            "Iteration 7990, loss = 1.96327834\n",
            "Iteration 7991, loss = 1.96293174\n",
            "Iteration 7992, loss = 1.96258538\n",
            "Iteration 7993, loss = 1.96223925\n",
            "Iteration 7994, loss = 1.96189335\n",
            "Iteration 7995, loss = 1.96154768\n",
            "Iteration 7996, loss = 1.96120224\n",
            "Iteration 7997, loss = 1.96085703\n",
            "Iteration 7998, loss = 1.96051205\n",
            "Iteration 7999, loss = 1.96016730\n",
            "Iteration 8000, loss = 1.95982278\n",
            "Iteration 8001, loss = 1.95947849\n",
            "Iteration 8002, loss = 1.95913443\n",
            "Iteration 8003, loss = 1.95879061\n",
            "Iteration 8004, loss = 1.95844701\n",
            "Iteration 8005, loss = 1.95810364\n",
            "Iteration 8006, loss = 1.95776050\n",
            "Iteration 8007, loss = 1.95741758\n",
            "Iteration 8008, loss = 1.95707490\n",
            "Iteration 8009, loss = 1.95673245\n",
            "Iteration 8010, loss = 1.95639022\n",
            "Iteration 8011, loss = 1.95604823\n",
            "Iteration 8012, loss = 1.95570646\n",
            "Iteration 8013, loss = 1.95536492\n",
            "Iteration 8014, loss = 1.95502361\n",
            "Iteration 8015, loss = 1.95468252\n",
            "Iteration 8016, loss = 1.95434167\n",
            "Iteration 8017, loss = 1.95400104\n",
            "Iteration 8018, loss = 1.95366064\n",
            "Iteration 8019, loss = 1.95332047\n",
            "Iteration 8020, loss = 1.95298052\n",
            "Iteration 8021, loss = 1.95264080\n",
            "Iteration 8022, loss = 1.95230131\n",
            "Iteration 8023, loss = 1.95196205\n",
            "Iteration 8024, loss = 1.95162301\n",
            "Iteration 8025, loss = 1.95128420\n",
            "Iteration 8026, loss = 1.95094562\n",
            "Iteration 8027, loss = 1.95060726\n",
            "Iteration 8028, loss = 1.95026913\n",
            "Iteration 8029, loss = 1.94993122\n",
            "Iteration 8030, loss = 1.94959354\n",
            "Iteration 8031, loss = 1.94925609\n",
            "Iteration 8032, loss = 1.94891886\n",
            "Iteration 8033, loss = 1.94858186\n",
            "Iteration 8034, loss = 1.94824508\n",
            "Iteration 8035, loss = 1.94790853\n",
            "Iteration 8036, loss = 1.94757220\n",
            "Iteration 8037, loss = 1.94723610\n",
            "Iteration 8038, loss = 1.94690023\n",
            "Iteration 8039, loss = 1.94656457\n",
            "Iteration 8040, loss = 1.94622915\n",
            "Iteration 8041, loss = 1.94589394\n",
            "Iteration 8042, loss = 1.94555897\n",
            "Iteration 8043, loss = 1.94522421\n",
            "Iteration 8044, loss = 1.94488968\n",
            "Iteration 8045, loss = 1.94455538\n",
            "Iteration 8046, loss = 1.94422129\n",
            "Iteration 8047, loss = 1.94388744\n",
            "Iteration 8048, loss = 1.94355380\n",
            "Iteration 8049, loss = 1.94322039\n",
            "Iteration 8050, loss = 1.94288720\n",
            "Iteration 8051, loss = 1.94255423\n",
            "Iteration 8052, loss = 1.94222149\n",
            "Iteration 8053, loss = 1.94188897\n",
            "Iteration 8054, loss = 1.94155668\n",
            "Iteration 8055, loss = 1.94122460\n",
            "Iteration 8056, loss = 1.94089275\n",
            "Iteration 8057, loss = 1.94056112\n",
            "Iteration 8058, loss = 1.94022971\n",
            "Iteration 8059, loss = 1.93989853\n",
            "Iteration 8060, loss = 1.93956756\n",
            "Iteration 8061, loss = 1.93923682\n",
            "Iteration 8062, loss = 1.93890630\n",
            "Iteration 8063, loss = 1.93857600\n",
            "Iteration 8064, loss = 1.93824593\n",
            "Iteration 8065, loss = 1.93791607\n",
            "Iteration 8066, loss = 1.93758643\n",
            "Iteration 8067, loss = 1.93725702\n",
            "Iteration 8068, loss = 1.93692783\n",
            "Iteration 8069, loss = 1.93659886\n",
            "Iteration 8070, loss = 1.93627010\n",
            "Iteration 8071, loss = 1.93594157\n",
            "Iteration 8072, loss = 1.93561326\n",
            "Iteration 8073, loss = 1.93528517\n",
            "Iteration 8074, loss = 1.93495730\n",
            "Iteration 8075, loss = 1.93462965\n",
            "Iteration 8076, loss = 1.93430222\n",
            "Iteration 8077, loss = 1.93397500\n",
            "Iteration 8078, loss = 1.93364801\n",
            "Iteration 8079, loss = 1.93332124\n",
            "Iteration 8080, loss = 1.93299469\n",
            "Iteration 8081, loss = 1.93266835\n",
            "Iteration 8082, loss = 1.93234224\n",
            "Iteration 8083, loss = 1.93201634\n",
            "Iteration 8084, loss = 1.93169066\n",
            "Iteration 8085, loss = 1.93136520\n",
            "Iteration 8086, loss = 1.93103996\n",
            "Iteration 8087, loss = 1.93071494\n",
            "Iteration 8088, loss = 1.93039013\n",
            "Iteration 8089, loss = 1.93006555\n",
            "Iteration 8090, loss = 1.92974118\n",
            "Iteration 8091, loss = 1.92941703\n",
            "Iteration 8092, loss = 1.92909309\n",
            "Iteration 8093, loss = 1.92876938\n",
            "Iteration 8094, loss = 1.92844588\n",
            "Iteration 8095, loss = 1.92812260\n",
            "Iteration 8096, loss = 1.92779953\n",
            "Iteration 8097, loss = 1.92747669\n",
            "Iteration 8098, loss = 1.92715406\n",
            "Iteration 8099, loss = 1.92683164\n",
            "Iteration 8100, loss = 1.92650945\n",
            "Iteration 8101, loss = 1.92618746\n",
            "Iteration 8102, loss = 1.92586570\n",
            "Iteration 8103, loss = 1.92554415\n",
            "Iteration 8104, loss = 1.92522282\n",
            "Iteration 8105, loss = 1.92490170\n",
            "Iteration 8106, loss = 1.92458080\n",
            "Iteration 8107, loss = 1.92426012\n",
            "Iteration 8108, loss = 1.92393965\n",
            "Iteration 8109, loss = 1.92361940\n",
            "Iteration 8110, loss = 1.92329936\n",
            "Iteration 8111, loss = 1.92297953\n",
            "Iteration 8112, loss = 1.92265992\n",
            "Iteration 8113, loss = 1.92234053\n",
            "Iteration 8114, loss = 1.92202135\n",
            "Iteration 8115, loss = 1.92170239\n",
            "Iteration 8116, loss = 1.92138364\n",
            "Iteration 8117, loss = 1.92106510\n",
            "Iteration 8118, loss = 1.92074678\n",
            "Iteration 8119, loss = 1.92042867\n",
            "Iteration 8120, loss = 1.92011078\n",
            "Iteration 8121, loss = 1.91979310\n",
            "Iteration 8122, loss = 1.91947564\n",
            "Iteration 8123, loss = 1.91915838\n",
            "Iteration 8124, loss = 1.91884134\n",
            "Iteration 8125, loss = 1.91852452\n",
            "Iteration 8126, loss = 1.91820791\n",
            "Iteration 8127, loss = 1.91789151\n",
            "Iteration 8128, loss = 1.91757532\n",
            "Iteration 8129, loss = 1.91725935\n",
            "Iteration 8130, loss = 1.91694359\n",
            "Iteration 8131, loss = 1.91662804\n",
            "Iteration 8132, loss = 1.91631270\n",
            "Iteration 8133, loss = 1.91599758\n",
            "Iteration 8134, loss = 1.91568267\n",
            "Iteration 8135, loss = 1.91536797\n",
            "Iteration 8136, loss = 1.91505348\n",
            "Iteration 8137, loss = 1.91473920\n",
            "Iteration 8138, loss = 1.91442514\n",
            "Iteration 8139, loss = 1.91411129\n",
            "Iteration 8140, loss = 1.91379764\n",
            "Iteration 8141, loss = 1.91348421\n",
            "Iteration 8142, loss = 1.91317099\n",
            "Iteration 8143, loss = 1.91285799\n",
            "Iteration 8144, loss = 1.91254519\n",
            "Iteration 8145, loss = 1.91223260\n",
            "Iteration 8146, loss = 1.91192022\n",
            "Iteration 8147, loss = 1.91160806\n",
            "Iteration 8148, loss = 1.91129610\n",
            "Iteration 8149, loss = 1.91098436\n",
            "Iteration 8150, loss = 1.91067282\n",
            "Iteration 8151, loss = 1.91036150\n",
            "Iteration 8152, loss = 1.91005038\n",
            "Iteration 8153, loss = 1.90973948\n",
            "Iteration 8154, loss = 1.90942878\n",
            "Iteration 8155, loss = 1.90911829\n",
            "Iteration 8156, loss = 1.90880802\n",
            "Iteration 8157, loss = 1.90849795\n",
            "Iteration 8158, loss = 1.90818809\n",
            "Iteration 8159, loss = 1.90787844\n",
            "Iteration 8160, loss = 1.90756900\n",
            "Iteration 8161, loss = 1.90725976\n",
            "Iteration 8162, loss = 1.90695074\n",
            "Iteration 8163, loss = 1.90664192\n",
            "Iteration 8164, loss = 1.90633332\n",
            "Iteration 8165, loss = 1.90602492\n",
            "Iteration 8166, loss = 1.90571672\n",
            "Iteration 8167, loss = 1.90540874\n",
            "Iteration 8168, loss = 1.90510096\n",
            "Iteration 8169, loss = 1.90479340\n",
            "Iteration 8170, loss = 1.90448604\n",
            "Iteration 8171, loss = 1.90417888\n",
            "Iteration 8172, loss = 1.90387194\n",
            "Iteration 8173, loss = 1.90356520\n",
            "Iteration 8174, loss = 1.90325866\n",
            "Iteration 8175, loss = 1.90295234\n",
            "Iteration 8176, loss = 1.90264622\n",
            "Iteration 8177, loss = 1.90234031\n",
            "Iteration 8178, loss = 1.90203460\n",
            "Iteration 8179, loss = 1.90172910\n",
            "Iteration 8180, loss = 1.90142381\n",
            "Iteration 8181, loss = 1.90111873\n",
            "Iteration 8182, loss = 1.90081385\n",
            "Iteration 8183, loss = 1.90050917\n",
            "Iteration 8184, loss = 1.90020470\n",
            "Iteration 8185, loss = 1.89990044\n",
            "Iteration 8186, loss = 1.89959638\n",
            "Iteration 8187, loss = 1.89929253\n",
            "Iteration 8188, loss = 1.89898888\n",
            "Iteration 8189, loss = 1.89868544\n",
            "Iteration 8190, loss = 1.89838220\n",
            "Iteration 8191, loss = 1.89807917\n",
            "Iteration 8192, loss = 1.89777634\n",
            "Iteration 8193, loss = 1.89747372\n",
            "Iteration 8194, loss = 1.89717130\n",
            "Iteration 8195, loss = 1.89686909\n",
            "Iteration 8196, loss = 1.89656708\n",
            "Iteration 8197, loss = 1.89626527\n",
            "Iteration 8198, loss = 1.89596367\n",
            "Iteration 8199, loss = 1.89566227\n",
            "Iteration 8200, loss = 1.89536108\n",
            "Iteration 8201, loss = 1.89506008\n",
            "Iteration 8202, loss = 1.89475930\n",
            "Iteration 8203, loss = 1.89445871\n",
            "Iteration 8204, loss = 1.89415833\n",
            "Iteration 8205, loss = 1.89385816\n",
            "Iteration 8206, loss = 1.89355818\n",
            "Iteration 8207, loss = 1.89325841\n",
            "Iteration 8208, loss = 1.89295884\n",
            "Iteration 8209, loss = 1.89265947\n",
            "Iteration 8210, loss = 1.89236031\n",
            "Iteration 8211, loss = 1.89206135\n",
            "Iteration 8212, loss = 1.89176259\n",
            "Iteration 8213, loss = 1.89146403\n",
            "Iteration 8214, loss = 1.89116568\n",
            "Iteration 8215, loss = 1.89086752\n",
            "Iteration 8216, loss = 1.89056957\n",
            "Iteration 8217, loss = 1.89027182\n",
            "Iteration 8218, loss = 1.88997427\n",
            "Iteration 8219, loss = 1.88967693\n",
            "Iteration 8220, loss = 1.88937978\n",
            "Iteration 8221, loss = 1.88908284\n",
            "Iteration 8222, loss = 1.88878609\n",
            "Iteration 8223, loss = 1.88848955\n",
            "Iteration 8224, loss = 1.88819321\n",
            "Iteration 8225, loss = 1.88789706\n",
            "Iteration 8226, loss = 1.88760112\n",
            "Iteration 8227, loss = 1.88730538\n",
            "Iteration 8228, loss = 1.88700984\n",
            "Iteration 8229, loss = 1.88671450\n",
            "Iteration 8230, loss = 1.88641936\n",
            "Iteration 8231, loss = 1.88612442\n",
            "Iteration 8232, loss = 1.88582968\n",
            "Iteration 8233, loss = 1.88553514\n",
            "Iteration 8234, loss = 1.88524080\n",
            "Iteration 8235, loss = 1.88494666\n",
            "Iteration 8236, loss = 1.88465271\n",
            "Iteration 8237, loss = 1.88435897\n",
            "Iteration 8238, loss = 1.88406542\n",
            "Iteration 8239, loss = 1.88377208\n",
            "Iteration 8240, loss = 1.88347893\n",
            "Iteration 8241, loss = 1.88318598\n",
            "Iteration 8242, loss = 1.88289323\n",
            "Iteration 8243, loss = 1.88260068\n",
            "Iteration 8244, loss = 1.88230833\n",
            "Iteration 8245, loss = 1.88201617\n",
            "Iteration 8246, loss = 1.88172422\n",
            "Iteration 8247, loss = 1.88143246\n",
            "Iteration 8248, loss = 1.88114090\n",
            "Iteration 8249, loss = 1.88084953\n",
            "Iteration 8250, loss = 1.88055837\n",
            "Iteration 8251, loss = 1.88026740\n",
            "Iteration 8252, loss = 1.87997663\n",
            "Iteration 8253, loss = 1.87968605\n",
            "Iteration 8254, loss = 1.87939568\n",
            "Iteration 8255, loss = 1.87910550\n",
            "Iteration 8256, loss = 1.87881551\n",
            "Iteration 8257, loss = 1.87852573\n",
            "Iteration 8258, loss = 1.87823614\n",
            "Iteration 8259, loss = 1.87794674\n",
            "Iteration 8260, loss = 1.87765755\n",
            "Iteration 8261, loss = 1.87736854\n",
            "Iteration 8262, loss = 1.87707974\n",
            "Iteration 8263, loss = 1.87679113\n",
            "Iteration 8264, loss = 1.87650272\n",
            "Iteration 8265, loss = 1.87621450\n",
            "Iteration 8266, loss = 1.87592648\n",
            "Iteration 8267, loss = 1.87563865\n",
            "Iteration 8268, loss = 1.87535102\n",
            "Iteration 8269, loss = 1.87506359\n",
            "Iteration 8270, loss = 1.87477635\n",
            "Iteration 8271, loss = 1.87448930\n",
            "Iteration 8272, loss = 1.87420245\n",
            "Iteration 8273, loss = 1.87391579\n",
            "Iteration 8274, loss = 1.87362933\n",
            "Iteration 8275, loss = 1.87334307\n",
            "Iteration 8276, loss = 1.87305699\n",
            "Iteration 8277, loss = 1.87277112\n",
            "Iteration 8278, loss = 1.87248543\n",
            "Iteration 8279, loss = 1.87219994\n",
            "Iteration 8280, loss = 1.87191465\n",
            "Iteration 8281, loss = 1.87162954\n",
            "Iteration 8282, loss = 1.87134464\n",
            "Iteration 8283, loss = 1.87105992\n",
            "Iteration 8284, loss = 1.87077540\n",
            "Iteration 8285, loss = 1.87049107\n",
            "Iteration 8286, loss = 1.87020694\n",
            "Iteration 8287, loss = 1.86992300\n",
            "Iteration 8288, loss = 1.86963925\n",
            "Iteration 8289, loss = 1.86935569\n",
            "Iteration 8290, loss = 1.86907233\n",
            "Iteration 8291, loss = 1.86878916\n",
            "Iteration 8292, loss = 1.86850618\n",
            "Iteration 8293, loss = 1.86822339\n",
            "Iteration 8294, loss = 1.86794080\n",
            "Iteration 8295, loss = 1.86765840\n",
            "Iteration 8296, loss = 1.86737619\n",
            "Iteration 8297, loss = 1.86709417\n",
            "Iteration 8298, loss = 1.86681235\n",
            "Iteration 8299, loss = 1.86653071\n",
            "Iteration 8300, loss = 1.86624927\n",
            "Iteration 8301, loss = 1.86596802\n",
            "Iteration 8302, loss = 1.86568696\n",
            "Iteration 8303, loss = 1.86540609\n",
            "Iteration 8304, loss = 1.86512541\n",
            "Iteration 8305, loss = 1.86484493\n",
            "Iteration 8306, loss = 1.86456463\n",
            "Iteration 8307, loss = 1.86428453\n",
            "Iteration 8308, loss = 1.86400461\n",
            "Iteration 8309, loss = 1.86372489\n",
            "Iteration 8310, loss = 1.86344535\n",
            "Iteration 8311, loss = 1.86316601\n",
            "Iteration 8312, loss = 1.86288686\n",
            "Iteration 8313, loss = 1.86260789\n",
            "Iteration 8314, loss = 1.86232912\n",
            "Iteration 8315, loss = 1.86205054\n",
            "Iteration 8316, loss = 1.86177214\n",
            "Iteration 8317, loss = 1.86149394\n",
            "Iteration 8318, loss = 1.86121592\n",
            "Iteration 8319, loss = 1.86093810\n",
            "Iteration 8320, loss = 1.86066046\n",
            "Iteration 8321, loss = 1.86038301\n",
            "Iteration 8322, loss = 1.86010576\n",
            "Iteration 8323, loss = 1.85982869\n",
            "Iteration 8324, loss = 1.85955181\n",
            "Iteration 8325, loss = 1.85927511\n",
            "Iteration 8326, loss = 1.85899861\n",
            "Iteration 8327, loss = 1.85872229\n",
            "Iteration 8328, loss = 1.85844617\n",
            "Iteration 8329, loss = 1.85817023\n",
            "Iteration 8330, loss = 1.85789448\n",
            "Iteration 8331, loss = 1.85761891\n",
            "Iteration 8332, loss = 1.85734354\n",
            "Iteration 8333, loss = 1.85706835\n",
            "Iteration 8334, loss = 1.85679335\n",
            "Iteration 8335, loss = 1.85651854\n",
            "Iteration 8336, loss = 1.85624391\n",
            "Iteration 8337, loss = 1.85596947\n",
            "Iteration 8338, loss = 1.85569522\n",
            "Iteration 8339, loss = 1.85542116\n",
            "Iteration 8340, loss = 1.85514728\n",
            "Iteration 8341, loss = 1.85487359\n",
            "Iteration 8342, loss = 1.85460008\n",
            "Iteration 8343, loss = 1.85432676\n",
            "Iteration 8344, loss = 1.85405363\n",
            "Iteration 8345, loss = 1.85378069\n",
            "Iteration 8346, loss = 1.85350793\n",
            "Iteration 8347, loss = 1.85323536\n",
            "Iteration 8348, loss = 1.85296297\n",
            "Iteration 8349, loss = 1.85269077\n",
            "Iteration 8350, loss = 1.85241875\n",
            "Iteration 8351, loss = 1.85214692\n",
            "Iteration 8352, loss = 1.85187528\n",
            "Iteration 8353, loss = 1.85160382\n",
            "Iteration 8354, loss = 1.85133254\n",
            "Iteration 8355, loss = 1.85106145\n",
            "Iteration 8356, loss = 1.85079055\n",
            "Iteration 8357, loss = 1.85051983\n",
            "Iteration 8358, loss = 1.85024930\n",
            "Iteration 8359, loss = 1.84997895\n",
            "Iteration 8360, loss = 1.84970878\n",
            "Iteration 8361, loss = 1.84943880\n",
            "Iteration 8362, loss = 1.84916900\n",
            "Iteration 8363, loss = 1.84889939\n",
            "Iteration 8364, loss = 1.84862996\n",
            "Iteration 8365, loss = 1.84836072\n",
            "Iteration 8366, loss = 1.84809166\n",
            "Iteration 8367, loss = 1.84782278\n",
            "Iteration 8368, loss = 1.84755409\n",
            "Iteration 8369, loss = 1.84728558\n",
            "Iteration 8370, loss = 1.84701725\n",
            "Iteration 8371, loss = 1.84674911\n",
            "Iteration 8372, loss = 1.84648115\n",
            "Iteration 8373, loss = 1.84621337\n",
            "Iteration 8374, loss = 1.84594578\n",
            "Iteration 8375, loss = 1.84567836\n",
            "Iteration 8376, loss = 1.84541114\n",
            "Iteration 8377, loss = 1.84514409\n",
            "Iteration 8378, loss = 1.84487723\n",
            "Iteration 8379, loss = 1.84461054\n",
            "Iteration 8380, loss = 1.84434404\n",
            "Iteration 8381, loss = 1.84407773\n",
            "Iteration 8382, loss = 1.84381159\n",
            "Iteration 8383, loss = 1.84354564\n",
            "Iteration 8384, loss = 1.84327987\n",
            "Iteration 8385, loss = 1.84301428\n",
            "Iteration 8386, loss = 1.84274887\n",
            "Iteration 8387, loss = 1.84248364\n",
            "Iteration 8388, loss = 1.84221860\n",
            "Iteration 8389, loss = 1.84195373\n",
            "Iteration 8390, loss = 1.84168905\n",
            "Iteration 8391, loss = 1.84142455\n",
            "Iteration 8392, loss = 1.84116023\n",
            "Iteration 8393, loss = 1.84089609\n",
            "Iteration 8394, loss = 1.84063213\n",
            "Iteration 8395, loss = 1.84036835\n",
            "Iteration 8396, loss = 1.84010475\n",
            "Iteration 8397, loss = 1.83984133\n",
            "Iteration 8398, loss = 1.83957809\n",
            "Iteration 8399, loss = 1.83931503\n",
            "Iteration 8400, loss = 1.83905215\n",
            "Iteration 8401, loss = 1.83878946\n",
            "Iteration 8402, loss = 1.83852694\n",
            "Iteration 8403, loss = 1.83826460\n",
            "Iteration 8404, loss = 1.83800244\n",
            "Iteration 8405, loss = 1.83774046\n",
            "Iteration 8406, loss = 1.83747866\n",
            "Iteration 8407, loss = 1.83721703\n",
            "Iteration 8408, loss = 1.83695559\n",
            "Iteration 8409, loss = 1.83669433\n",
            "Iteration 8410, loss = 1.83643324\n",
            "Iteration 8411, loss = 1.83617234\n",
            "Iteration 8412, loss = 1.83591161\n",
            "Iteration 8413, loss = 1.83565106\n",
            "Iteration 8414, loss = 1.83539069\n",
            "Iteration 8415, loss = 1.83513050\n",
            "Iteration 8416, loss = 1.83487048\n",
            "Iteration 8417, loss = 1.83461064\n",
            "Iteration 8418, loss = 1.83435099\n",
            "Iteration 8419, loss = 1.83409150\n",
            "Iteration 8420, loss = 1.83383220\n",
            "Iteration 8421, loss = 1.83357308\n",
            "Iteration 8422, loss = 1.83331413\n",
            "Iteration 8423, loss = 1.83305536\n",
            "Iteration 8424, loss = 1.83279676\n",
            "Iteration 8425, loss = 1.83253835\n",
            "Iteration 8426, loss = 1.83228011\n",
            "Iteration 8427, loss = 1.83202205\n",
            "Iteration 8428, loss = 1.83176416\n",
            "Iteration 8429, loss = 1.83150645\n",
            "Iteration 8430, loss = 1.83124892\n",
            "Iteration 8431, loss = 1.83099157\n",
            "Iteration 8432, loss = 1.83073439\n",
            "Iteration 8433, loss = 1.83047738\n",
            "Iteration 8434, loss = 1.83022056\n",
            "Iteration 8435, loss = 1.82996391\n",
            "Iteration 8436, loss = 1.82970743\n",
            "Iteration 8437, loss = 1.82945113\n",
            "Iteration 8438, loss = 1.82919501\n",
            "Iteration 8439, loss = 1.82893906\n",
            "Iteration 8440, loss = 1.82868329\n",
            "Iteration 8441, loss = 1.82842769\n",
            "Iteration 8442, loss = 1.82817227\n",
            "Iteration 8443, loss = 1.82791702\n",
            "Iteration 8444, loss = 1.82766195\n",
            "Iteration 8445, loss = 1.82740705\n",
            "Iteration 8446, loss = 1.82715233\n",
            "Iteration 8447, loss = 1.82689779\n",
            "Iteration 8448, loss = 1.82664341\n",
            "Iteration 8449, loss = 1.82638922\n",
            "Iteration 8450, loss = 1.82613519\n",
            "Iteration 8451, loss = 1.82588134\n",
            "Iteration 8452, loss = 1.82562767\n",
            "Iteration 8453, loss = 1.82537417\n",
            "Iteration 8454, loss = 1.82512084\n",
            "Iteration 8455, loss = 1.82486769\n",
            "Iteration 8456, loss = 1.82461471\n",
            "Iteration 8457, loss = 1.82436190\n",
            "Iteration 8458, loss = 1.82410927\n",
            "Iteration 8459, loss = 1.82385681\n",
            "Iteration 8460, loss = 1.82360452\n",
            "Iteration 8461, loss = 1.82335241\n",
            "Iteration 8462, loss = 1.82310047\n",
            "Iteration 8463, loss = 1.82284871\n",
            "Iteration 8464, loss = 1.82259711\n",
            "Iteration 8465, loss = 1.82234569\n",
            "Iteration 8466, loss = 1.82209444\n",
            "Iteration 8467, loss = 1.82184337\n",
            "Iteration 8468, loss = 1.82159246\n",
            "Iteration 8469, loss = 1.82134173\n",
            "Iteration 8470, loss = 1.82109117\n",
            "Iteration 8471, loss = 1.82084079\n",
            "Iteration 8472, loss = 1.82059057\n",
            "Iteration 8473, loss = 1.82034053\n",
            "Iteration 8474, loss = 1.82009066\n",
            "Iteration 8475, loss = 1.81984096\n",
            "Iteration 8476, loss = 1.81959143\n",
            "Iteration 8477, loss = 1.81934207\n",
            "Iteration 8478, loss = 1.81909289\n",
            "Iteration 8479, loss = 1.81884387\n",
            "Iteration 8480, loss = 1.81859503\n",
            "Iteration 8481, loss = 1.81834636\n",
            "Iteration 8482, loss = 1.81809786\n",
            "Iteration 8483, loss = 1.81784952\n",
            "Iteration 8484, loss = 1.81760136\n",
            "Iteration 8485, loss = 1.81735338\n",
            "Iteration 8486, loss = 1.81710556\n",
            "Iteration 8487, loss = 1.81685791\n",
            "Iteration 8488, loss = 1.81661043\n",
            "Iteration 8489, loss = 1.81636312\n",
            "Iteration 8490, loss = 1.81611598\n",
            "Iteration 8491, loss = 1.81586902\n",
            "Iteration 8492, loss = 1.81562222\n",
            "Iteration 8493, loss = 1.81537559\n",
            "Iteration 8494, loss = 1.81512913\n",
            "Iteration 8495, loss = 1.81488284\n",
            "Iteration 8496, loss = 1.81463672\n",
            "Iteration 8497, loss = 1.81439077\n",
            "Iteration 8498, loss = 1.81414499\n",
            "Iteration 8499, loss = 1.81389937\n",
            "Iteration 8500, loss = 1.81365393\n",
            "Iteration 8501, loss = 1.81340865\n",
            "Iteration 8502, loss = 1.81316355\n",
            "Iteration 8503, loss = 1.81291861\n",
            "Iteration 8504, loss = 1.81267384\n",
            "Iteration 8505, loss = 1.81242924\n",
            "Iteration 8506, loss = 1.81218481\n",
            "Iteration 8507, loss = 1.81194054\n",
            "Iteration 8508, loss = 1.81169645\n",
            "Iteration 8509, loss = 1.81145252\n",
            "Iteration 8510, loss = 1.81120876\n",
            "Iteration 8511, loss = 1.81096517\n",
            "Iteration 8512, loss = 1.81072174\n",
            "Iteration 8513, loss = 1.81047848\n",
            "Iteration 8514, loss = 1.81023539\n",
            "Iteration 8515, loss = 1.80999247\n",
            "Iteration 8516, loss = 1.80974972\n",
            "Iteration 8517, loss = 1.80950713\n",
            "Iteration 8518, loss = 1.80926471\n",
            "Iteration 8519, loss = 1.80902245\n",
            "Iteration 8520, loss = 1.80878036\n",
            "Iteration 8521, loss = 1.80853844\n",
            "Iteration 8522, loss = 1.80829669\n",
            "Iteration 8523, loss = 1.80805510\n",
            "Iteration 8524, loss = 1.80781368\n",
            "Iteration 8525, loss = 1.80757243\n",
            "Iteration 8526, loss = 1.80733134\n",
            "Iteration 8527, loss = 1.80709041\n",
            "Iteration 8528, loss = 1.80684966\n",
            "Iteration 8529, loss = 1.80660906\n",
            "Iteration 8530, loss = 1.80636864\n",
            "Iteration 8531, loss = 1.80612838\n",
            "Iteration 8532, loss = 1.80588829\n",
            "Iteration 8533, loss = 1.80564836\n",
            "Iteration 8534, loss = 1.80540859\n",
            "Iteration 8535, loss = 1.80516899\n",
            "Iteration 8536, loss = 1.80492956\n",
            "Iteration 8537, loss = 1.80469029\n",
            "Iteration 8538, loss = 1.80445119\n",
            "Iteration 8539, loss = 1.80421225\n",
            "Iteration 8540, loss = 1.80397348\n",
            "Iteration 8541, loss = 1.80373487\n",
            "Iteration 8542, loss = 1.80349642\n",
            "Iteration 8543, loss = 1.80325814\n",
            "Iteration 8544, loss = 1.80302003\n",
            "Iteration 8545, loss = 1.80278208\n",
            "Iteration 8546, loss = 1.80254429\n",
            "Iteration 8547, loss = 1.80230666\n",
            "Iteration 8548, loss = 1.80206920\n",
            "Iteration 8549, loss = 1.80183191\n",
            "Iteration 8550, loss = 1.80159477\n",
            "Iteration 8551, loss = 1.80135780\n",
            "Iteration 8552, loss = 1.80112100\n",
            "Iteration 8553, loss = 1.80088436\n",
            "Iteration 8554, loss = 1.80064788\n",
            "Iteration 8555, loss = 1.80041156\n",
            "Iteration 8556, loss = 1.80017541\n",
            "Iteration 8557, loss = 1.79993942\n",
            "Iteration 8558, loss = 1.79970359\n",
            "Iteration 8559, loss = 1.79946792\n",
            "Iteration 8560, loss = 1.79923242\n",
            "Iteration 8561, loss = 1.79899708\n",
            "Iteration 8562, loss = 1.79876191\n",
            "Iteration 8563, loss = 1.79852689\n",
            "Iteration 8564, loss = 1.79829204\n",
            "Iteration 8565, loss = 1.79805735\n",
            "Iteration 8566, loss = 1.79782282\n",
            "Iteration 8567, loss = 1.79758845\n",
            "Iteration 8568, loss = 1.79735424\n",
            "Iteration 8569, loss = 1.79712020\n",
            "Iteration 8570, loss = 1.79688632\n",
            "Iteration 8571, loss = 1.79665260\n",
            "Iteration 8572, loss = 1.79641904\n",
            "Iteration 8573, loss = 1.79618564\n",
            "Iteration 8574, loss = 1.79595240\n",
            "Iteration 8575, loss = 1.79571933\n",
            "Iteration 8576, loss = 1.79548641\n",
            "Iteration 8577, loss = 1.79525366\n",
            "Iteration 8578, loss = 1.79502106\n",
            "Iteration 8579, loss = 1.79478863\n",
            "Iteration 8580, loss = 1.79455636\n",
            "Iteration 8581, loss = 1.79432425\n",
            "Iteration 8582, loss = 1.79409230\n",
            "Iteration 8583, loss = 1.79386050\n",
            "Iteration 8584, loss = 1.79362887\n",
            "Iteration 8585, loss = 1.79339740\n",
            "Iteration 8586, loss = 1.79316609\n",
            "Iteration 8587, loss = 1.79293494\n",
            "Iteration 8588, loss = 1.79270395\n",
            "Iteration 8589, loss = 1.79247311\n",
            "Iteration 8590, loss = 1.79224244\n",
            "Iteration 8591, loss = 1.79201193\n",
            "Iteration 8592, loss = 1.79178157\n",
            "Iteration 8593, loss = 1.79155138\n",
            "Iteration 8594, loss = 1.79132134\n",
            "Iteration 8595, loss = 1.79109146\n",
            "Iteration 8596, loss = 1.79086175\n",
            "Iteration 8597, loss = 1.79063219\n",
            "Iteration 8598, loss = 1.79040279\n",
            "Iteration 8599, loss = 1.79017354\n",
            "Iteration 8600, loss = 1.78994446\n",
            "Iteration 8601, loss = 1.78971553\n",
            "Iteration 8602, loss = 1.78948677\n",
            "Iteration 8603, loss = 1.78925816\n",
            "Iteration 8604, loss = 1.78902971\n",
            "Iteration 8605, loss = 1.78880141\n",
            "Iteration 8606, loss = 1.78857328\n",
            "Iteration 8607, loss = 1.78834530\n",
            "Iteration 8608, loss = 1.78811748\n",
            "Iteration 8609, loss = 1.78788982\n",
            "Iteration 8610, loss = 1.78766231\n",
            "Iteration 8611, loss = 1.78743497\n",
            "Iteration 8612, loss = 1.78720778\n",
            "Iteration 8613, loss = 1.78698074\n",
            "Iteration 8614, loss = 1.78675387\n",
            "Iteration 8615, loss = 1.78652715\n",
            "Iteration 8616, loss = 1.78630058\n",
            "Iteration 8617, loss = 1.78607418\n",
            "Iteration 8618, loss = 1.78584793\n",
            "Iteration 8619, loss = 1.78562184\n",
            "Iteration 8620, loss = 1.78539590\n",
            "Iteration 8621, loss = 1.78517012\n",
            "Iteration 8622, loss = 1.78494450\n",
            "Iteration 8623, loss = 1.78471903\n",
            "Iteration 8624, loss = 1.78449372\n",
            "Iteration 8625, loss = 1.78426856\n",
            "Iteration 8626, loss = 1.78404356\n",
            "Iteration 8627, loss = 1.78381872\n",
            "Iteration 8628, loss = 1.78359403\n",
            "Iteration 8629, loss = 1.78336950\n",
            "Iteration 8630, loss = 1.78314512\n",
            "Iteration 8631, loss = 1.78292089\n",
            "Iteration 8632, loss = 1.78269683\n",
            "Iteration 8633, loss = 1.78247292\n",
            "Iteration 8634, loss = 1.78224916\n",
            "Iteration 8635, loss = 1.78202556\n",
            "Iteration 8636, loss = 1.78180211\n",
            "Iteration 8637, loss = 1.78157882\n",
            "Iteration 8638, loss = 1.78135568\n",
            "Iteration 8639, loss = 1.78113269\n",
            "Iteration 8640, loss = 1.78090987\n",
            "Iteration 8641, loss = 1.78068719\n",
            "Iteration 8642, loss = 1.78046467\n",
            "Iteration 8643, loss = 1.78024230\n",
            "Iteration 8644, loss = 1.78002009\n",
            "Iteration 8645, loss = 1.77979803\n",
            "Iteration 8646, loss = 1.77957613\n",
            "Iteration 8647, loss = 1.77935438\n",
            "Iteration 8648, loss = 1.77913278\n",
            "Iteration 8649, loss = 1.77891133\n",
            "Iteration 8650, loss = 1.77869004\n",
            "Iteration 8651, loss = 1.77846891\n",
            "Iteration 8652, loss = 1.77824792\n",
            "Iteration 8653, loss = 1.77802709\n",
            "Iteration 8654, loss = 1.77780641\n",
            "Iteration 8655, loss = 1.77758589\n",
            "Iteration 8656, loss = 1.77736551\n",
            "Iteration 8657, loss = 1.77714529\n",
            "Iteration 8658, loss = 1.77692523\n",
            "Iteration 8659, loss = 1.77670531\n",
            "Iteration 8660, loss = 1.77648555\n",
            "Iteration 8661, loss = 1.77626594\n",
            "Iteration 8662, loss = 1.77604648\n",
            "Iteration 8663, loss = 1.77582718\n",
            "Iteration 8664, loss = 1.77560802\n",
            "Iteration 8665, loss = 1.77538902\n",
            "Iteration 8666, loss = 1.77517017\n",
            "Iteration 8667, loss = 1.77495147\n",
            "Iteration 8668, loss = 1.77473292\n",
            "Iteration 8669, loss = 1.77451453\n",
            "Iteration 8670, loss = 1.77429629\n",
            "Iteration 8671, loss = 1.77407819\n",
            "Iteration 8672, loss = 1.77386025\n",
            "Iteration 8673, loss = 1.77364246\n",
            "Iteration 8674, loss = 1.77342482\n",
            "Iteration 8675, loss = 1.77320733\n",
            "Iteration 8676, loss = 1.77299000\n",
            "Iteration 8677, loss = 1.77277281\n",
            "Iteration 8678, loss = 1.77255577\n",
            "Iteration 8679, loss = 1.77233889\n",
            "Iteration 8680, loss = 1.77212215\n",
            "Iteration 8681, loss = 1.77190557\n",
            "Iteration 8682, loss = 1.77168913\n",
            "Iteration 8683, loss = 1.77147285\n",
            "Iteration 8684, loss = 1.77125671\n",
            "Iteration 8685, loss = 1.77104073\n",
            "Iteration 8686, loss = 1.77082489\n",
            "Iteration 8687, loss = 1.77060921\n",
            "Iteration 8688, loss = 1.77039367\n",
            "Iteration 8689, loss = 1.77017829\n",
            "Iteration 8690, loss = 1.76996305\n",
            "Iteration 8691, loss = 1.76974796\n",
            "Iteration 8692, loss = 1.76953303\n",
            "Iteration 8693, loss = 1.76931824\n",
            "Iteration 8694, loss = 1.76910360\n",
            "Iteration 8695, loss = 1.76888911\n",
            "Iteration 8696, loss = 1.76867477\n",
            "Iteration 8697, loss = 1.76846057\n",
            "Iteration 8698, loss = 1.76824653\n",
            "Iteration 8699, loss = 1.76803263\n",
            "Iteration 8700, loss = 1.76781888\n",
            "Iteration 8701, loss = 1.76760529\n",
            "Iteration 8702, loss = 1.76739183\n",
            "Iteration 8703, loss = 1.76717853\n",
            "Iteration 8704, loss = 1.76696538\n",
            "Iteration 8705, loss = 1.76675237\n",
            "Iteration 8706, loss = 1.76653951\n",
            "Iteration 8707, loss = 1.76632680\n",
            "Iteration 8708, loss = 1.76611424\n",
            "Iteration 8709, loss = 1.76590182\n",
            "Iteration 8710, loss = 1.76568955\n",
            "Iteration 8711, loss = 1.76547743\n",
            "Iteration 8712, loss = 1.76526546\n",
            "Iteration 8713, loss = 1.76505363\n",
            "Iteration 8714, loss = 1.76484195\n",
            "Iteration 8715, loss = 1.76463042\n",
            "Iteration 8716, loss = 1.76441903\n",
            "Iteration 8717, loss = 1.76420780\n",
            "Iteration 8718, loss = 1.76399670\n",
            "Iteration 8719, loss = 1.76378576\n",
            "Iteration 8720, loss = 1.76357496\n",
            "Iteration 8721, loss = 1.76336431\n",
            "Iteration 8722, loss = 1.76315380\n",
            "Iteration 8723, loss = 1.76294344\n",
            "Iteration 8724, loss = 1.76273323\n",
            "Iteration 8725, loss = 1.76252316\n",
            "Iteration 8726, loss = 1.76231324\n",
            "Iteration 8727, loss = 1.76210346\n",
            "Iteration 8728, loss = 1.76189383\n",
            "Iteration 8729, loss = 1.76168435\n",
            "Iteration 8730, loss = 1.76147501\n",
            "Iteration 8731, loss = 1.76126581\n",
            "Iteration 8732, loss = 1.76105677\n",
            "Iteration 8733, loss = 1.76084786\n",
            "Iteration 8734, loss = 1.76063911\n",
            "Iteration 8735, loss = 1.76043049\n",
            "Iteration 8736, loss = 1.76022202\n",
            "Iteration 8737, loss = 1.76001370\n",
            "Iteration 8738, loss = 1.75980552\n",
            "Iteration 8739, loss = 1.75959749\n",
            "Iteration 8740, loss = 1.75938960\n",
            "Iteration 8741, loss = 1.75918186\n",
            "Iteration 8742, loss = 1.75897426\n",
            "Iteration 8743, loss = 1.75876680\n",
            "Iteration 8744, loss = 1.75855949\n",
            "Iteration 8745, loss = 1.75835232\n",
            "Iteration 8746, loss = 1.75814530\n",
            "Iteration 8747, loss = 1.75793842\n",
            "Iteration 8748, loss = 1.75773168\n",
            "Iteration 8749, loss = 1.75752509\n",
            "Iteration 8750, loss = 1.75731864\n",
            "Iteration 8751, loss = 1.75711234\n",
            "Iteration 8752, loss = 1.75690618\n",
            "Iteration 8753, loss = 1.75670016\n",
            "Iteration 8754, loss = 1.75649429\n",
            "Iteration 8755, loss = 1.75628855\n",
            "Iteration 8756, loss = 1.75608297\n",
            "Iteration 8757, loss = 1.75587752\n",
            "Iteration 8758, loss = 1.75567222\n",
            "Iteration 8759, loss = 1.75546706\n",
            "Iteration 8760, loss = 1.75526204\n",
            "Iteration 8761, loss = 1.75505717\n",
            "Iteration 8762, loss = 1.75485243\n",
            "Iteration 8763, loss = 1.75464784\n",
            "Iteration 8764, loss = 1.75444340\n",
            "Iteration 8765, loss = 1.75423909\n",
            "Iteration 8766, loss = 1.75403493\n",
            "Iteration 8767, loss = 1.75383091\n",
            "Iteration 8768, loss = 1.75362703\n",
            "Iteration 8769, loss = 1.75342329\n",
            "Iteration 8770, loss = 1.75321970\n",
            "Iteration 8771, loss = 1.75301624\n",
            "Iteration 8772, loss = 1.75281293\n",
            "Iteration 8773, loss = 1.75260976\n",
            "Iteration 8774, loss = 1.75240673\n",
            "Iteration 8775, loss = 1.75220384\n",
            "Iteration 8776, loss = 1.75200109\n",
            "Iteration 8777, loss = 1.75179849\n",
            "Iteration 8778, loss = 1.75159602\n",
            "Iteration 8779, loss = 1.75139370\n",
            "Iteration 8780, loss = 1.75119151\n",
            "Iteration 8781, loss = 1.75098947\n",
            "Iteration 8782, loss = 1.75078757\n",
            "Iteration 8783, loss = 1.75058581\n",
            "Iteration 8784, loss = 1.75038419\n",
            "Iteration 8785, loss = 1.75018271\n",
            "Iteration 8786, loss = 1.74998137\n",
            "Iteration 8787, loss = 1.74978016\n",
            "Iteration 8788, loss = 1.74957910\n",
            "Iteration 8789, loss = 1.74937818\n",
            "Iteration 8790, loss = 1.74917740\n",
            "Iteration 8791, loss = 1.74897676\n",
            "Iteration 8792, loss = 1.74877626\n",
            "Iteration 8793, loss = 1.74857590\n",
            "Iteration 8794, loss = 1.74837568\n",
            "Iteration 8795, loss = 1.74817559\n",
            "Iteration 8796, loss = 1.74797565\n",
            "Iteration 8797, loss = 1.74777585\n",
            "Iteration 8798, loss = 1.74757618\n",
            "Iteration 8799, loss = 1.74737665\n",
            "Iteration 8800, loss = 1.74717727\n",
            "Iteration 8801, loss = 1.74697802\n",
            "Iteration 8802, loss = 1.74677891\n",
            "Iteration 8803, loss = 1.74657994\n",
            "Iteration 8804, loss = 1.74638111\n",
            "Iteration 8805, loss = 1.74618241\n",
            "Iteration 8806, loss = 1.74598386\n",
            "Iteration 8807, loss = 1.74578544\n",
            "Iteration 8808, loss = 1.74558716\n",
            "Iteration 8809, loss = 1.74538902\n",
            "Iteration 8810, loss = 1.74519101\n",
            "Iteration 8811, loss = 1.74499315\n",
            "Iteration 8812, loss = 1.74479542\n",
            "Iteration 8813, loss = 1.74459783\n",
            "Iteration 8814, loss = 1.74440038\n",
            "Iteration 8815, loss = 1.74420307\n",
            "Iteration 8816, loss = 1.74400589\n",
            "Iteration 8817, loss = 1.74380885\n",
            "Iteration 8818, loss = 1.74361195\n",
            "Iteration 8819, loss = 1.74341518\n",
            "Iteration 8820, loss = 1.74321855\n",
            "Iteration 8821, loss = 1.74302206\n",
            "Iteration 8822, loss = 1.74282571\n",
            "Iteration 8823, loss = 1.74262949\n",
            "Iteration 8824, loss = 1.74243341\n",
            "Iteration 8825, loss = 1.74223746\n",
            "Iteration 8826, loss = 1.74204165\n",
            "Iteration 8827, loss = 1.74184598\n",
            "Iteration 8828, loss = 1.74165045\n",
            "Iteration 8829, loss = 1.74145505\n",
            "Iteration 8830, loss = 1.74125979\n",
            "Iteration 8831, loss = 1.74106466\n",
            "Iteration 8832, loss = 1.74086967\n",
            "Iteration 8833, loss = 1.74067481\n",
            "Iteration 8834, loss = 1.74048009\n",
            "Iteration 8835, loss = 1.74028551\n",
            "Iteration 8836, loss = 1.74009106\n",
            "Iteration 8837, loss = 1.73989675\n",
            "Iteration 8838, loss = 1.73970257\n",
            "Iteration 8839, loss = 1.73950853\n",
            "Iteration 8840, loss = 1.73931463\n",
            "Iteration 8841, loss = 1.73912085\n",
            "Iteration 8842, loss = 1.73892722\n",
            "Iteration 8843, loss = 1.73873372\n",
            "Iteration 8844, loss = 1.73854035\n",
            "Iteration 8845, loss = 1.73834712\n",
            "Iteration 8846, loss = 1.73815402\n",
            "Iteration 8847, loss = 1.73796106\n",
            "Iteration 8848, loss = 1.73776823\n",
            "Iteration 8849, loss = 1.73757554\n",
            "Iteration 8850, loss = 1.73738298\n",
            "Iteration 8851, loss = 1.73719055\n",
            "Iteration 8852, loss = 1.73699826\n",
            "Iteration 8853, loss = 1.73680611\n",
            "Iteration 8854, loss = 1.73661408\n",
            "Iteration 8855, loss = 1.73642219\n",
            "Iteration 8856, loss = 1.73623044\n",
            "Iteration 8857, loss = 1.73603882\n",
            "Iteration 8858, loss = 1.73584733\n",
            "Iteration 8859, loss = 1.73565597\n",
            "Iteration 8860, loss = 1.73546475\n",
            "Iteration 8861, loss = 1.73527367\n",
            "Iteration 8862, loss = 1.73508271\n",
            "Iteration 8863, loss = 1.73489189\n",
            "Iteration 8864, loss = 1.73470120\n",
            "Iteration 8865, loss = 1.73451065\n",
            "Iteration 8866, loss = 1.73432022\n",
            "Iteration 8867, loss = 1.73412993\n",
            "Iteration 8868, loss = 1.73393978\n",
            "Iteration 8869, loss = 1.73374975\n",
            "Iteration 8870, loss = 1.73355986\n",
            "Iteration 8871, loss = 1.73337010\n",
            "Iteration 8872, loss = 1.73318047\n",
            "Iteration 8873, loss = 1.73299098\n",
            "Iteration 8874, loss = 1.73280162\n",
            "Iteration 8875, loss = 1.73261238\n",
            "Iteration 8876, loss = 1.73242329\n",
            "Iteration 8877, loss = 1.73223432\n",
            "Iteration 8878, loss = 1.73204548\n",
            "Iteration 8879, loss = 1.73185678\n",
            "Iteration 8880, loss = 1.73166821\n",
            "Iteration 8881, loss = 1.73147977\n",
            "Iteration 8882, loss = 1.73129146\n",
            "Iteration 8883, loss = 1.73110328\n",
            "Iteration 8884, loss = 1.73091523\n",
            "Iteration 8885, loss = 1.73072732\n",
            "Iteration 8886, loss = 1.73053954\n",
            "Iteration 8887, loss = 1.73035188\n",
            "Iteration 8888, loss = 1.73016436\n",
            "Iteration 8889, loss = 1.72997697\n",
            "Iteration 8890, loss = 1.72978971\n",
            "Iteration 8891, loss = 1.72960258\n",
            "Iteration 8892, loss = 1.72941558\n",
            "Iteration 8893, loss = 1.72922871\n",
            "Iteration 8894, loss = 1.72904197\n",
            "Iteration 8895, loss = 1.72885536\n",
            "Iteration 8896, loss = 1.72866888\n",
            "Iteration 8897, loss = 1.72848253\n",
            "Iteration 8898, loss = 1.72829632\n",
            "Iteration 8899, loss = 1.72811023\n",
            "Iteration 8900, loss = 1.72792427\n",
            "Iteration 8901, loss = 1.72773844\n",
            "Iteration 8902, loss = 1.72755274\n",
            "Iteration 8903, loss = 1.72736717\n",
            "Iteration 8904, loss = 1.72718173\n",
            "Iteration 8905, loss = 1.72699642\n",
            "Iteration 8906, loss = 1.72681124\n",
            "Iteration 8907, loss = 1.72662618\n",
            "Iteration 8908, loss = 1.72644126\n",
            "Iteration 8909, loss = 1.72625647\n",
            "Iteration 8910, loss = 1.72607180\n",
            "Iteration 8911, loss = 1.72588726\n",
            "Iteration 8912, loss = 1.72570286\n",
            "Iteration 8913, loss = 1.72551858\n",
            "Iteration 8914, loss = 1.72533443\n",
            "Iteration 8915, loss = 1.72515040\n",
            "Iteration 8916, loss = 1.72496651\n",
            "Iteration 8917, loss = 1.72478275\n",
            "Iteration 8918, loss = 1.72459911\n",
            "Iteration 8919, loss = 1.72441560\n",
            "Iteration 8920, loss = 1.72423222\n",
            "Iteration 8921, loss = 1.72404896\n",
            "Iteration 8922, loss = 1.72386584\n",
            "Iteration 8923, loss = 1.72368284\n",
            "Iteration 8924, loss = 1.72349997\n",
            "Iteration 8925, loss = 1.72331723\n",
            "Iteration 8926, loss = 1.72313461\n",
            "Iteration 8927, loss = 1.72295213\n",
            "Iteration 8928, loss = 1.72276977\n",
            "Iteration 8929, loss = 1.72258754\n",
            "Iteration 8930, loss = 1.72240543\n",
            "Iteration 8931, loss = 1.72222345\n",
            "Iteration 8932, loss = 1.72204160\n",
            "Iteration 8933, loss = 1.72185987\n",
            "Iteration 8934, loss = 1.72167828\n",
            "Iteration 8935, loss = 1.72149681\n",
            "Iteration 8936, loss = 1.72131546\n",
            "Iteration 8937, loss = 1.72113424\n",
            "Iteration 8938, loss = 1.72095315\n",
            "Iteration 8939, loss = 1.72077219\n",
            "Iteration 8940, loss = 1.72059135\n",
            "Iteration 8941, loss = 1.72041064\n",
            "Iteration 8942, loss = 1.72023005\n",
            "Iteration 8943, loss = 1.72004959\n",
            "Iteration 8944, loss = 1.71986925\n",
            "Iteration 8945, loss = 1.71968905\n",
            "Iteration 8946, loss = 1.71950896\n",
            "Iteration 8947, loss = 1.71932901\n",
            "Iteration 8948, loss = 1.71914917\n",
            "Iteration 8949, loss = 1.71896947\n",
            "Iteration 8950, loss = 1.71878989\n",
            "Iteration 8951, loss = 1.71861043\n",
            "Iteration 8952, loss = 1.71843110\n",
            "Iteration 8953, loss = 1.71825190\n",
            "Iteration 8954, loss = 1.71807282\n",
            "Iteration 8955, loss = 1.71789386\n",
            "Iteration 8956, loss = 1.71771503\n",
            "Iteration 8957, loss = 1.71753633\n",
            "Iteration 8958, loss = 1.71735775\n",
            "Iteration 8959, loss = 1.71717929\n",
            "Iteration 8960, loss = 1.71700096\n",
            "Iteration 8961, loss = 1.71682275\n",
            "Iteration 8962, loss = 1.71664467\n",
            "Iteration 8963, loss = 1.71646671\n",
            "Iteration 8964, loss = 1.71628888\n",
            "Iteration 8965, loss = 1.71611117\n",
            "Iteration 8966, loss = 1.71593358\n",
            "Iteration 8967, loss = 1.71575612\n",
            "Iteration 8968, loss = 1.71557878\n",
            "Iteration 8969, loss = 1.71540157\n",
            "Iteration 8970, loss = 1.71522448\n",
            "Iteration 8971, loss = 1.71504751\n",
            "Iteration 8972, loss = 1.71487066\n",
            "Iteration 8973, loss = 1.71469394\n",
            "Iteration 8974, loss = 1.71451735\n",
            "Iteration 8975, loss = 1.71434087\n",
            "Iteration 8976, loss = 1.71416452\n",
            "Iteration 8977, loss = 1.71398830\n",
            "Iteration 8978, loss = 1.71381219\n",
            "Iteration 8979, loss = 1.71363621\n",
            "Iteration 8980, loss = 1.71346035\n",
            "Iteration 8981, loss = 1.71328462\n",
            "Iteration 8982, loss = 1.71310900\n",
            "Iteration 8983, loss = 1.71293351\n",
            "Iteration 8984, loss = 1.71275814\n",
            "Iteration 8985, loss = 1.71258290\n",
            "Iteration 8986, loss = 1.71240777\n",
            "Iteration 8987, loss = 1.71223277\n",
            "Iteration 8988, loss = 1.71205789\n",
            "Iteration 8989, loss = 1.71188314\n",
            "Iteration 8990, loss = 1.71170850\n",
            "Iteration 8991, loss = 1.71153399\n",
            "Iteration 8992, loss = 1.71135960\n",
            "Iteration 8993, loss = 1.71118533\n",
            "Iteration 8994, loss = 1.71101118\n",
            "Iteration 8995, loss = 1.71083715\n",
            "Iteration 8996, loss = 1.71066325\n",
            "Iteration 8997, loss = 1.71048946\n",
            "Iteration 8998, loss = 1.71031580\n",
            "Iteration 8999, loss = 1.71014226\n",
            "Iteration 9000, loss = 1.70996884\n",
            "Iteration 9001, loss = 1.70979554\n",
            "Iteration 9002, loss = 1.70962236\n",
            "Iteration 9003, loss = 1.70944930\n",
            "Iteration 9004, loss = 1.70927637\n",
            "Iteration 9005, loss = 1.70910355\n",
            "Iteration 9006, loss = 1.70893085\n",
            "Iteration 9007, loss = 1.70875828\n",
            "Iteration 9008, loss = 1.70858582\n",
            "Iteration 9009, loss = 1.70841349\n",
            "Iteration 9010, loss = 1.70824128\n",
            "Iteration 9011, loss = 1.70806918\n",
            "Iteration 9012, loss = 1.70789721\n",
            "Iteration 9013, loss = 1.70772536\n",
            "Iteration 9014, loss = 1.70755362\n",
            "Iteration 9015, loss = 1.70738201\n",
            "Iteration 9016, loss = 1.70721051\n",
            "Iteration 9017, loss = 1.70703914\n",
            "Iteration 9018, loss = 1.70686788\n",
            "Iteration 9019, loss = 1.70669675\n",
            "Iteration 9020, loss = 1.70652573\n",
            "Iteration 9021, loss = 1.70635484\n",
            "Iteration 9022, loss = 1.70618406\n",
            "Iteration 9023, loss = 1.70601340\n",
            "Iteration 9024, loss = 1.70584286\n",
            "Iteration 9025, loss = 1.70567244\n",
            "Iteration 9026, loss = 1.70550214\n",
            "Iteration 9027, loss = 1.70533196\n",
            "Iteration 9028, loss = 1.70516189\n",
            "Iteration 9029, loss = 1.70499195\n",
            "Iteration 9030, loss = 1.70482212\n",
            "Iteration 9031, loss = 1.70465241\n",
            "Iteration 9032, loss = 1.70448282\n",
            "Iteration 9033, loss = 1.70431335\n",
            "Iteration 9034, loss = 1.70414400\n",
            "Iteration 9035, loss = 1.70397476\n",
            "Iteration 9036, loss = 1.70380565\n",
            "Iteration 9037, loss = 1.70363665\n",
            "Iteration 9038, loss = 1.70346777\n",
            "Iteration 9039, loss = 1.70329900\n",
            "Iteration 9040, loss = 1.70313036\n",
            "Iteration 9041, loss = 1.70296183\n",
            "Iteration 9042, loss = 1.70279342\n",
            "Iteration 9043, loss = 1.70262513\n",
            "Iteration 9044, loss = 1.70245695\n",
            "Iteration 9045, loss = 1.70228890\n",
            "Iteration 9046, loss = 1.70212095\n",
            "Iteration 9047, loss = 1.70195313\n",
            "Iteration 9048, loss = 1.70178542\n",
            "Iteration 9049, loss = 1.70161783\n",
            "Iteration 9050, loss = 1.70145036\n",
            "Iteration 9051, loss = 1.70128301\n",
            "Iteration 9052, loss = 1.70111577\n",
            "Iteration 9053, loss = 1.70094864\n",
            "Iteration 9054, loss = 1.70078164\n",
            "Iteration 9055, loss = 1.70061475\n",
            "Iteration 9056, loss = 1.70044798\n",
            "Iteration 9057, loss = 1.70028132\n",
            "Iteration 9058, loss = 1.70011478\n",
            "Iteration 9059, loss = 1.69994835\n",
            "Iteration 9060, loss = 1.69978205\n",
            "Iteration 9061, loss = 1.69961585\n",
            "Iteration 9062, loss = 1.69944978\n",
            "Iteration 9063, loss = 1.69928382\n",
            "Iteration 9064, loss = 1.69911797\n",
            "Iteration 9065, loss = 1.69895224\n",
            "Iteration 9066, loss = 1.69878663\n",
            "Iteration 9067, loss = 1.69862113\n",
            "Iteration 9068, loss = 1.69845575\n",
            "Iteration 9069, loss = 1.69829048\n",
            "Iteration 9070, loss = 1.69812533\n",
            "Iteration 9071, loss = 1.69796029\n",
            "Iteration 9072, loss = 1.69779537\n",
            "Iteration 9073, loss = 1.69763056\n",
            "Iteration 9074, loss = 1.69746587\n",
            "Iteration 9075, loss = 1.69730129\n",
            "Iteration 9076, loss = 1.69713683\n",
            "Iteration 9077, loss = 1.69697248\n",
            "Iteration 9078, loss = 1.69680825\n",
            "Iteration 9079, loss = 1.69664413\n",
            "Iteration 9080, loss = 1.69648013\n",
            "Iteration 9081, loss = 1.69631624\n",
            "Iteration 9082, loss = 1.69615246\n",
            "Iteration 9083, loss = 1.69598880\n",
            "Iteration 9084, loss = 1.69582525\n",
            "Iteration 9085, loss = 1.69566182\n",
            "Iteration 9086, loss = 1.69549850\n",
            "Iteration 9087, loss = 1.69533529\n",
            "Iteration 9088, loss = 1.69517220\n",
            "Iteration 9089, loss = 1.69500922\n",
            "Iteration 9090, loss = 1.69484636\n",
            "Iteration 9091, loss = 1.69468361\n",
            "Iteration 9092, loss = 1.69452097\n",
            "Iteration 9093, loss = 1.69435844\n",
            "Iteration 9094, loss = 1.69419603\n",
            "Iteration 9095, loss = 1.69403374\n",
            "Iteration 9096, loss = 1.69387155\n",
            "Iteration 9097, loss = 1.69370948\n",
            "Iteration 9098, loss = 1.69354752\n",
            "Iteration 9099, loss = 1.69338567\n",
            "Iteration 9100, loss = 1.69322394\n",
            "Iteration 9101, loss = 1.69306232\n",
            "Iteration 9102, loss = 1.69290081\n",
            "Iteration 9103, loss = 1.69273942\n",
            "Iteration 9104, loss = 1.69257813\n",
            "Iteration 9105, loss = 1.69241696\n",
            "Iteration 9106, loss = 1.69225590\n",
            "Iteration 9107, loss = 1.69209496\n",
            "Iteration 9108, loss = 1.69193412\n",
            "Iteration 9109, loss = 1.69177340\n",
            "Iteration 9110, loss = 1.69161279\n",
            "Iteration 9111, loss = 1.69145229\n",
            "Iteration 9112, loss = 1.69129191\n",
            "Iteration 9113, loss = 1.69113163\n",
            "Iteration 9114, loss = 1.69097147\n",
            "Iteration 9115, loss = 1.69081142\n",
            "Iteration 9116, loss = 1.69065148\n",
            "Iteration 9117, loss = 1.69049165\n",
            "Iteration 9118, loss = 1.69033193\n",
            "Iteration 9119, loss = 1.69017233\n",
            "Iteration 9120, loss = 1.69001283\n",
            "Iteration 9121, loss = 1.68985345\n",
            "Iteration 9122, loss = 1.68969418\n",
            "Iteration 9123, loss = 1.68953501\n",
            "Iteration 9124, loss = 1.68937596\n",
            "Iteration 9125, loss = 1.68921702\n",
            "Iteration 9126, loss = 1.68905819\n",
            "Iteration 9127, loss = 1.68889947\n",
            "Iteration 9128, loss = 1.68874087\n",
            "Iteration 9129, loss = 1.68858237\n",
            "Iteration 9130, loss = 1.68842398\n",
            "Iteration 9131, loss = 1.68826570\n",
            "Iteration 9132, loss = 1.68810754\n",
            "Iteration 9133, loss = 1.68794948\n",
            "Iteration 9134, loss = 1.68779153\n",
            "Iteration 9135, loss = 1.68763369\n",
            "Iteration 9136, loss = 1.68747597\n",
            "Iteration 9137, loss = 1.68731835\n",
            "Iteration 9138, loss = 1.68716084\n",
            "Iteration 9139, loss = 1.68700344\n",
            "Iteration 9140, loss = 1.68684616\n",
            "Iteration 9141, loss = 1.68668898\n",
            "Iteration 9142, loss = 1.68653191\n",
            "Iteration 9143, loss = 1.68637495\n",
            "Iteration 9144, loss = 1.68621810\n",
            "Iteration 9145, loss = 1.68606135\n",
            "Iteration 9146, loss = 1.68590472\n",
            "Iteration 9147, loss = 1.68574820\n",
            "Iteration 9148, loss = 1.68559178\n",
            "Iteration 9149, loss = 1.68543547\n",
            "Iteration 9150, loss = 1.68527928\n",
            "Iteration 9151, loss = 1.68512319\n",
            "Iteration 9152, loss = 1.68496721\n",
            "Iteration 9153, loss = 1.68481134\n",
            "Iteration 9154, loss = 1.68465557\n",
            "Iteration 9155, loss = 1.68449992\n",
            "Iteration 9156, loss = 1.68434437\n",
            "Iteration 9157, loss = 1.68418893\n",
            "Iteration 9158, loss = 1.68403360\n",
            "Iteration 9159, loss = 1.68387838\n",
            "Iteration 9160, loss = 1.68372327\n",
            "Iteration 9161, loss = 1.68356826\n",
            "Iteration 9162, loss = 1.68341336\n",
            "Iteration 9163, loss = 1.68325857\n",
            "Iteration 9164, loss = 1.68310389\n",
            "Iteration 9165, loss = 1.68294931\n",
            "Iteration 9166, loss = 1.68279484\n",
            "Iteration 9167, loss = 1.68264048\n",
            "Iteration 9168, loss = 1.68248623\n",
            "Iteration 9169, loss = 1.68233208\n",
            "Iteration 9170, loss = 1.68217804\n",
            "Iteration 9171, loss = 1.68202411\n",
            "Iteration 9172, loss = 1.68187028\n",
            "Iteration 9173, loss = 1.68171656\n",
            "Iteration 9174, loss = 1.68156295\n",
            "Iteration 9175, loss = 1.68140945\n",
            "Iteration 9176, loss = 1.68125605\n",
            "Iteration 9177, loss = 1.68110276\n",
            "Iteration 9178, loss = 1.68094957\n",
            "Iteration 9179, loss = 1.68079650\n",
            "Iteration 9180, loss = 1.68064352\n",
            "Iteration 9181, loss = 1.68049066\n",
            "Iteration 9182, loss = 1.68033790\n",
            "Iteration 9183, loss = 1.68018524\n",
            "Iteration 9184, loss = 1.68003270\n",
            "Iteration 9185, loss = 1.67988026\n",
            "Iteration 9186, loss = 1.67972792\n",
            "Iteration 9187, loss = 1.67957569\n",
            "Iteration 9188, loss = 1.67942357\n",
            "Iteration 9189, loss = 1.67927155\n",
            "Iteration 9190, loss = 1.67911964\n",
            "Iteration 9191, loss = 1.67896783\n",
            "Iteration 9192, loss = 1.67881613\n",
            "Iteration 9193, loss = 1.67866453\n",
            "Iteration 9194, loss = 1.67851304\n",
            "Iteration 9195, loss = 1.67836165\n",
            "Iteration 9196, loss = 1.67821037\n",
            "Iteration 9197, loss = 1.67805920\n",
            "Iteration 9198, loss = 1.67790813\n",
            "Iteration 9199, loss = 1.67775716\n",
            "Iteration 9200, loss = 1.67760630\n",
            "Iteration 9201, loss = 1.67745554\n",
            "Iteration 9202, loss = 1.67730489\n",
            "Iteration 9203, loss = 1.67715434\n",
            "Iteration 9204, loss = 1.67700390\n",
            "Iteration 9205, loss = 1.67685356\n",
            "Iteration 9206, loss = 1.67670333\n",
            "Iteration 9207, loss = 1.67655320\n",
            "Iteration 9208, loss = 1.67640317\n",
            "Iteration 9209, loss = 1.67625325\n",
            "Iteration 9210, loss = 1.67610343\n",
            "Iteration 9211, loss = 1.67595372\n",
            "Iteration 9212, loss = 1.67580411\n",
            "Iteration 9213, loss = 1.67565460\n",
            "Iteration 9214, loss = 1.67550520\n",
            "Iteration 9215, loss = 1.67535590\n",
            "Iteration 9216, loss = 1.67520671\n",
            "Iteration 9217, loss = 1.67505762\n",
            "Iteration 9218, loss = 1.67490863\n",
            "Iteration 9219, loss = 1.67475974\n",
            "Iteration 9220, loss = 1.67461096\n",
            "Iteration 9221, loss = 1.67446228\n",
            "Iteration 9222, loss = 1.67431371\n",
            "Iteration 9223, loss = 1.67416524\n",
            "Iteration 9224, loss = 1.67401687\n",
            "Iteration 9225, loss = 1.67386860\n",
            "Iteration 9226, loss = 1.67372044\n",
            "Iteration 9227, loss = 1.67357238\n",
            "Iteration 9228, loss = 1.67342442\n",
            "Iteration 9229, loss = 1.67327656\n",
            "Iteration 9230, loss = 1.67312881\n",
            "Iteration 9231, loss = 1.67298116\n",
            "Iteration 9232, loss = 1.67283361\n",
            "Iteration 9233, loss = 1.67268616\n",
            "Iteration 9234, loss = 1.67253882\n",
            "Iteration 9235, loss = 1.67239158\n",
            "Iteration 9236, loss = 1.67224444\n",
            "Iteration 9237, loss = 1.67209740\n",
            "Iteration 9238, loss = 1.67195046\n",
            "Iteration 9239, loss = 1.67180363\n",
            "Iteration 9240, loss = 1.67165689\n",
            "Iteration 9241, loss = 1.67151026\n",
            "Iteration 9242, loss = 1.67136373\n",
            "Iteration 9243, loss = 1.67121730\n",
            "Iteration 9244, loss = 1.67107098\n",
            "Iteration 9245, loss = 1.67092475\n",
            "Iteration 9246, loss = 1.67077863\n",
            "Iteration 9247, loss = 1.67063261\n",
            "Iteration 9248, loss = 1.67048668\n",
            "Iteration 9249, loss = 1.67034086\n",
            "Iteration 9250, loss = 1.67019514\n",
            "Iteration 9251, loss = 1.67004952\n",
            "Iteration 9252, loss = 1.66990400\n",
            "Iteration 9253, loss = 1.66975859\n",
            "Iteration 9254, loss = 1.66961327\n",
            "Iteration 9255, loss = 1.66946805\n",
            "Iteration 9256, loss = 1.66932294\n",
            "Iteration 9257, loss = 1.66917792\n",
            "Iteration 9258, loss = 1.66903301\n",
            "Iteration 9259, loss = 1.66888819\n",
            "Iteration 9260, loss = 1.66874348\n",
            "Iteration 9261, loss = 1.66859886\n",
            "Iteration 9262, loss = 1.66845435\n",
            "Iteration 9263, loss = 1.66830993\n",
            "Iteration 9264, loss = 1.66816562\n",
            "Iteration 9265, loss = 1.66802140\n",
            "Iteration 9266, loss = 1.66787729\n",
            "Iteration 9267, loss = 1.66773327\n",
            "Iteration 9268, loss = 1.66758935\n",
            "Iteration 9269, loss = 1.66744554\n",
            "Iteration 9270, loss = 1.66730182\n",
            "Iteration 9271, loss = 1.66715820\n",
            "Iteration 9272, loss = 1.66701468\n",
            "Iteration 9273, loss = 1.66687126\n",
            "Iteration 9274, loss = 1.66672794\n",
            "Iteration 9275, loss = 1.66658472\n",
            "Iteration 9276, loss = 1.66644160\n",
            "Iteration 9277, loss = 1.66629858\n",
            "Iteration 9278, loss = 1.66615565\n",
            "Iteration 9279, loss = 1.66601283\n",
            "Iteration 9280, loss = 1.66587010\n",
            "Iteration 9281, loss = 1.66572747\n",
            "Iteration 9282, loss = 1.66558494\n",
            "Iteration 9283, loss = 1.66544251\n",
            "Iteration 9284, loss = 1.66530017\n",
            "Iteration 9285, loss = 1.66515794\n",
            "Iteration 9286, loss = 1.66501580\n",
            "Iteration 9287, loss = 1.66487376\n",
            "Iteration 9288, loss = 1.66473182\n",
            "Iteration 9289, loss = 1.66458998\n",
            "Iteration 9290, loss = 1.66444823\n",
            "Iteration 9291, loss = 1.66430659\n",
            "Iteration 9292, loss = 1.66416504\n",
            "Iteration 9293, loss = 1.66402359\n",
            "Iteration 9294, loss = 1.66388223\n",
            "Iteration 9295, loss = 1.66374098\n",
            "Iteration 9296, loss = 1.66359982\n",
            "Iteration 9297, loss = 1.66345876\n",
            "Iteration 9298, loss = 1.66331779\n",
            "Iteration 9299, loss = 1.66317692\n",
            "Iteration 9300, loss = 1.66303615\n",
            "Iteration 9301, loss = 1.66289548\n",
            "Iteration 9302, loss = 1.66275490\n",
            "Iteration 9303, loss = 1.66261443\n",
            "Iteration 9304, loss = 1.66247404\n",
            "Iteration 9305, loss = 1.66233376\n",
            "Iteration 9306, loss = 1.66219357\n",
            "Iteration 9307, loss = 1.66205348\n",
            "Iteration 9308, loss = 1.66191348\n",
            "Iteration 9309, loss = 1.66177358\n",
            "Iteration 9310, loss = 1.66163378\n",
            "Iteration 9311, loss = 1.66149407\n",
            "Iteration 9312, loss = 1.66135446\n",
            "Iteration 9313, loss = 1.66121495\n",
            "Iteration 9314, loss = 1.66107553\n",
            "Iteration 9315, loss = 1.66093621\n",
            "Iteration 9316, loss = 1.66079699\n",
            "Iteration 9317, loss = 1.66065786\n",
            "Iteration 9318, loss = 1.66051882\n",
            "Iteration 9319, loss = 1.66037988\n",
            "Iteration 9320, loss = 1.66024104\n",
            "Iteration 9321, loss = 1.66010229\n",
            "Iteration 9322, loss = 1.65996364\n",
            "Iteration 9323, loss = 1.65982509\n",
            "Iteration 9324, loss = 1.65968663\n",
            "Iteration 9325, loss = 1.65954826\n",
            "Iteration 9326, loss = 1.65940999\n",
            "Iteration 9327, loss = 1.65927181\n",
            "Iteration 9328, loss = 1.65913373\n",
            "Iteration 9329, loss = 1.65899575\n",
            "Iteration 9330, loss = 1.65885786\n",
            "Iteration 9331, loss = 1.65872006\n",
            "Iteration 9332, loss = 1.65858236\n",
            "Iteration 9333, loss = 1.65844476\n",
            "Iteration 9334, loss = 1.65830725\n",
            "Iteration 9335, loss = 1.65816983\n",
            "Iteration 9336, loss = 1.65803251\n",
            "Iteration 9337, loss = 1.65789528\n",
            "Iteration 9338, loss = 1.65775815\n",
            "Iteration 9339, loss = 1.65762111\n",
            "Iteration 9340, loss = 1.65748416\n",
            "Iteration 9341, loss = 1.65734731\n",
            "Iteration 9342, loss = 1.65721056\n",
            "Iteration 9343, loss = 1.65707389\n",
            "Iteration 9344, loss = 1.65693732\n",
            "Iteration 9345, loss = 1.65680085\n",
            "Iteration 9346, loss = 1.65666447\n",
            "Iteration 9347, loss = 1.65652818\n",
            "Iteration 9348, loss = 1.65639199\n",
            "Iteration 9349, loss = 1.65625588\n",
            "Iteration 9350, loss = 1.65611988\n",
            "Iteration 9351, loss = 1.65598396\n",
            "Iteration 9352, loss = 1.65584814\n",
            "Iteration 9353, loss = 1.65571242\n",
            "Iteration 9354, loss = 1.65557678\n",
            "Iteration 9355, loss = 1.65544124\n",
            "Iteration 9356, loss = 1.65530579\n",
            "Iteration 9357, loss = 1.65517044\n",
            "Iteration 9358, loss = 1.65503518\n",
            "Iteration 9359, loss = 1.65490001\n",
            "Iteration 9360, loss = 1.65476493\n",
            "Iteration 9361, loss = 1.65462995\n",
            "Iteration 9362, loss = 1.65449505\n",
            "Iteration 9363, loss = 1.65436026\n",
            "Iteration 9364, loss = 1.65422555\n",
            "Iteration 9365, loss = 1.65409093\n",
            "Iteration 9366, loss = 1.65395641\n",
            "Iteration 9367, loss = 1.65382198\n",
            "Iteration 9368, loss = 1.65368765\n",
            "Iteration 9369, loss = 1.65355340\n",
            "Iteration 9370, loss = 1.65341925\n",
            "Iteration 9371, loss = 1.65328518\n",
            "Iteration 9372, loss = 1.65315121\n",
            "Iteration 9373, loss = 1.65301734\n",
            "Iteration 9374, loss = 1.65288355\n",
            "Iteration 9375, loss = 1.65274986\n",
            "Iteration 9376, loss = 1.65261625\n",
            "Iteration 9377, loss = 1.65248274\n",
            "Iteration 9378, loss = 1.65234932\n",
            "Iteration 9379, loss = 1.65221599\n",
            "Iteration 9380, loss = 1.65208275\n",
            "Iteration 9381, loss = 1.65194961\n",
            "Iteration 9382, loss = 1.65181655\n",
            "Iteration 9383, loss = 1.65168359\n",
            "Iteration 9384, loss = 1.65155071\n",
            "Iteration 9385, loss = 1.65141793\n",
            "Iteration 9386, loss = 1.65128524\n",
            "Iteration 9387, loss = 1.65115264\n",
            "Iteration 9388, loss = 1.65102013\n",
            "Iteration 9389, loss = 1.65088771\n",
            "Iteration 9390, loss = 1.65075538\n",
            "Iteration 9391, loss = 1.65062314\n",
            "Iteration 9392, loss = 1.65049099\n",
            "Iteration 9393, loss = 1.65035894\n",
            "Iteration 9394, loss = 1.65022697\n",
            "Iteration 9395, loss = 1.65009509\n",
            "Iteration 9396, loss = 1.64996330\n",
            "Iteration 9397, loss = 1.64983161\n",
            "Iteration 9398, loss = 1.64970000\n",
            "Iteration 9399, loss = 1.64956848\n",
            "Iteration 9400, loss = 1.64943705\n",
            "Iteration 9401, loss = 1.64930572\n",
            "Iteration 9402, loss = 1.64917447\n",
            "Iteration 9403, loss = 1.64904331\n",
            "Iteration 9404, loss = 1.64891224\n",
            "Iteration 9405, loss = 1.64878126\n",
            "Iteration 9406, loss = 1.64865037\n",
            "Iteration 9407, loss = 1.64851957\n",
            "Iteration 9408, loss = 1.64838886\n",
            "Iteration 9409, loss = 1.64825823\n",
            "Iteration 9410, loss = 1.64812770\n",
            "Iteration 9411, loss = 1.64799726\n",
            "Iteration 9412, loss = 1.64786690\n",
            "Iteration 9413, loss = 1.64773663\n",
            "Iteration 9414, loss = 1.64760646\n",
            "Iteration 9415, loss = 1.64747637\n",
            "Iteration 9416, loss = 1.64734637\n",
            "Iteration 9417, loss = 1.64721646\n",
            "Iteration 9418, loss = 1.64708663\n",
            "Iteration 9419, loss = 1.64695690\n",
            "Iteration 9420, loss = 1.64682725\n",
            "Iteration 9421, loss = 1.64669769\n",
            "Iteration 9422, loss = 1.64656822\n",
            "Iteration 9423, loss = 1.64643884\n",
            "Iteration 9424, loss = 1.64630955\n",
            "Iteration 9425, loss = 1.64618034\n",
            "Iteration 9426, loss = 1.64605122\n",
            "Iteration 9427, loss = 1.64592219\n",
            "Iteration 9428, loss = 1.64579325\n",
            "Iteration 9429, loss = 1.64566440\n",
            "Iteration 9430, loss = 1.64553563\n",
            "Iteration 9431, loss = 1.64540695\n",
            "Iteration 9432, loss = 1.64527836\n",
            "Iteration 9433, loss = 1.64514986\n",
            "Iteration 9434, loss = 1.64502144\n",
            "Iteration 9435, loss = 1.64489311\n",
            "Iteration 9436, loss = 1.64476487\n",
            "Iteration 9437, loss = 1.64463672\n",
            "Iteration 9438, loss = 1.64450865\n",
            "Iteration 9439, loss = 1.64438067\n",
            "Iteration 9440, loss = 1.64425277\n",
            "Iteration 9441, loss = 1.64412497\n",
            "Iteration 9442, loss = 1.64399725\n",
            "Iteration 9443, loss = 1.64386961\n",
            "Iteration 9444, loss = 1.64374207\n",
            "Iteration 9445, loss = 1.64361461\n",
            "Iteration 9446, loss = 1.64348724\n",
            "Iteration 9447, loss = 1.64335995\n",
            "Iteration 9448, loss = 1.64323275\n",
            "Iteration 9449, loss = 1.64310563\n",
            "Iteration 9450, loss = 1.64297861\n",
            "Iteration 9451, loss = 1.64285166\n",
            "Iteration 9452, loss = 1.64272481\n",
            "Iteration 9453, loss = 1.64259804\n",
            "Iteration 9454, loss = 1.64247136\n",
            "Iteration 9455, loss = 1.64234476\n",
            "Iteration 9456, loss = 1.64221825\n",
            "Iteration 9457, loss = 1.64209182\n",
            "Iteration 9458, loss = 1.64196548\n",
            "Iteration 9459, loss = 1.64183923\n",
            "Iteration 9460, loss = 1.64171306\n",
            "Iteration 9461, loss = 1.64158697\n",
            "Iteration 9462, loss = 1.64146097\n",
            "Iteration 9463, loss = 1.64133506\n",
            "Iteration 9464, loss = 1.64120923\n",
            "Iteration 9465, loss = 1.64108349\n",
            "Iteration 9466, loss = 1.64095783\n",
            "Iteration 9467, loss = 1.64083226\n",
            "Iteration 9468, loss = 1.64070678\n",
            "Iteration 9469, loss = 1.64058137\n",
            "Iteration 9470, loss = 1.64045606\n",
            "Iteration 9471, loss = 1.64033082\n",
            "Iteration 9472, loss = 1.64020568\n",
            "Iteration 9473, loss = 1.64008061\n",
            "Iteration 9474, loss = 1.63995563\n",
            "Iteration 9475, loss = 1.63983074\n",
            "Iteration 9476, loss = 1.63970593\n",
            "Iteration 9477, loss = 1.63958120\n",
            "Iteration 9478, loss = 1.63945656\n",
            "Iteration 9479, loss = 1.63933201\n",
            "Iteration 9480, loss = 1.63920753\n",
            "Iteration 9481, loss = 1.63908315\n",
            "Iteration 9482, loss = 1.63895884\n",
            "Iteration 9483, loss = 1.63883462\n",
            "Iteration 9484, loss = 1.63871048\n",
            "Iteration 9485, loss = 1.63858643\n",
            "Iteration 9486, loss = 1.63846246\n",
            "Iteration 9487, loss = 1.63833858\n",
            "Iteration 9488, loss = 1.63821477\n",
            "Iteration 9489, loss = 1.63809106\n",
            "Iteration 9490, loss = 1.63796742\n",
            "Iteration 9491, loss = 1.63784387\n",
            "Iteration 9492, loss = 1.63772040\n",
            "Iteration 9493, loss = 1.63759702\n",
            "Iteration 9494, loss = 1.63747371\n",
            "Iteration 9495, loss = 1.63735050\n",
            "Iteration 9496, loss = 1.63722736\n",
            "Iteration 9497, loss = 1.63710431\n",
            "Iteration 9498, loss = 1.63698134\n",
            "Iteration 9499, loss = 1.63685845\n",
            "Iteration 9500, loss = 1.63673565\n",
            "Iteration 9501, loss = 1.63661293\n",
            "Iteration 9502, loss = 1.63649029\n",
            "Iteration 9503, loss = 1.63636773\n",
            "Iteration 9504, loss = 1.63624526\n",
            "Iteration 9505, loss = 1.63612287\n",
            "Iteration 9506, loss = 1.63600056\n",
            "Iteration 9507, loss = 1.63587833\n",
            "Iteration 9508, loss = 1.63575619\n",
            "Iteration 9509, loss = 1.63563412\n",
            "Iteration 9510, loss = 1.63551214\n",
            "Iteration 9511, loss = 1.63539025\n",
            "Iteration 9512, loss = 1.63526843\n",
            "Iteration 9513, loss = 1.63514670\n",
            "Iteration 9514, loss = 1.63502504\n",
            "Iteration 9515, loss = 1.63490347\n",
            "Iteration 9516, loss = 1.63478198\n",
            "Iteration 9517, loss = 1.63466058\n",
            "Iteration 9518, loss = 1.63453925\n",
            "Iteration 9519, loss = 1.63441801\n",
            "Iteration 9520, loss = 1.63429684\n",
            "Iteration 9521, loss = 1.63417576\n",
            "Iteration 9522, loss = 1.63405476\n",
            "Iteration 9523, loss = 1.63393384\n",
            "Iteration 9524, loss = 1.63381300\n",
            "Iteration 9525, loss = 1.63369225\n",
            "Iteration 9526, loss = 1.63357157\n",
            "Iteration 9527, loss = 1.63345097\n",
            "Iteration 9528, loss = 1.63333046\n",
            "Iteration 9529, loss = 1.63321003\n",
            "Iteration 9530, loss = 1.63308967\n",
            "Iteration 9531, loss = 1.63296940\n",
            "Iteration 9532, loss = 1.63284921\n",
            "Iteration 9533, loss = 1.63272910\n",
            "Iteration 9534, loss = 1.63260907\n",
            "Iteration 9535, loss = 1.63248912\n",
            "Iteration 9536, loss = 1.63236925\n",
            "Iteration 9537, loss = 1.63224946\n",
            "Iteration 9538, loss = 1.63212975\n",
            "Iteration 9539, loss = 1.63201012\n",
            "Iteration 9540, loss = 1.63189057\n",
            "Iteration 9541, loss = 1.63177110\n",
            "Iteration 9542, loss = 1.63165171\n",
            "Iteration 9543, loss = 1.63153240\n",
            "Iteration 9544, loss = 1.63141317\n",
            "Iteration 9545, loss = 1.63129402\n",
            "Iteration 9546, loss = 1.63117495\n",
            "Iteration 9547, loss = 1.63105596\n",
            "Iteration 9548, loss = 1.63093705\n",
            "Iteration 9549, loss = 1.63081822\n",
            "Iteration 9550, loss = 1.63069946\n",
            "Iteration 9551, loss = 1.63058079\n",
            "Iteration 9552, loss = 1.63046220\n",
            "Iteration 9553, loss = 1.63034368\n",
            "Iteration 9554, loss = 1.63022524\n",
            "Iteration 9555, loss = 1.63010689\n",
            "Iteration 9556, loss = 1.62998861\n",
            "Iteration 9557, loss = 1.62987041\n",
            "Iteration 9558, loss = 1.62975229\n",
            "Iteration 9559, loss = 1.62963425\n",
            "Iteration 9560, loss = 1.62951628\n",
            "Iteration 9561, loss = 1.62939840\n",
            "Iteration 9562, loss = 1.62928059\n",
            "Iteration 9563, loss = 1.62916287\n",
            "Iteration 9564, loss = 1.62904522\n",
            "Iteration 9565, loss = 1.62892765\n",
            "Iteration 9566, loss = 1.62881016\n",
            "Iteration 9567, loss = 1.62869274\n",
            "Iteration 9568, loss = 1.62857541\n",
            "Iteration 9569, loss = 1.62845815\n",
            "Iteration 9570, loss = 1.62834097\n",
            "Iteration 9571, loss = 1.62822387\n",
            "Iteration 9572, loss = 1.62810684\n",
            "Iteration 9573, loss = 1.62798990\n",
            "Iteration 9574, loss = 1.62787303\n",
            "Iteration 9575, loss = 1.62775624\n",
            "Iteration 9576, loss = 1.62763952\n",
            "Iteration 9577, loss = 1.62752289\n",
            "Iteration 9578, loss = 1.62740633\n",
            "Iteration 9579, loss = 1.62728985\n",
            "Iteration 9580, loss = 1.62717344\n",
            "Iteration 9581, loss = 1.62705712\n",
            "Iteration 9582, loss = 1.62694087\n",
            "Iteration 9583, loss = 1.62682470\n",
            "Iteration 9584, loss = 1.62670860\n",
            "Iteration 9585, loss = 1.62659259\n",
            "Iteration 9586, loss = 1.62647664\n",
            "Iteration 9587, loss = 1.62636078\n",
            "Iteration 9588, loss = 1.62624499\n",
            "Iteration 9589, loss = 1.62612928\n",
            "Iteration 9590, loss = 1.62601365\n",
            "Iteration 9591, loss = 1.62589809\n",
            "Iteration 9592, loss = 1.62578261\n",
            "Iteration 9593, loss = 1.62566721\n",
            "Iteration 9594, loss = 1.62555188\n",
            "Iteration 9595, loss = 1.62543663\n",
            "Iteration 9596, loss = 1.62532145\n",
            "Iteration 9597, loss = 1.62520635\n",
            "Iteration 9598, loss = 1.62509133\n",
            "Iteration 9599, loss = 1.62497638\n",
            "Iteration 9600, loss = 1.62486151\n",
            "Iteration 9601, loss = 1.62474672\n",
            "Iteration 9602, loss = 1.62463200\n",
            "Iteration 9603, loss = 1.62451736\n",
            "Iteration 9604, loss = 1.62440279\n",
            "Iteration 9605, loss = 1.62428830\n",
            "Iteration 9606, loss = 1.62417388\n",
            "Iteration 9607, loss = 1.62405954\n",
            "Iteration 9608, loss = 1.62394528\n",
            "Iteration 9609, loss = 1.62383109\n",
            "Iteration 9610, loss = 1.62371697\n",
            "Iteration 9611, loss = 1.62360293\n",
            "Iteration 9612, loss = 1.62348897\n",
            "Iteration 9613, loss = 1.62337508\n",
            "Iteration 9614, loss = 1.62326127\n",
            "Iteration 9615, loss = 1.62314753\n",
            "Iteration 9616, loss = 1.62303386\n",
            "Iteration 9617, loss = 1.62292028\n",
            "Iteration 9618, loss = 1.62280676\n",
            "Iteration 9619, loss = 1.62269332\n",
            "Iteration 9620, loss = 1.62257996\n",
            "Iteration 9621, loss = 1.62246667\n",
            "Iteration 9622, loss = 1.62235345\n",
            "Iteration 9623, loss = 1.62224031\n",
            "Iteration 9624, loss = 1.62212725\n",
            "Iteration 9625, loss = 1.62201425\n",
            "Iteration 9626, loss = 1.62190134\n",
            "Iteration 9627, loss = 1.62178849\n",
            "Iteration 9628, loss = 1.62167572\n",
            "Iteration 9629, loss = 1.62156303\n",
            "Iteration 9630, loss = 1.62145041\n",
            "Iteration 9631, loss = 1.62133786\n",
            "Iteration 9632, loss = 1.62122539\n",
            "Iteration 9633, loss = 1.62111299\n",
            "Iteration 9634, loss = 1.62100066\n",
            "Iteration 9635, loss = 1.62088841\n",
            "Iteration 9636, loss = 1.62077623\n",
            "Iteration 9637, loss = 1.62066412\n",
            "Iteration 9638, loss = 1.62055209\n",
            "Iteration 9639, loss = 1.62044014\n",
            "Iteration 9640, loss = 1.62032825\n",
            "Iteration 9641, loss = 1.62021644\n",
            "Iteration 9642, loss = 1.62010470\n",
            "Iteration 9643, loss = 1.61999304\n",
            "Iteration 9644, loss = 1.61988144\n",
            "Iteration 9645, loss = 1.61976993\n",
            "Iteration 9646, loss = 1.61965848\n",
            "Iteration 9647, loss = 1.61954711\n",
            "Iteration 9648, loss = 1.61943581\n",
            "Iteration 9649, loss = 1.61932458\n",
            "Iteration 9650, loss = 1.61921343\n",
            "Iteration 9651, loss = 1.61910234\n",
            "Iteration 9652, loss = 1.61899133\n",
            "Iteration 9653, loss = 1.61888040\n",
            "Iteration 9654, loss = 1.61876953\n",
            "Iteration 9655, loss = 1.61865874\n",
            "Iteration 9656, loss = 1.61854802\n",
            "Iteration 9657, loss = 1.61843738\n",
            "Iteration 9658, loss = 1.61832680\n",
            "Iteration 9659, loss = 1.61821630\n",
            "Iteration 9660, loss = 1.61810587\n",
            "Iteration 9661, loss = 1.61799551\n",
            "Iteration 9662, loss = 1.61788522\n",
            "Iteration 9663, loss = 1.61777501\n",
            "Iteration 9664, loss = 1.61766486\n",
            "Iteration 9665, loss = 1.61755479\n",
            "Iteration 9666, loss = 1.61744479\n",
            "Iteration 9667, loss = 1.61733486\n",
            "Iteration 9668, loss = 1.61722501\n",
            "Iteration 9669, loss = 1.61711522\n",
            "Iteration 9670, loss = 1.61700551\n",
            "Iteration 9671, loss = 1.61689587\n",
            "Iteration 9672, loss = 1.61678629\n",
            "Iteration 9673, loss = 1.61667679\n",
            "Iteration 9674, loss = 1.61656737\n",
            "Iteration 9675, loss = 1.61645801\n",
            "Iteration 9676, loss = 1.61634872\n",
            "Iteration 9677, loss = 1.61623951\n",
            "Iteration 9678, loss = 1.61613036\n",
            "Iteration 9679, loss = 1.61602129\n",
            "Iteration 9680, loss = 1.61591228\n",
            "Iteration 9681, loss = 1.61580335\n",
            "Iteration 9682, loss = 1.61569449\n",
            "Iteration 9683, loss = 1.61558570\n",
            "Iteration 9684, loss = 1.61547698\n",
            "Iteration 9685, loss = 1.61536833\n",
            "Iteration 9686, loss = 1.61525975\n",
            "Iteration 9687, loss = 1.61515124\n",
            "Iteration 9688, loss = 1.61504280\n",
            "Iteration 9689, loss = 1.61493443\n",
            "Iteration 9690, loss = 1.61482613\n",
            "Iteration 9691, loss = 1.61471791\n",
            "Iteration 9692, loss = 1.61460975\n",
            "Iteration 9693, loss = 1.61450166\n",
            "Iteration 9694, loss = 1.61439364\n",
            "Iteration 9695, loss = 1.61428569\n",
            "Iteration 9696, loss = 1.61417781\n",
            "Iteration 9697, loss = 1.61407000\n",
            "Iteration 9698, loss = 1.61396226\n",
            "Iteration 9699, loss = 1.61385459\n",
            "Iteration 9700, loss = 1.61374699\n",
            "Iteration 9701, loss = 1.61363946\n",
            "Iteration 9702, loss = 1.61353200\n",
            "Iteration 9703, loss = 1.61342460\n",
            "Iteration 9704, loss = 1.61331728\n",
            "Iteration 9705, loss = 1.61321003\n",
            "Iteration 9706, loss = 1.61310284\n",
            "Iteration 9707, loss = 1.61299572\n",
            "Iteration 9708, loss = 1.61288868\n",
            "Iteration 9709, loss = 1.61278170\n",
            "Iteration 9710, loss = 1.61267479\n",
            "Iteration 9711, loss = 1.61256795\n",
            "Iteration 9712, loss = 1.61246118\n",
            "Iteration 9713, loss = 1.61235447\n",
            "Iteration 9714, loss = 1.61224784\n",
            "Iteration 9715, loss = 1.61214127\n",
            "Iteration 9716, loss = 1.61203478\n",
            "Iteration 9717, loss = 1.61192835\n",
            "Iteration 9718, loss = 1.61182199\n",
            "Iteration 9719, loss = 1.61171569\n",
            "Iteration 9720, loss = 1.61160947\n",
            "Iteration 9721, loss = 1.61150331\n",
            "Iteration 9722, loss = 1.61139723\n",
            "Iteration 9723, loss = 1.61129121\n",
            "Iteration 9724, loss = 1.61118525\n",
            "Iteration 9725, loss = 1.61107937\n",
            "Iteration 9726, loss = 1.61097355\n",
            "Iteration 9727, loss = 1.61086781\n",
            "Iteration 9728, loss = 1.61076212\n",
            "Iteration 9729, loss = 1.61065651\n",
            "Iteration 9730, loss = 1.61055097\n",
            "Iteration 9731, loss = 1.61044549\n",
            "Iteration 9732, loss = 1.61034008\n",
            "Iteration 9733, loss = 1.61023473\n",
            "Iteration 9734, loss = 1.61012946\n",
            "Iteration 9735, loss = 1.61002425\n",
            "Iteration 9736, loss = 1.60991911\n",
            "Iteration 9737, loss = 1.60981404\n",
            "Iteration 9738, loss = 1.60970903\n",
            "Iteration 9739, loss = 1.60960409\n",
            "Iteration 9740, loss = 1.60949922\n",
            "Iteration 9741, loss = 1.60939441\n",
            "Iteration 9742, loss = 1.60928967\n",
            "Iteration 9743, loss = 1.60918500\n",
            "Iteration 9744, loss = 1.60908039\n",
            "Iteration 9745, loss = 1.60897585\n",
            "Iteration 9746, loss = 1.60887138\n",
            "Iteration 9747, loss = 1.60876698\n",
            "Iteration 9748, loss = 1.60866264\n",
            "Iteration 9749, loss = 1.60855836\n",
            "Iteration 9750, loss = 1.60845416\n",
            "Iteration 9751, loss = 1.60835002\n",
            "Iteration 9752, loss = 1.60824594\n",
            "Iteration 9753, loss = 1.60814193\n",
            "Iteration 9754, loss = 1.60803799\n",
            "Iteration 9755, loss = 1.60793412\n",
            "Iteration 9756, loss = 1.60783031\n",
            "Iteration 9757, loss = 1.60772656\n",
            "Iteration 9758, loss = 1.60762288\n",
            "Iteration 9759, loss = 1.60751927\n",
            "Iteration 9760, loss = 1.60741572\n",
            "Iteration 9761, loss = 1.60731224\n",
            "Iteration 9762, loss = 1.60720883\n",
            "Iteration 9763, loss = 1.60710548\n",
            "Iteration 9764, loss = 1.60700219\n",
            "Iteration 9765, loss = 1.60689897\n",
            "Iteration 9766, loss = 1.60679582\n",
            "Iteration 9767, loss = 1.60669273\n",
            "Iteration 9768, loss = 1.60658971\n",
            "Iteration 9769, loss = 1.60648675\n",
            "Iteration 9770, loss = 1.60638386\n",
            "Iteration 9771, loss = 1.60628103\n",
            "Iteration 9772, loss = 1.60617827\n",
            "Iteration 9773, loss = 1.60607557\n",
            "Iteration 9774, loss = 1.60597293\n",
            "Iteration 9775, loss = 1.60587037\n",
            "Iteration 9776, loss = 1.60576786\n",
            "Iteration 9777, loss = 1.60566542\n",
            "Iteration 9778, loss = 1.60556305\n",
            "Iteration 9779, loss = 1.60546074\n",
            "Iteration 9780, loss = 1.60535849\n",
            "Iteration 9781, loss = 1.60525631\n",
            "Iteration 9782, loss = 1.60515420\n",
            "Iteration 9783, loss = 1.60505214\n",
            "Iteration 9784, loss = 1.60495015\n",
            "Iteration 9785, loss = 1.60484823\n",
            "Iteration 9786, loss = 1.60474637\n",
            "Iteration 9787, loss = 1.60464457\n",
            "Iteration 9788, loss = 1.60454284\n",
            "Iteration 9789, loss = 1.60444117\n",
            "Iteration 9790, loss = 1.60433957\n",
            "Iteration 9791, loss = 1.60423803\n",
            "Iteration 9792, loss = 1.60413655\n",
            "Iteration 9793, loss = 1.60403514\n",
            "Iteration 9794, loss = 1.60393379\n",
            "Iteration 9795, loss = 1.60383251\n",
            "Iteration 9796, loss = 1.60373128\n",
            "Iteration 9797, loss = 1.60363012\n",
            "Iteration 9798, loss = 1.60352903\n",
            "Iteration 9799, loss = 1.60342800\n",
            "Iteration 9800, loss = 1.60332703\n",
            "Iteration 9801, loss = 1.60322612\n",
            "Iteration 9802, loss = 1.60312528\n",
            "Iteration 9803, loss = 1.60302450\n",
            "Iteration 9804, loss = 1.60292378\n",
            "Iteration 9805, loss = 1.60282313\n",
            "Iteration 9806, loss = 1.60272254\n",
            "Iteration 9807, loss = 1.60262201\n",
            "Iteration 9808, loss = 1.60252154\n",
            "Iteration 9809, loss = 1.60242114\n",
            "Iteration 9810, loss = 1.60232080\n",
            "Iteration 9811, loss = 1.60222053\n",
            "Iteration 9812, loss = 1.60212031\n",
            "Iteration 9813, loss = 1.60202016\n",
            "Iteration 9814, loss = 1.60192007\n",
            "Iteration 9815, loss = 1.60182004\n",
            "Iteration 9816, loss = 1.60172007\n",
            "Iteration 9817, loss = 1.60162017\n",
            "Iteration 9818, loss = 1.60152033\n",
            "Iteration 9819, loss = 1.60142055\n",
            "Iteration 9820, loss = 1.60132083\n",
            "Iteration 9821, loss = 1.60122118\n",
            "Iteration 9822, loss = 1.60112159\n",
            "Iteration 9823, loss = 1.60102205\n",
            "Iteration 9824, loss = 1.60092258\n",
            "Iteration 9825, loss = 1.60082318\n",
            "Iteration 9826, loss = 1.60072383\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(10, 7, 4), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=10000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=80, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=10,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6XQ76GG9QCk2",
        "outputId": "948a547f-82ec-4a5d-866e-5f4f137b40e2"
      },
      "source": [
        "y_pred = mlp.predict(xtest)\n",
        "y_pred"
      ],
      "id": "6XQ76GG9QCk2",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iCeU-01TPEXF",
        "outputId": "e3c972f0-07a2-420e-ce41-3e478788fb75"
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cm = multilabel_confusion_matrix(y_pred, ytest)\n",
        "\n",
        "acc = accuracy_score(ytest, y_pred)\n",
        "\n",
        "print('PrecisiÃ³n de la RNA MLPClassifier : % 2.3f' % acc)"
      ],
      "id": "iCeU-01TPEXF",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PrecisiÃ³n de la RNA MLPClassifier :  1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Mxd4p3xcNIe6",
        "outputId": "9ff8684a-ca3f-4c35-8e3e-84cf1288c4f5"
      },
      "source": [
        "cm"
      ],
      "id": "Mxd4p3xcNIe6",
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0],\n",
              "        [0, 1]],\n",
              "\n",
              "       [[1, 0],\n",
              "        [0, 0]],\n",
              "\n",
              "       [[1, 0],\n",
              "        [0, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[1, 0],\n",
              "        [0, 0]],\n",
              "\n",
              "       [[1, 0],\n",
              "        [0, 0]],\n",
              "\n",
              "       [[0, 0],\n",
              "        [0, 1]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFqFBJ5tVlVp"
      },
      "source": [
        "*Enlace de repositorio*\n",
        "\n"
      ],
      "id": "PFqFBJ5tVlVp"
    }
  ]
}